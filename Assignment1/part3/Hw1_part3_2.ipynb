{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cc99091b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import argparse\n",
    "import torch\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "from torchsummary import summary\n",
    "use_cuda = torch.cuda.is_available()\n",
    "torch.manual_seed(1)\n",
    "\n",
    "device = torch.device(\"cuda\" if use_cuda else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6dd9ebc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CnnNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CnnNet, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 32, 3, 1)\n",
    "        self.dropout1 = nn.Dropout(0.25)\n",
    "        self.dropout2 = nn.Dropout(0.5)\n",
    "        self.fc1 = nn.Linear(5408, 128)\n",
    "        self.fc2 = nn.Linear(128, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = F.relu(x)\n",
    "        x = F.max_pool2d(x, 2)\n",
    "        x = self.dropout1(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = self.fc1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.dropout2(x)\n",
    "        x = self.fc2(x)\n",
    "        output = F.log_softmax(x, dim=1)\n",
    "        return output\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0c23a92d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1           [-1, 32, 26, 26]             320\n",
      "           Dropout-2           [-1, 32, 13, 13]               0\n",
      "            Linear-3                  [-1, 128]         692,352\n",
      "           Dropout-4                  [-1, 128]               0\n",
      "            Linear-5                   [-1, 10]           1,290\n",
      "================================================================\n",
      "Total params: 693,962\n",
      "Trainable params: 693,962\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.00\n",
      "Forward/backward pass size (MB): 0.21\n",
      "Params size (MB): 2.65\n",
      "Estimated Total Size (MB): 2.86\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "model = CnnNet().to(device)\n",
    "summary(model,(1,28,28))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "be538ea5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CnnNet2(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CnnNet2, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 16, 3, 1)\n",
    "        self.dropout1 = nn.Dropout(0.25)\n",
    "        self.dropout2 = nn.Dropout(0.5)\n",
    "        self.fc1 = nn.Linear(2704, 128)\n",
    "        self.fc2 = nn.Linear(128, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = F.relu(x)\n",
    "        x = F.max_pool2d(x, 2)\n",
    "        x = self.dropout1(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = self.fc1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.dropout2(x)\n",
    "        x = self.fc2(x)\n",
    "        output = F.log_softmax(x, dim=1)\n",
    "        return output\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8b6c931a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1           [-1, 16, 26, 26]             160\n",
      "           Dropout-2           [-1, 16, 13, 13]               0\n",
      "            Linear-3                  [-1, 128]         346,240\n",
      "           Dropout-4                  [-1, 128]               0\n",
      "            Linear-5                   [-1, 10]           1,290\n",
      "================================================================\n",
      "Total params: 347,690\n",
      "Trainable params: 347,690\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.00\n",
      "Forward/backward pass size (MB): 0.11\n",
      "Params size (MB): 1.33\n",
      "Estimated Total Size (MB): 1.43\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "model = CnnNet2().to(device)\n",
    "summary(model,(1,28,28))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5ac5ee07",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CnnNet3(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CnnNet3, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 16, 3, 1)\n",
    "        self.dropout1 = nn.Dropout(0.25)\n",
    "        self.dropout2 = nn.Dropout(0.5)\n",
    "        self.fc1 = nn.Linear(2704, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = F.relu(x)\n",
    "        x = F.max_pool2d(x, 2)\n",
    "        x = self.dropout1(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = self.fc1(x)\n",
    "        output = F.log_softmax(x, dim=1)\n",
    "        return output\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "246a1053",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1           [-1, 16, 26, 26]             160\n",
      "           Dropout-2           [-1, 16, 13, 13]               0\n",
      "            Linear-3                   [-1, 10]          27,050\n",
      "================================================================\n",
      "Total params: 27,210\n",
      "Trainable params: 27,210\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.00\n",
      "Forward/backward pass size (MB): 0.10\n",
      "Params size (MB): 0.10\n",
      "Estimated Total Size (MB): 0.21\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "model = CnnNet3().to(device)\n",
    "summary(model,(1,28,28))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c2ccc1b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CnnNet4(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CnnNet4, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 16, 3, 1)\n",
    "        self.dropout1 = nn.Dropout(0.25)\n",
    "        self.dropout2 = nn.Dropout(0.5)\n",
    "        self.fc1 = nn.Linear(2704, 512)\n",
    "        self.fc2 = nn.Linear(512, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = F.relu(x)\n",
    "        x = F.max_pool2d(x, 2)\n",
    "        x = self.dropout1(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = self.fc1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.dropout2(x)\n",
    "        x = self.fc2(x)\n",
    "        output = F.log_softmax(x, dim=1)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "bc65e881",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1           [-1, 16, 26, 26]             160\n",
      "           Dropout-2           [-1, 16, 13, 13]               0\n",
      "            Linear-3                  [-1, 512]       1,384,960\n",
      "           Dropout-4                  [-1, 512]               0\n",
      "            Linear-5                   [-1, 10]           5,130\n",
      "================================================================\n",
      "Total params: 1,390,250\n",
      "Trainable params: 1,390,250\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.00\n",
      "Forward/backward pass size (MB): 0.11\n",
      "Params size (MB): 5.30\n",
      "Estimated Total Size (MB): 5.42\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "model = CnnNet4().to(device)\n",
    "summary(model,(1,28,28))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "cf468919",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CnnNet5(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CnnNet5, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 16, 3, 1)\n",
    "        self.conv2 = nn.Conv2d(16, 32, 3, 1)\n",
    "        self.dropout1 = nn.Dropout(0.25)\n",
    "        self.dropout2 = nn.Dropout(0.5)\n",
    "        self.fc1 = nn.Linear(4608, 256)\n",
    "        self.fc2 = nn.Linear(256, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.conv2(x)\n",
    "        x = F.relu(x)\n",
    "        x = F.max_pool2d(x, 2)\n",
    "        x = self.dropout1(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = self.fc1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.dropout2(x)\n",
    "        x = self.fc2(x)\n",
    "        output = F.log_softmax(x, dim=1)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c9919f58",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1           [-1, 16, 26, 26]             160\n",
      "            Conv2d-2           [-1, 32, 24, 24]           4,640\n",
      "           Dropout-3           [-1, 32, 12, 12]               0\n",
      "            Linear-4                  [-1, 256]       1,179,904\n",
      "           Dropout-5                  [-1, 256]               0\n",
      "            Linear-6                   [-1, 10]           2,570\n",
      "================================================================\n",
      "Total params: 1,187,274\n",
      "Trainable params: 1,187,274\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.00\n",
      "Forward/backward pass size (MB): 0.26\n",
      "Params size (MB): 4.53\n",
      "Estimated Total Size (MB): 4.79\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "model = CnnNet5().to(device)\n",
    "summary(model,(1,28,28))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "83b0670b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CnnNet6(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CnnNet6, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 16, 3, 1)\n",
    "        self.conv2 = nn.Conv2d(16, 32, 3, 1)\n",
    "        self.dropout1 = nn.Dropout(0.25)\n",
    "        self.dropout2 = nn.Dropout(0.5)\n",
    "        self.fc1 = nn.Linear(4608, 512)\n",
    "        self.fc2 = nn.Linear(512, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.conv2(x)\n",
    "        x = F.relu(x)\n",
    "        x = F.max_pool2d(x, 2)\n",
    "        x = self.dropout1(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = self.fc1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.dropout2(x)\n",
    "        x = self.fc2(x)\n",
    "        output = F.log_softmax(x, dim=1)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "93c7f172",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1           [-1, 16, 26, 26]             160\n",
      "            Conv2d-2           [-1, 32, 24, 24]           4,640\n",
      "           Dropout-3           [-1, 32, 12, 12]               0\n",
      "            Linear-4                  [-1, 512]       2,359,808\n",
      "           Dropout-5                  [-1, 512]               0\n",
      "            Linear-6                   [-1, 10]           5,130\n",
      "================================================================\n",
      "Total params: 2,369,738\n",
      "Trainable params: 2,369,738\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.00\n",
      "Forward/backward pass size (MB): 0.27\n",
      "Params size (MB): 9.04\n",
      "Estimated Total Size (MB): 9.31\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "model = CnnNet6().to(device)\n",
    "summary(model,(1,28,28))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "614a5d9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train( model, device, train_loader, optimizer,loss_fn, epoch):\n",
    "    train_loss=0\n",
    "    model.train()\n",
    "    correct = 0\n",
    "    loss_fn = nn.CrossEntropyLoss(reduction='sum')\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        loss = loss_fn(output, target)\n",
    "        train_loss+=loss.item()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        pred = output.argmax(dim=1, keepdim=True)  # get the index of the max log-probability\n",
    "        correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "    train_loss /= len(train_loader.dataset)\n",
    "    print(\"Training - Epoch:\", epoch, \"loss:\", train_loss, \"accuracy:\", 100. * correct / len(train_loader.dataset))  \n",
    "    return train_loss, 100. * correct / len(train_loader.dataset)\n",
    "\n",
    "def test(model, device, test_loader):\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    loss_fn = nn.CrossEntropyLoss(reduction='sum')\n",
    "    with torch.no_grad():\n",
    "        for data, target in test_loader:\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            output = model(data)\n",
    "            test_loss += loss_fn(output, target)  # sum up batch loss\n",
    "            pred = output.argmax(dim=1, keepdim=True)  # get the index of the max log-probability\n",
    "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "\n",
    "    test_loss /= len(test_loader.dataset)\n",
    "\n",
    "    print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
    "        test_loss, correct, len(test_loader.dataset),\n",
    "        100. * correct / len(test_loader.dataset)))\n",
    "    return test_loss, 100. * correct / len(test_loader.dataset)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "215d9d07",
   "metadata": {},
   "outputs": [],
   "source": [
    "use_cuda = torch.cuda.is_available()\n",
    "torch.manual_seed(1)\n",
    "\n",
    "device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
    "\n",
    "train_kwargs = {'batch_size': 512}\n",
    "test_kwargs = {'batch_size': 512}\n",
    "if use_cuda:\n",
    "    cuda_kwargs = {'num_workers': 1,\n",
    "                   'pin_memory': True,\n",
    "                   'shuffle': True}\n",
    "    train_kwargs.update(cuda_kwargs)\n",
    "    test_kwargs.update(cuda_kwargs)\n",
    "\n",
    "transform=transforms.Compose([\n",
    "        transforms.ToTensor()\n",
    "        ])\n",
    "dataset1 = datasets.MNIST('../data', train=True, download=True, transform=transform)\n",
    "dataset2 = datasets.MNIST('../data', train=False, transform=transform)\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(dataset1,**train_kwargs)\n",
    "test_loader = torch.utils.data.DataLoader(dataset2, **test_kwargs)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4d244c6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def returnModelAccAndLoss(model, device, train_loader, test_loader, optimizer, loss, epoch, scheduler):\n",
    "    train_loss=[]\n",
    "    test_loss=[]\n",
    "    accuracy_train=[]\n",
    "    accuracy_test=[]\n",
    "    for epoch in range(1, epoch+1):\n",
    "        tr_loss, tr_accuracy=train(model, device, train_loader, optimizer, loss, epoch)\n",
    "        train_loss.append(tr_loss)\n",
    "        accuracy_train.append(tr_accuracy)\n",
    "        tst_loss, tst_accuracy = test(model, device, test_loader)\n",
    "        test_loss.append(tst_loss)\n",
    "        accuracy_test.append(tst_accuracy)\n",
    "        #scheduler.step()\n",
    "    return train_loss, accuracy_train, test_loss, accuracy_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "33625fa4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "693962\n",
      "Training - Epoch: 1 loss: 1.790820398203532 accuracy: 53.135\n",
      "\n",
      "Test set: Average loss: 1.1104, Accuracy: 8102/10000 (81%)\n",
      "\n",
      "Training - Epoch: 2 loss: 0.8813461731592814 accuracy: 77.04333333333334\n",
      "\n",
      "Test set: Average loss: 0.5339, Accuracy: 8811/10000 (88%)\n",
      "\n",
      "Training - Epoch: 3 loss: 0.5782252535502116 accuracy: 83.60666666666667\n",
      "\n",
      "Test set: Average loss: 0.3934, Accuracy: 9042/10000 (90%)\n",
      "\n",
      "Training - Epoch: 4 loss: 0.4810165751139323 accuracy: 86.04333333333334\n",
      "\n",
      "Test set: Average loss: 0.3382, Accuracy: 9135/10000 (91%)\n",
      "\n",
      "Training - Epoch: 5 loss: 0.43151773935953774 accuracy: 87.59833333333333\n",
      "\n",
      "Test set: Average loss: 0.3077, Accuracy: 9188/10000 (92%)\n",
      "\n",
      "Training - Epoch: 6 loss: 0.40016012566884357 accuracy: 88.36166666666666\n",
      "\n",
      "Test set: Average loss: 0.2865, Accuracy: 9237/10000 (92%)\n",
      "\n",
      "Training - Epoch: 7 loss: 0.3762335540771484 accuracy: 88.90333333333334\n",
      "\n",
      "Test set: Average loss: 0.2706, Accuracy: 9247/10000 (92%)\n",
      "\n",
      "Training - Epoch: 8 loss: 0.3597145875295003 accuracy: 89.50166666666667\n",
      "\n",
      "Test set: Average loss: 0.2574, Accuracy: 9278/10000 (93%)\n",
      "\n",
      "Training - Epoch: 9 loss: 0.3420131200790405 accuracy: 89.96833333333333\n",
      "\n",
      "Test set: Average loss: 0.2453, Accuracy: 9296/10000 (93%)\n",
      "\n",
      "Training - Epoch: 10 loss: 0.33271211926142374 accuracy: 90.40833333333333\n",
      "\n",
      "Test set: Average loss: 0.2367, Accuracy: 9324/10000 (93%)\n",
      "\n",
      "Training - Epoch: 11 loss: 0.318047390238444 accuracy: 90.815\n",
      "\n",
      "Test set: Average loss: 0.2277, Accuracy: 9339/10000 (93%)\n",
      "\n",
      "Training - Epoch: 12 loss: 0.30860153210957847 accuracy: 91.01166666666667\n",
      "\n",
      "Test set: Average loss: 0.2202, Accuracy: 9367/10000 (94%)\n",
      "\n",
      "Training - Epoch: 13 loss: 0.2985232353846232 accuracy: 91.40833333333333\n",
      "\n",
      "Test set: Average loss: 0.2124, Accuracy: 9379/10000 (94%)\n",
      "\n",
      "Training - Epoch: 14 loss: 0.290958859761556 accuracy: 91.56333333333333\n",
      "\n",
      "Test set: Average loss: 0.2051, Accuracy: 9396/10000 (94%)\n",
      "\n",
      "Training - Epoch: 15 loss: 0.28173065039316814 accuracy: 91.77166666666666\n",
      "\n",
      "Test set: Average loss: 0.2000, Accuracy: 9417/10000 (94%)\n",
      "\n",
      "Training - Epoch: 16 loss: 0.2725108637491862 accuracy: 91.97\n",
      "\n",
      "Test set: Average loss: 0.1938, Accuracy: 9444/10000 (94%)\n",
      "\n",
      "Training - Epoch: 17 loss: 0.26699539683659873 accuracy: 92.30166666666666\n",
      "\n",
      "Test set: Average loss: 0.1889, Accuracy: 9444/10000 (94%)\n",
      "\n",
      "Training - Epoch: 18 loss: 0.2622499703725179 accuracy: 92.37\n",
      "\n",
      "Test set: Average loss: 0.1837, Accuracy: 9467/10000 (95%)\n",
      "\n",
      "Training - Epoch: 19 loss: 0.2560808883031209 accuracy: 92.56333333333333\n",
      "\n",
      "Test set: Average loss: 0.1788, Accuracy: 9481/10000 (95%)\n",
      "\n",
      "Training - Epoch: 20 loss: 0.2490642723083496 accuracy: 92.81333333333333\n",
      "\n",
      "Test set: Average loss: 0.1745, Accuracy: 9489/10000 (95%)\n",
      "\n",
      "Training - Epoch: 21 loss: 0.24440929724375407 accuracy: 92.94833333333334\n",
      "\n",
      "Test set: Average loss: 0.1707, Accuracy: 9499/10000 (95%)\n",
      "\n",
      "Training - Epoch: 22 loss: 0.23805226580301922 accuracy: 93.01333333333334\n",
      "\n",
      "Test set: Average loss: 0.1662, Accuracy: 9518/10000 (95%)\n",
      "\n",
      "Training - Epoch: 23 loss: 0.23634543822606405 accuracy: 93.10333333333334\n",
      "\n",
      "Test set: Average loss: 0.1633, Accuracy: 9526/10000 (95%)\n",
      "\n",
      "Training - Epoch: 24 loss: 0.23062727473576863 accuracy: 93.315\n",
      "\n",
      "Test set: Average loss: 0.1590, Accuracy: 9544/10000 (95%)\n",
      "\n",
      "Training - Epoch: 25 loss: 0.22573659426371256 accuracy: 93.43833333333333\n",
      "\n",
      "Test set: Average loss: 0.1560, Accuracy: 9543/10000 (95%)\n",
      "\n",
      "Training - Epoch: 26 loss: 0.2209153029759725 accuracy: 93.58166666666666\n",
      "\n",
      "Test set: Average loss: 0.1532, Accuracy: 9559/10000 (96%)\n",
      "\n",
      "Training - Epoch: 27 loss: 0.21767476828893026 accuracy: 93.71833333333333\n",
      "\n",
      "Test set: Average loss: 0.1498, Accuracy: 9553/10000 (96%)\n",
      "\n",
      "Training - Epoch: 28 loss: 0.21215204728444417 accuracy: 93.80833333333334\n",
      "\n",
      "Test set: Average loss: 0.1467, Accuracy: 9578/10000 (96%)\n",
      "\n",
      "Training - Epoch: 29 loss: 0.20959202796618143 accuracy: 93.95833333333333\n",
      "\n",
      "Test set: Average loss: 0.1438, Accuracy: 9573/10000 (96%)\n",
      "\n",
      "Training - Epoch: 30 loss: 0.20786846974690756 accuracy: 94.00833333333334\n",
      "\n",
      "Test set: Average loss: 0.1412, Accuracy: 9582/10000 (96%)\n",
      "\n",
      "Training - Epoch: 31 loss: 0.20185028400421143 accuracy: 94.10833333333333\n",
      "\n",
      "Test set: Average loss: 0.1384, Accuracy: 9596/10000 (96%)\n",
      "\n",
      "Training - Epoch: 32 loss: 0.20130534674326578 accuracy: 94.13833333333334\n",
      "\n",
      "Test set: Average loss: 0.1365, Accuracy: 9594/10000 (96%)\n",
      "\n",
      "Training - Epoch: 33 loss: 0.19643650382359823 accuracy: 94.255\n",
      "\n",
      "Test set: Average loss: 0.1332, Accuracy: 9605/10000 (96%)\n",
      "\n",
      "Training - Epoch: 34 loss: 0.19398950774272283 accuracy: 94.44\n",
      "\n",
      "Test set: Average loss: 0.1308, Accuracy: 9611/10000 (96%)\n",
      "\n",
      "Training - Epoch: 35 loss: 0.19254075679779054 accuracy: 94.45833333333333\n",
      "\n",
      "Test set: Average loss: 0.1294, Accuracy: 9624/10000 (96%)\n",
      "\n",
      "Training - Epoch: 36 loss: 0.18847046785354615 accuracy: 94.62333333333333\n",
      "\n",
      "Test set: Average loss: 0.1272, Accuracy: 9628/10000 (96%)\n",
      "\n",
      "Training - Epoch: 37 loss: 0.18544135519663493 accuracy: 94.64\n",
      "\n",
      "Test set: Average loss: 0.1257, Accuracy: 9633/10000 (96%)\n",
      "\n",
      "Training - Epoch: 38 loss: 0.18361825664838155 accuracy: 94.68833333333333\n",
      "\n",
      "Test set: Average loss: 0.1232, Accuracy: 9638/10000 (96%)\n",
      "\n",
      "Training - Epoch: 39 loss: 0.17935031716028849 accuracy: 94.89333333333333\n",
      "\n",
      "Test set: Average loss: 0.1208, Accuracy: 9640/10000 (96%)\n",
      "\n",
      "Training - Epoch: 40 loss: 0.17837926394144693 accuracy: 94.84166666666667\n",
      "\n",
      "Test set: Average loss: 0.1194, Accuracy: 9640/10000 (96%)\n",
      "\n",
      "Training - Epoch: 41 loss: 0.17800048894882203 accuracy: 94.825\n",
      "\n",
      "Test set: Average loss: 0.1180, Accuracy: 9655/10000 (97%)\n",
      "\n",
      "Training - Epoch: 42 loss: 0.1743886519432068 accuracy: 94.98833333333333\n",
      "\n",
      "Test set: Average loss: 0.1167, Accuracy: 9659/10000 (97%)\n",
      "\n",
      "Training - Epoch: 43 loss: 0.1712047601699829 accuracy: 94.94333333333333\n",
      "\n",
      "Test set: Average loss: 0.1145, Accuracy: 9661/10000 (97%)\n",
      "\n",
      "Training - Epoch: 44 loss: 0.17241752325693765 accuracy: 94.96166666666667\n",
      "\n",
      "Test set: Average loss: 0.1134, Accuracy: 9668/10000 (97%)\n",
      "\n",
      "Training - Epoch: 45 loss: 0.16552337783972423 accuracy: 95.235\n",
      "\n",
      "Test set: Average loss: 0.1115, Accuracy: 9671/10000 (97%)\n",
      "\n",
      "Training - Epoch: 46 loss: 0.16697716948191324 accuracy: 95.20833333333333\n",
      "\n",
      "Test set: Average loss: 0.1103, Accuracy: 9685/10000 (97%)\n",
      "\n",
      "Training - Epoch: 47 loss: 0.1641662909825643 accuracy: 95.22833333333334\n",
      "\n",
      "Test set: Average loss: 0.1088, Accuracy: 9675/10000 (97%)\n",
      "\n",
      "Training - Epoch: 48 loss: 0.1608248015085856 accuracy: 95.29833333333333\n",
      "\n",
      "Test set: Average loss: 0.1070, Accuracy: 9680/10000 (97%)\n",
      "\n",
      "Training - Epoch: 49 loss: 0.16080850779215494 accuracy: 95.35\n",
      "\n",
      "Test set: Average loss: 0.1059, Accuracy: 9688/10000 (97%)\n",
      "\n",
      "Training - Epoch: 50 loss: 0.15834705526034037 accuracy: 95.38166666666666\n",
      "\n",
      "Test set: Average loss: 0.1044, Accuracy: 9692/10000 (97%)\n",
      "\n",
      "Training - Epoch: 51 loss: 0.15885121130943297 accuracy: 95.49166666666666\n",
      "\n",
      "Test set: Average loss: 0.1033, Accuracy: 9696/10000 (97%)\n",
      "\n",
      "Training - Epoch: 52 loss: 0.15555537246068318 accuracy: 95.51833333333333\n",
      "\n",
      "Test set: Average loss: 0.1016, Accuracy: 9702/10000 (97%)\n",
      "\n",
      "Training - Epoch: 53 loss: 0.15361088072458903 accuracy: 95.57666666666667\n",
      "\n",
      "Test set: Average loss: 0.1009, Accuracy: 9697/10000 (97%)\n",
      "\n",
      "Training - Epoch: 54 loss: 0.15396480188369752 accuracy: 95.67166666666667\n",
      "\n",
      "Test set: Average loss: 0.0998, Accuracy: 9700/10000 (97%)\n",
      "\n",
      "Training - Epoch: 55 loss: 0.14957968635559082 accuracy: 95.67833333333333\n",
      "\n",
      "Test set: Average loss: 0.0987, Accuracy: 9711/10000 (97%)\n",
      "\n",
      "Training - Epoch: 56 loss: 0.14741860523223876 accuracy: 95.81\n",
      "\n",
      "Test set: Average loss: 0.0976, Accuracy: 9709/10000 (97%)\n",
      "\n",
      "Training - Epoch: 57 loss: 0.14796309792200724 accuracy: 95.70666666666666\n",
      "\n",
      "Test set: Average loss: 0.0960, Accuracy: 9719/10000 (97%)\n",
      "\n",
      "Training - Epoch: 58 loss: 0.14575191869735718 accuracy: 95.84\n",
      "\n",
      "Test set: Average loss: 0.0954, Accuracy: 9714/10000 (97%)\n",
      "\n",
      "Training - Epoch: 59 loss: 0.14644365339279175 accuracy: 95.82333333333334\n",
      "\n",
      "Test set: Average loss: 0.0945, Accuracy: 9715/10000 (97%)\n",
      "\n",
      "Training - Epoch: 60 loss: 0.141421275806427 accuracy: 95.82666666666667\n",
      "\n",
      "Test set: Average loss: 0.0929, Accuracy: 9726/10000 (97%)\n",
      "\n",
      "Training - Epoch: 61 loss: 0.14136638507843016 accuracy: 95.92\n",
      "\n",
      "Test set: Average loss: 0.0923, Accuracy: 9729/10000 (97%)\n",
      "\n",
      "Training - Epoch: 62 loss: 0.14037806499799094 accuracy: 95.96333333333334\n",
      "\n",
      "Test set: Average loss: 0.0912, Accuracy: 9734/10000 (97%)\n",
      "\n",
      "Training - Epoch: 63 loss: 0.13728269362449647 accuracy: 95.97833333333334\n",
      "\n",
      "Test set: Average loss: 0.0902, Accuracy: 9730/10000 (97%)\n",
      "\n",
      "Training - Epoch: 64 loss: 0.13945752509435017 accuracy: 96.01333333333334\n",
      "\n",
      "Test set: Average loss: 0.0896, Accuracy: 9736/10000 (97%)\n",
      "\n",
      "Training - Epoch: 65 loss: 0.13648647322654725 accuracy: 96.09333333333333\n",
      "\n",
      "Test set: Average loss: 0.0880, Accuracy: 9742/10000 (97%)\n",
      "\n",
      "Training - Epoch: 66 loss: 0.13568541195392608 accuracy: 96.06\n",
      "\n",
      "Test set: Average loss: 0.0879, Accuracy: 9744/10000 (97%)\n",
      "\n",
      "Training - Epoch: 67 loss: 0.13436459827423095 accuracy: 96.175\n",
      "\n",
      "Test set: Average loss: 0.0873, Accuracy: 9746/10000 (97%)\n",
      "\n",
      "Training - Epoch: 68 loss: 0.13354626738230388 accuracy: 96.12\n",
      "\n",
      "Test set: Average loss: 0.0866, Accuracy: 9747/10000 (97%)\n",
      "\n",
      "Training - Epoch: 69 loss: 0.13226920173168183 accuracy: 96.175\n",
      "\n",
      "Test set: Average loss: 0.0853, Accuracy: 9753/10000 (98%)\n",
      "\n",
      "Training - Epoch: 70 loss: 0.13096106363932292 accuracy: 96.20666666666666\n",
      "\n",
      "Test set: Average loss: 0.0849, Accuracy: 9751/10000 (98%)\n",
      "\n",
      "Training - Epoch: 71 loss: 0.12916359419822693 accuracy: 96.29\n",
      "\n",
      "Test set: Average loss: 0.0835, Accuracy: 9759/10000 (98%)\n",
      "\n",
      "Training - Epoch: 72 loss: 0.1294907447020213 accuracy: 96.19333333333333\n",
      "\n",
      "Test set: Average loss: 0.0832, Accuracy: 9754/10000 (98%)\n",
      "\n",
      "Training - Epoch: 73 loss: 0.12767838204701742 accuracy: 96.33166666666666\n",
      "\n",
      "Test set: Average loss: 0.0826, Accuracy: 9760/10000 (98%)\n",
      "\n",
      "Training - Epoch: 74 loss: 0.12569604592323302 accuracy: 96.42833333333333\n",
      "\n",
      "Test set: Average loss: 0.0818, Accuracy: 9760/10000 (98%)\n",
      "\n",
      "Training - Epoch: 75 loss: 0.12542429787317913 accuracy: 96.43833333333333\n",
      "\n",
      "Test set: Average loss: 0.0809, Accuracy: 9760/10000 (98%)\n",
      "\n",
      "Training - Epoch: 76 loss: 0.12644187558492026 accuracy: 96.40833333333333\n",
      "\n",
      "Test set: Average loss: 0.0808, Accuracy: 9763/10000 (98%)\n",
      "\n",
      "Training - Epoch: 77 loss: 0.12490469358762105 accuracy: 96.49333333333334\n",
      "\n",
      "Test set: Average loss: 0.0795, Accuracy: 9766/10000 (98%)\n",
      "\n",
      "Training - Epoch: 78 loss: 0.12470733551979064 accuracy: 96.42\n",
      "\n",
      "Test set: Average loss: 0.0792, Accuracy: 9765/10000 (98%)\n",
      "\n",
      "Training - Epoch: 79 loss: 0.1219251015663147 accuracy: 96.47833333333334\n",
      "\n",
      "Test set: Average loss: 0.0787, Accuracy: 9767/10000 (98%)\n",
      "\n",
      "Training - Epoch: 80 loss: 0.12140233454704284 accuracy: 96.465\n",
      "\n",
      "Test set: Average loss: 0.0778, Accuracy: 9774/10000 (98%)\n",
      "\n",
      "Training - Epoch: 81 loss: 0.12079312451680502 accuracy: 96.53666666666666\n",
      "\n",
      "Test set: Average loss: 0.0779, Accuracy: 9768/10000 (98%)\n",
      "\n",
      "Training - Epoch: 82 loss: 0.12006407070159912 accuracy: 96.48833333333333\n",
      "\n",
      "Test set: Average loss: 0.0768, Accuracy: 9771/10000 (98%)\n",
      "\n",
      "Training - Epoch: 83 loss: 0.11937835694948833 accuracy: 96.575\n",
      "\n",
      "Test set: Average loss: 0.0761, Accuracy: 9777/10000 (98%)\n",
      "\n",
      "Training - Epoch: 84 loss: 0.11909268123308818 accuracy: 96.535\n",
      "\n",
      "Test set: Average loss: 0.0756, Accuracy: 9773/10000 (98%)\n",
      "\n",
      "Training - Epoch: 85 loss: 0.11583155396779378 accuracy: 96.63166666666666\n",
      "\n",
      "Test set: Average loss: 0.0752, Accuracy: 9781/10000 (98%)\n",
      "\n",
      "Training - Epoch: 86 loss: 0.11638432072003682 accuracy: 96.67666666666666\n",
      "\n",
      "Test set: Average loss: 0.0747, Accuracy: 9782/10000 (98%)\n",
      "\n",
      "Training - Epoch: 87 loss: 0.11696293080647786 accuracy: 96.61166666666666\n",
      "\n",
      "Test set: Average loss: 0.0745, Accuracy: 9778/10000 (98%)\n",
      "\n",
      "Training - Epoch: 88 loss: 0.1150744740486145 accuracy: 96.68333333333334\n",
      "\n",
      "Test set: Average loss: 0.0739, Accuracy: 9781/10000 (98%)\n",
      "\n",
      "Training - Epoch: 89 loss: 0.11379166965484619 accuracy: 96.78333333333333\n",
      "\n",
      "Test set: Average loss: 0.0731, Accuracy: 9784/10000 (98%)\n",
      "\n",
      "Training - Epoch: 90 loss: 0.1132971511999766 accuracy: 96.73\n",
      "\n",
      "Test set: Average loss: 0.0726, Accuracy: 9783/10000 (98%)\n",
      "\n",
      "Training - Epoch: 91 loss: 0.113574946808815 accuracy: 96.70666666666666\n",
      "\n",
      "Test set: Average loss: 0.0716, Accuracy: 9786/10000 (98%)\n",
      "\n",
      "Training - Epoch: 92 loss: 0.11228140732447306 accuracy: 96.715\n",
      "\n",
      "Test set: Average loss: 0.0715, Accuracy: 9786/10000 (98%)\n",
      "\n",
      "Training - Epoch: 93 loss: 0.11200938247044881 accuracy: 96.72666666666667\n",
      "\n",
      "Test set: Average loss: 0.0714, Accuracy: 9789/10000 (98%)\n",
      "\n",
      "Training - Epoch: 94 loss: 0.11113812325000763 accuracy: 96.70666666666666\n",
      "\n",
      "Test set: Average loss: 0.0707, Accuracy: 9787/10000 (98%)\n",
      "\n",
      "Training - Epoch: 95 loss: 0.11192742006778716 accuracy: 96.74666666666667\n",
      "\n",
      "Test set: Average loss: 0.0700, Accuracy: 9790/10000 (98%)\n",
      "\n",
      "Training - Epoch: 96 loss: 0.11001194229125977 accuracy: 96.82666666666667\n",
      "\n",
      "Test set: Average loss: 0.0699, Accuracy: 9791/10000 (98%)\n",
      "\n",
      "Training - Epoch: 97 loss: 0.10852980817159016 accuracy: 96.89333333333333\n",
      "\n",
      "Test set: Average loss: 0.0691, Accuracy: 9793/10000 (98%)\n",
      "\n",
      "Training - Epoch: 98 loss: 0.10897584796746572 accuracy: 96.86833333333334\n",
      "\n",
      "Test set: Average loss: 0.0684, Accuracy: 9794/10000 (98%)\n",
      "\n",
      "Training - Epoch: 99 loss: 0.10674137194951375 accuracy: 96.99833333333333\n",
      "\n",
      "Test set: Average loss: 0.0681, Accuracy: 9786/10000 (98%)\n",
      "\n",
      "Training - Epoch: 100 loss: 0.10693340489069621 accuracy: 96.90833333333333\n",
      "\n",
      "Test set: Average loss: 0.0682, Accuracy: 9791/10000 (98%)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "epoch=100\n",
    "model = CnnNet().to(device)\n",
    "pytorch_total_params = sum(p.numel() for p in model.parameters())\n",
    "print(pytorch_total_params)\n",
    "optimizer = optim.Adadelta(model.parameters(), lr=0.01)\n",
    "scheduler = StepLR(optimizer, step_size=1, gamma=0.1)\n",
    "loss = nn.CrossEntropyLoss()\n",
    "train_loss_1, accuracy_train_1, test_loss_1, test_accuracy_1 = returnModelAccAndLoss(model, device, train_loader, test_loader, optimizer, loss, epoch, scheduler)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7f112d60",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training - Epoch: 1 loss: 0.6820473617553711 accuracy: 77.87166666666667\n",
      "\n",
      "Test set: Average loss: 0.1759, Accuracy: 9464/10000 (95%)\n",
      "\n",
      "Training - Epoch: 2 loss: 0.23565664033889772 accuracy: 93.02166666666666\n",
      "\n",
      "Test set: Average loss: 0.0983, Accuracy: 9697/10000 (97%)\n",
      "\n",
      "Training - Epoch: 3 loss: 0.1676724850098292 accuracy: 94.97666666666667\n",
      "\n",
      "Test set: Average loss: 0.0770, Accuracy: 9746/10000 (97%)\n",
      "\n",
      "Training - Epoch: 4 loss: 0.13223248626391093 accuracy: 95.94666666666667\n",
      "\n",
      "Test set: Average loss: 0.0602, Accuracy: 9793/10000 (98%)\n",
      "\n",
      "Training - Epoch: 5 loss: 0.11629669346014658 accuracy: 96.55166666666666\n",
      "\n",
      "Test set: Average loss: 0.0606, Accuracy: 9790/10000 (98%)\n",
      "\n",
      "Training - Epoch: 6 loss: 0.10282802509466807 accuracy: 96.99333333333334\n",
      "\n",
      "Test set: Average loss: 0.0547, Accuracy: 9820/10000 (98%)\n",
      "\n",
      "Training - Epoch: 7 loss: 0.0905383253733317 accuracy: 97.22333333333333\n",
      "\n",
      "Test set: Average loss: 0.0531, Accuracy: 9825/10000 (98%)\n",
      "\n",
      "Training - Epoch: 8 loss: 0.08362468538284301 accuracy: 97.51\n",
      "\n",
      "Test set: Average loss: 0.0434, Accuracy: 9848/10000 (98%)\n",
      "\n",
      "Training - Epoch: 9 loss: 0.07798986570835113 accuracy: 97.605\n",
      "\n",
      "Test set: Average loss: 0.0419, Accuracy: 9854/10000 (99%)\n",
      "\n",
      "Training - Epoch: 10 loss: 0.07204813319047292 accuracy: 97.79333333333334\n",
      "\n",
      "Test set: Average loss: 0.0392, Accuracy: 9860/10000 (99%)\n",
      "\n",
      "Training - Epoch: 11 loss: 0.06926723010937373 accuracy: 97.905\n",
      "\n",
      "Test set: Average loss: 0.0393, Accuracy: 9858/10000 (99%)\n",
      "\n",
      "Training - Epoch: 12 loss: 0.06734319030443828 accuracy: 97.94666666666667\n",
      "\n",
      "Test set: Average loss: 0.0389, Accuracy: 9868/10000 (99%)\n",
      "\n",
      "Training - Epoch: 13 loss: 0.06321909384727478 accuracy: 98.04\n",
      "\n",
      "Test set: Average loss: 0.0378, Accuracy: 9878/10000 (99%)\n",
      "\n",
      "Training - Epoch: 14 loss: 0.06171000488599141 accuracy: 98.13166666666666\n",
      "\n",
      "Test set: Average loss: 0.0375, Accuracy: 9869/10000 (99%)\n",
      "\n",
      "Training - Epoch: 15 loss: 0.058037141942977905 accuracy: 98.16333333333333\n",
      "\n",
      "Test set: Average loss: 0.0375, Accuracy: 9872/10000 (99%)\n",
      "\n",
      "Training - Epoch: 16 loss: 0.056703141919771834 accuracy: 98.23\n",
      "\n",
      "Test set: Average loss: 0.0375, Accuracy: 9878/10000 (99%)\n",
      "\n",
      "Training - Epoch: 17 loss: 0.05341779954036077 accuracy: 98.33833333333334\n",
      "\n",
      "Test set: Average loss: 0.0376, Accuracy: 9883/10000 (99%)\n",
      "\n",
      "Training - Epoch: 18 loss: 0.053073435831069944 accuracy: 98.40166666666667\n",
      "\n",
      "Test set: Average loss: 0.0393, Accuracy: 9874/10000 (99%)\n",
      "\n",
      "Training - Epoch: 19 loss: 0.05242435606221358 accuracy: 98.37666666666667\n",
      "\n",
      "Test set: Average loss: 0.0372, Accuracy: 9876/10000 (99%)\n",
      "\n",
      "Training - Epoch: 20 loss: 0.05192940037250519 accuracy: 98.455\n",
      "\n",
      "Test set: Average loss: 0.0365, Accuracy: 9878/10000 (99%)\n",
      "\n",
      "Training - Epoch: 21 loss: 0.05145537635485331 accuracy: 98.45666666666666\n",
      "\n",
      "Test set: Average loss: 0.0360, Accuracy: 9882/10000 (99%)\n",
      "\n",
      "Training - Epoch: 22 loss: 0.04950144619941711 accuracy: 98.495\n",
      "\n",
      "Test set: Average loss: 0.0377, Accuracy: 9878/10000 (99%)\n",
      "\n",
      "Training - Epoch: 23 loss: 0.050141467380523684 accuracy: 98.46166666666667\n",
      "\n",
      "Test set: Average loss: 0.0383, Accuracy: 9885/10000 (99%)\n",
      "\n",
      "Training - Epoch: 24 loss: 0.04850203947623571 accuracy: 98.495\n",
      "\n",
      "Test set: Average loss: 0.0414, Accuracy: 9871/10000 (99%)\n",
      "\n",
      "Training - Epoch: 25 loss: 0.04921907806396485 accuracy: 98.525\n",
      "\n",
      "Test set: Average loss: 0.0382, Accuracy: 9881/10000 (99%)\n",
      "\n",
      "Training - Epoch: 26 loss: 0.047461821528275806 accuracy: 98.58\n",
      "\n",
      "Test set: Average loss: 0.0378, Accuracy: 9889/10000 (99%)\n",
      "\n",
      "Training - Epoch: 27 loss: 0.0478927095691363 accuracy: 98.565\n",
      "\n",
      "Test set: Average loss: 0.0384, Accuracy: 9885/10000 (99%)\n",
      "\n",
      "Training - Epoch: 28 loss: 0.046201550388336184 accuracy: 98.61166666666666\n",
      "\n",
      "Test set: Average loss: 0.0390, Accuracy: 9884/10000 (99%)\n",
      "\n",
      "Training - Epoch: 29 loss: 0.04763122250636419 accuracy: 98.58166666666666\n",
      "\n",
      "Test set: Average loss: 0.0374, Accuracy: 9891/10000 (99%)\n",
      "\n",
      "Training - Epoch: 30 loss: 0.04741876672108968 accuracy: 98.57333333333334\n",
      "\n",
      "Test set: Average loss: 0.0386, Accuracy: 9885/10000 (99%)\n",
      "\n",
      "Training - Epoch: 31 loss: 0.04677501847147941 accuracy: 98.58333333333333\n",
      "\n",
      "Test set: Average loss: 0.0372, Accuracy: 9889/10000 (99%)\n",
      "\n",
      "Training - Epoch: 32 loss: 0.046475289996465045 accuracy: 98.59666666666666\n",
      "\n",
      "Test set: Average loss: 0.0394, Accuracy: 9882/10000 (99%)\n",
      "\n",
      "Training - Epoch: 33 loss: 0.04669518600702286 accuracy: 98.60333333333334\n",
      "\n",
      "Test set: Average loss: 0.0384, Accuracy: 9884/10000 (99%)\n",
      "\n",
      "Training - Epoch: 34 loss: 0.04630653334458669 accuracy: 98.55333333333333\n",
      "\n",
      "Test set: Average loss: 0.0388, Accuracy: 9890/10000 (99%)\n",
      "\n",
      "Training - Epoch: 35 loss: 0.043730637232462566 accuracy: 98.67\n",
      "\n",
      "Test set: Average loss: 0.0423, Accuracy: 9885/10000 (99%)\n",
      "\n",
      "Training - Epoch: 36 loss: 0.0453154972632726 accuracy: 98.63833333333334\n",
      "\n",
      "Test set: Average loss: 0.0398, Accuracy: 9884/10000 (99%)\n",
      "\n",
      "Training - Epoch: 37 loss: 0.04715595301787059 accuracy: 98.58166666666666\n",
      "\n",
      "Test set: Average loss: 0.0410, Accuracy: 9891/10000 (99%)\n",
      "\n",
      "Training - Epoch: 38 loss: 0.04536630412340164 accuracy: 98.65333333333334\n",
      "\n",
      "Test set: Average loss: 0.0409, Accuracy: 9883/10000 (99%)\n",
      "\n",
      "Training - Epoch: 39 loss: 0.045215330092112226 accuracy: 98.68166666666667\n",
      "\n",
      "Test set: Average loss: 0.0407, Accuracy: 9889/10000 (99%)\n",
      "\n",
      "Training - Epoch: 40 loss: 0.04650872717301051 accuracy: 98.655\n",
      "\n",
      "Test set: Average loss: 0.0404, Accuracy: 9886/10000 (99%)\n",
      "\n",
      "Training - Epoch: 41 loss: 0.04709862607121468 accuracy: 98.575\n",
      "\n",
      "Test set: Average loss: 0.0399, Accuracy: 9884/10000 (99%)\n",
      "\n",
      "Training - Epoch: 42 loss: 0.0457903804063797 accuracy: 98.63833333333334\n",
      "\n",
      "Test set: Average loss: 0.0421, Accuracy: 9877/10000 (99%)\n",
      "\n",
      "Training - Epoch: 43 loss: 0.04714022445281347 accuracy: 98.63333333333334\n",
      "\n",
      "Test set: Average loss: 0.0377, Accuracy: 9890/10000 (99%)\n",
      "\n",
      "Training - Epoch: 44 loss: 0.04573393536011378 accuracy: 98.61333333333333\n",
      "\n",
      "Test set: Average loss: 0.0438, Accuracy: 9881/10000 (99%)\n",
      "\n",
      "Training - Epoch: 45 loss: 0.045585996121168135 accuracy: 98.66166666666666\n",
      "\n",
      "Test set: Average loss: 0.0431, Accuracy: 9889/10000 (99%)\n",
      "\n",
      "Training - Epoch: 46 loss: 0.0493554001112779 accuracy: 98.62\n",
      "\n",
      "Test set: Average loss: 0.0437, Accuracy: 9884/10000 (99%)\n",
      "\n",
      "Training - Epoch: 47 loss: 0.04835681219895681 accuracy: 98.55666666666667\n",
      "\n",
      "Test set: Average loss: 0.0373, Accuracy: 9891/10000 (99%)\n",
      "\n",
      "Training - Epoch: 48 loss: 0.04578549513022105 accuracy: 98.63833333333334\n",
      "\n",
      "Test set: Average loss: 0.0453, Accuracy: 9879/10000 (99%)\n",
      "\n",
      "Training - Epoch: 49 loss: 0.045186934717496234 accuracy: 98.70333333333333\n",
      "\n",
      "Test set: Average loss: 0.0394, Accuracy: 9895/10000 (99%)\n",
      "\n",
      "Training - Epoch: 50 loss: 0.045039733799298605 accuracy: 98.685\n",
      "\n",
      "Test set: Average loss: 0.0386, Accuracy: 9885/10000 (99%)\n",
      "\n",
      "Training - Epoch: 51 loss: 0.045961511611938476 accuracy: 98.62166666666667\n",
      "\n",
      "Test set: Average loss: 0.0456, Accuracy: 9876/10000 (99%)\n",
      "\n",
      "Training - Epoch: 52 loss: 0.04686567460199197 accuracy: 98.64\n",
      "\n",
      "Test set: Average loss: 0.0418, Accuracy: 9893/10000 (99%)\n",
      "\n",
      "Training - Epoch: 53 loss: 0.044132221174240115 accuracy: 98.69833333333334\n",
      "\n",
      "Test set: Average loss: 0.0432, Accuracy: 9894/10000 (99%)\n",
      "\n",
      "Training - Epoch: 54 loss: 0.04866343810558319 accuracy: 98.65333333333334\n",
      "\n",
      "Test set: Average loss: 0.0393, Accuracy: 9883/10000 (99%)\n",
      "\n",
      "Training - Epoch: 55 loss: 0.04611805401245753 accuracy: 98.65333333333334\n",
      "\n",
      "Test set: Average loss: 0.0428, Accuracy: 9881/10000 (99%)\n",
      "\n",
      "Training - Epoch: 56 loss: 0.04577002192139626 accuracy: 98.68\n",
      "\n",
      "Test set: Average loss: 0.0400, Accuracy: 9883/10000 (99%)\n",
      "\n",
      "Training - Epoch: 57 loss: 0.04782350265979767 accuracy: 98.61833333333334\n",
      "\n",
      "Test set: Average loss: 0.0432, Accuracy: 9885/10000 (99%)\n",
      "\n",
      "Training - Epoch: 58 loss: 0.04252265655994415 accuracy: 98.73666666666666\n",
      "\n",
      "Test set: Average loss: 0.0420, Accuracy: 9895/10000 (99%)\n",
      "\n",
      "Training - Epoch: 59 loss: 0.04640068605343501 accuracy: 98.71333333333334\n",
      "\n",
      "Test set: Average loss: 0.0444, Accuracy: 9878/10000 (99%)\n",
      "\n",
      "Training - Epoch: 60 loss: 0.04671353114147981 accuracy: 98.655\n",
      "\n",
      "Test set: Average loss: 0.0419, Accuracy: 9891/10000 (99%)\n",
      "\n",
      "Training - Epoch: 61 loss: 0.04467500198384126 accuracy: 98.69333333333333\n",
      "\n",
      "Test set: Average loss: 0.0425, Accuracy: 9888/10000 (99%)\n",
      "\n",
      "Training - Epoch: 62 loss: 0.04632565762996674 accuracy: 98.62333333333333\n",
      "\n",
      "Test set: Average loss: 0.0432, Accuracy: 9882/10000 (99%)\n",
      "\n",
      "Training - Epoch: 63 loss: 0.0483002876996994 accuracy: 98.59833333333333\n",
      "\n",
      "Test set: Average loss: 0.0435, Accuracy: 9888/10000 (99%)\n",
      "\n",
      "Training - Epoch: 64 loss: 0.04678568654855093 accuracy: 98.70166666666667\n",
      "\n",
      "Test set: Average loss: 0.0437, Accuracy: 9874/10000 (99%)\n",
      "\n",
      "Training - Epoch: 65 loss: 0.04752371874650319 accuracy: 98.63\n",
      "\n",
      "Test set: Average loss: 0.0415, Accuracy: 9886/10000 (99%)\n",
      "\n",
      "Training - Epoch: 66 loss: 0.04780582408308983 accuracy: 98.62166666666667\n",
      "\n",
      "Test set: Average loss: 0.0434, Accuracy: 9882/10000 (99%)\n",
      "\n",
      "Training - Epoch: 67 loss: 0.04537836705843608 accuracy: 98.68166666666667\n",
      "\n",
      "Test set: Average loss: 0.0487, Accuracy: 9886/10000 (99%)\n",
      "\n",
      "Training - Epoch: 68 loss: 0.04880531457265218 accuracy: 98.655\n",
      "\n",
      "Test set: Average loss: 0.0465, Accuracy: 9880/10000 (99%)\n",
      "\n",
      "Training - Epoch: 69 loss: 0.046848489689826966 accuracy: 98.74166666666666\n",
      "\n",
      "Test set: Average loss: 0.0417, Accuracy: 9882/10000 (99%)\n",
      "\n",
      "Training - Epoch: 70 loss: 0.047968176720539726 accuracy: 98.675\n",
      "\n",
      "Test set: Average loss: 0.0400, Accuracy: 9890/10000 (99%)\n",
      "\n",
      "Training - Epoch: 71 loss: 0.04819004129568736 accuracy: 98.60166666666667\n",
      "\n",
      "Test set: Average loss: 0.0469, Accuracy: 9885/10000 (99%)\n",
      "\n",
      "Training - Epoch: 72 loss: 0.04897855788866679 accuracy: 98.56666666666666\n",
      "\n",
      "Test set: Average loss: 0.0402, Accuracy: 9886/10000 (99%)\n",
      "\n",
      "Training - Epoch: 73 loss: 0.049120893836021425 accuracy: 98.68333333333334\n",
      "\n",
      "Test set: Average loss: 0.0437, Accuracy: 9901/10000 (99%)\n",
      "\n",
      "Training - Epoch: 74 loss: 0.04732437772552173 accuracy: 98.63666666666667\n",
      "\n",
      "Test set: Average loss: 0.0440, Accuracy: 9879/10000 (99%)\n",
      "\n",
      "Training - Epoch: 75 loss: 0.044206392153104145 accuracy: 98.71\n",
      "\n",
      "Test set: Average loss: 0.0428, Accuracy: 9891/10000 (99%)\n",
      "\n",
      "Training - Epoch: 76 loss: 0.04913435768087705 accuracy: 98.64166666666667\n",
      "\n",
      "Test set: Average loss: 0.0408, Accuracy: 9890/10000 (99%)\n",
      "\n",
      "Training - Epoch: 77 loss: 0.0517424173116684 accuracy: 98.62\n",
      "\n",
      "Test set: Average loss: 0.0470, Accuracy: 9893/10000 (99%)\n",
      "\n",
      "Training - Epoch: 78 loss: 0.050125088393688205 accuracy: 98.55\n",
      "\n",
      "Test set: Average loss: 0.0442, Accuracy: 9888/10000 (99%)\n",
      "\n",
      "Training - Epoch: 79 loss: 0.047280560994148256 accuracy: 98.62166666666667\n",
      "\n",
      "Test set: Average loss: 0.0442, Accuracy: 9883/10000 (99%)\n",
      "\n",
      "Training - Epoch: 80 loss: 0.050682915834585826 accuracy: 98.59666666666666\n",
      "\n",
      "Test set: Average loss: 0.0448, Accuracy: 9884/10000 (99%)\n",
      "\n",
      "Training - Epoch: 81 loss: 0.04697794343829155 accuracy: 98.69166666666666\n",
      "\n",
      "Test set: Average loss: 0.0463, Accuracy: 9890/10000 (99%)\n",
      "\n",
      "Training - Epoch: 82 loss: 0.04980386253595352 accuracy: 98.58833333333334\n",
      "\n",
      "Test set: Average loss: 0.0444, Accuracy: 9877/10000 (99%)\n",
      "\n",
      "Training - Epoch: 83 loss: 0.04580731586615244 accuracy: 98.735\n",
      "\n",
      "Test set: Average loss: 0.0502, Accuracy: 9882/10000 (99%)\n",
      "\n",
      "Training - Epoch: 84 loss: 0.04482420831521352 accuracy: 98.72166666666666\n",
      "\n",
      "Test set: Average loss: 0.0463, Accuracy: 9880/10000 (99%)\n",
      "\n",
      "Training - Epoch: 85 loss: 0.04977520004908244 accuracy: 98.60833333333333\n",
      "\n",
      "Test set: Average loss: 0.0480, Accuracy: 9884/10000 (99%)\n",
      "\n",
      "Training - Epoch: 86 loss: 0.048557514921824134 accuracy: 98.65166666666667\n",
      "\n",
      "Test set: Average loss: 0.0441, Accuracy: 9888/10000 (99%)\n",
      "\n",
      "Training - Epoch: 87 loss: 0.049995227354764936 accuracy: 98.63833333333334\n",
      "\n",
      "Test set: Average loss: 0.0416, Accuracy: 9891/10000 (99%)\n",
      "\n",
      "Training - Epoch: 88 loss: 0.04878096976280213 accuracy: 98.64666666666666\n",
      "\n",
      "Test set: Average loss: 0.0455, Accuracy: 9872/10000 (99%)\n",
      "\n",
      "Training - Epoch: 89 loss: 0.04946666070620219 accuracy: 98.595\n",
      "\n",
      "Test set: Average loss: 0.0472, Accuracy: 9888/10000 (99%)\n",
      "\n",
      "Training - Epoch: 90 loss: 0.046920092399915056 accuracy: 98.69333333333333\n",
      "\n",
      "Test set: Average loss: 0.0518, Accuracy: 9888/10000 (99%)\n",
      "\n",
      "Training - Epoch: 91 loss: 0.0482391969203949 accuracy: 98.63\n",
      "\n",
      "Test set: Average loss: 0.0484, Accuracy: 9883/10000 (99%)\n",
      "\n",
      "Training - Epoch: 92 loss: 0.049204126119613646 accuracy: 98.66833333333334\n",
      "\n",
      "Test set: Average loss: 0.0435, Accuracy: 9887/10000 (99%)\n",
      "\n",
      "Training - Epoch: 93 loss: 0.04620269624392192 accuracy: 98.72\n",
      "\n",
      "Test set: Average loss: 0.0455, Accuracy: 9895/10000 (99%)\n",
      "\n",
      "Training - Epoch: 94 loss: 0.05098609790007273 accuracy: 98.54\n",
      "\n",
      "Test set: Average loss: 0.0500, Accuracy: 9886/10000 (99%)\n",
      "\n",
      "Training - Epoch: 95 loss: 0.05023577457666397 accuracy: 98.67833333333333\n",
      "\n",
      "Test set: Average loss: 0.0497, Accuracy: 9882/10000 (99%)\n",
      "\n",
      "Training - Epoch: 96 loss: 0.05204939595858256 accuracy: 98.56\n",
      "\n",
      "Test set: Average loss: 0.0449, Accuracy: 9877/10000 (99%)\n",
      "\n",
      "Training - Epoch: 97 loss: 0.04862858276367187 accuracy: 98.73\n",
      "\n",
      "Test set: Average loss: 0.0532, Accuracy: 9889/10000 (99%)\n",
      "\n",
      "Training - Epoch: 98 loss: 0.049204388658205665 accuracy: 98.68\n",
      "\n",
      "Test set: Average loss: 0.0487, Accuracy: 9889/10000 (99%)\n",
      "\n",
      "Training - Epoch: 99 loss: 0.05224372126261393 accuracy: 98.58666666666667\n",
      "\n",
      "Test set: Average loss: 0.0472, Accuracy: 9888/10000 (99%)\n",
      "\n",
      "Training - Epoch: 100 loss: 0.04940301194588343 accuracy: 98.61166666666666\n",
      "\n",
      "Test set: Average loss: 0.0538, Accuracy: 9882/10000 (99%)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "epoch=100\n",
    "model = CnnNet2().to(device)\n",
    "model2_params = sum(p.numel() for p in model.parameters())\n",
    "optimizer = optim.Adadelta(model.parameters(), lr=1)\n",
    "scheduler = StepLR(optimizer, step_size=1, gamma=0.1)\n",
    "loss = nn.CrossEntropyLoss()\n",
    "train_loss_2, accuracy_train_2, test_loss_2, test_accuracy_2 = returnModelAccAndLoss(model, device, train_loader, test_loader, optimizer, loss, epoch, scheduler)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "834c2890",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training - Epoch: 1 loss: 0.5479920604070028 accuracy: 82.67\n",
      "\n",
      "Test set: Average loss: 0.2395, Accuracy: 9273/10000 (93%)\n",
      "\n",
      "Training - Epoch: 2 loss: 0.2061085606098175 accuracy: 93.87333333333333\n",
      "\n",
      "Test set: Average loss: 0.1354, Accuracy: 9614/10000 (96%)\n",
      "\n",
      "Training - Epoch: 3 loss: 0.14129106918970744 accuracy: 95.83666666666667\n",
      "\n",
      "Test set: Average loss: 0.1048, Accuracy: 9677/10000 (97%)\n",
      "\n",
      "Training - Epoch: 4 loss: 0.11082885449727377 accuracy: 96.72\n",
      "\n",
      "Test set: Average loss: 0.0776, Accuracy: 9765/10000 (98%)\n",
      "\n",
      "Training - Epoch: 5 loss: 0.09333309355576833 accuracy: 97.27666666666667\n",
      "\n",
      "Test set: Average loss: 0.0755, Accuracy: 9780/10000 (98%)\n",
      "\n",
      "Training - Epoch: 6 loss: 0.08540583364963532 accuracy: 97.49333333333334\n",
      "\n",
      "Test set: Average loss: 0.0636, Accuracy: 9789/10000 (98%)\n",
      "\n",
      "Training - Epoch: 7 loss: 0.07727318857510884 accuracy: 97.67166666666667\n",
      "\n",
      "Test set: Average loss: 0.0640, Accuracy: 9795/10000 (98%)\n",
      "\n",
      "Training - Epoch: 8 loss: 0.07072202070951462 accuracy: 97.84666666666666\n",
      "\n",
      "Test set: Average loss: 0.0608, Accuracy: 9804/10000 (98%)\n",
      "\n",
      "Training - Epoch: 9 loss: 0.06864240485827128 accuracy: 97.93166666666667\n",
      "\n",
      "Test set: Average loss: 0.0568, Accuracy: 9815/10000 (98%)\n",
      "\n",
      "Training - Epoch: 10 loss: 0.06548535973230998 accuracy: 98.005\n",
      "\n",
      "Test set: Average loss: 0.0542, Accuracy: 9815/10000 (98%)\n",
      "\n",
      "Training - Epoch: 11 loss: 0.061688683303197225 accuracy: 98.095\n",
      "\n",
      "Test set: Average loss: 0.0536, Accuracy: 9824/10000 (98%)\n",
      "\n",
      "Training - Epoch: 12 loss: 0.05915892933209737 accuracy: 98.20833333333333\n",
      "\n",
      "Test set: Average loss: 0.0519, Accuracy: 9832/10000 (98%)\n",
      "\n",
      "Training - Epoch: 13 loss: 0.057919187597433724 accuracy: 98.23833333333333\n",
      "\n",
      "Test set: Average loss: 0.0508, Accuracy: 9831/10000 (98%)\n",
      "\n",
      "Training - Epoch: 14 loss: 0.05614614229996999 accuracy: 98.27\n",
      "\n",
      "Test set: Average loss: 0.0524, Accuracy: 9829/10000 (98%)\n",
      "\n",
      "Training - Epoch: 15 loss: 0.05430058902104696 accuracy: 98.35166666666667\n",
      "\n",
      "Test set: Average loss: 0.0486, Accuracy: 9838/10000 (98%)\n",
      "\n",
      "Training - Epoch: 16 loss: 0.051617201364040376 accuracy: 98.405\n",
      "\n",
      "Test set: Average loss: 0.0481, Accuracy: 9842/10000 (98%)\n",
      "\n",
      "Training - Epoch: 17 loss: 0.05031097911993662 accuracy: 98.43\n",
      "\n",
      "Test set: Average loss: 0.0480, Accuracy: 9840/10000 (98%)\n",
      "\n",
      "Training - Epoch: 18 loss: 0.049719765011469526 accuracy: 98.48666666666666\n",
      "\n",
      "Test set: Average loss: 0.0518, Accuracy: 9831/10000 (98%)\n",
      "\n",
      "Training - Epoch: 19 loss: 0.04886262162526449 accuracy: 98.495\n",
      "\n",
      "Test set: Average loss: 0.0482, Accuracy: 9842/10000 (98%)\n",
      "\n",
      "Training - Epoch: 20 loss: 0.046073671213785805 accuracy: 98.59833333333333\n",
      "\n",
      "Test set: Average loss: 0.0472, Accuracy: 9845/10000 (98%)\n",
      "\n",
      "Training - Epoch: 21 loss: 0.04662847314675649 accuracy: 98.56666666666666\n",
      "\n",
      "Test set: Average loss: 0.0487, Accuracy: 9841/10000 (98%)\n",
      "\n",
      "Training - Epoch: 22 loss: 0.04456577418645223 accuracy: 98.575\n",
      "\n",
      "Test set: Average loss: 0.0488, Accuracy: 9840/10000 (98%)\n",
      "\n",
      "Training - Epoch: 23 loss: 0.04469627141157786 accuracy: 98.57166666666667\n",
      "\n",
      "Test set: Average loss: 0.0465, Accuracy: 9843/10000 (98%)\n",
      "\n",
      "Training - Epoch: 24 loss: 0.04236397620836894 accuracy: 98.69166666666666\n",
      "\n",
      "Test set: Average loss: 0.0459, Accuracy: 9853/10000 (99%)\n",
      "\n",
      "Training - Epoch: 25 loss: 0.042313513278961185 accuracy: 98.68666666666667\n",
      "\n",
      "Test set: Average loss: 0.0466, Accuracy: 9849/10000 (98%)\n",
      "\n",
      "Training - Epoch: 26 loss: 0.04146439255028963 accuracy: 98.74666666666667\n",
      "\n",
      "Test set: Average loss: 0.0463, Accuracy: 9846/10000 (98%)\n",
      "\n",
      "Training - Epoch: 27 loss: 0.040637774030367536 accuracy: 98.69666666666667\n",
      "\n",
      "Test set: Average loss: 0.0468, Accuracy: 9848/10000 (98%)\n",
      "\n",
      "Training - Epoch: 28 loss: 0.04075268467863401 accuracy: 98.70333333333333\n",
      "\n",
      "Test set: Average loss: 0.0462, Accuracy: 9849/10000 (98%)\n",
      "\n",
      "Training - Epoch: 29 loss: 0.03978984972635905 accuracy: 98.73666666666666\n",
      "\n",
      "Test set: Average loss: 0.0466, Accuracy: 9853/10000 (99%)\n",
      "\n",
      "Training - Epoch: 30 loss: 0.039161121491591136 accuracy: 98.70833333333333\n",
      "\n",
      "Test set: Average loss: 0.0481, Accuracy: 9835/10000 (98%)\n",
      "\n",
      "Training - Epoch: 31 loss: 0.039645639463265735 accuracy: 98.71833333333333\n",
      "\n",
      "Test set: Average loss: 0.0463, Accuracy: 9846/10000 (98%)\n",
      "\n",
      "Training - Epoch: 32 loss: 0.038173236592610675 accuracy: 98.775\n",
      "\n",
      "Test set: Average loss: 0.0485, Accuracy: 9848/10000 (98%)\n",
      "\n",
      "Training - Epoch: 33 loss: 0.0383365093489488 accuracy: 98.82666666666667\n",
      "\n",
      "Test set: Average loss: 0.0454, Accuracy: 9848/10000 (98%)\n",
      "\n",
      "Training - Epoch: 34 loss: 0.03690998287200928 accuracy: 98.87833333333333\n",
      "\n",
      "Test set: Average loss: 0.0469, Accuracy: 9849/10000 (98%)\n",
      "\n",
      "Training - Epoch: 35 loss: 0.03730461739301681 accuracy: 98.78333333333333\n",
      "\n",
      "Test set: Average loss: 0.0459, Accuracy: 9846/10000 (98%)\n",
      "\n",
      "Training - Epoch: 36 loss: 0.0368080496152242 accuracy: 98.835\n",
      "\n",
      "Test set: Average loss: 0.0458, Accuracy: 9852/10000 (99%)\n",
      "\n",
      "Training - Epoch: 37 loss: 0.03578194374243419 accuracy: 98.88666666666667\n",
      "\n",
      "Test set: Average loss: 0.0462, Accuracy: 9853/10000 (99%)\n",
      "\n",
      "Training - Epoch: 38 loss: 0.03518079580863317 accuracy: 98.895\n",
      "\n",
      "Test set: Average loss: 0.0484, Accuracy: 9839/10000 (98%)\n",
      "\n",
      "Training - Epoch: 39 loss: 0.03549124359289805 accuracy: 98.84833333333333\n",
      "\n",
      "Test set: Average loss: 0.0458, Accuracy: 9845/10000 (98%)\n",
      "\n",
      "Training - Epoch: 40 loss: 0.035248597641785936 accuracy: 98.86833333333334\n",
      "\n",
      "Test set: Average loss: 0.0455, Accuracy: 9854/10000 (99%)\n",
      "\n",
      "Training - Epoch: 41 loss: 0.035428244892756144 accuracy: 98.85666666666667\n",
      "\n",
      "Test set: Average loss: 0.0458, Accuracy: 9847/10000 (98%)\n",
      "\n",
      "Training - Epoch: 42 loss: 0.0347630214770635 accuracy: 98.87666666666667\n",
      "\n",
      "Test set: Average loss: 0.0449, Accuracy: 9853/10000 (99%)\n",
      "\n",
      "Training - Epoch: 43 loss: 0.03402830471992493 accuracy: 98.88666666666667\n",
      "\n",
      "Test set: Average loss: 0.0465, Accuracy: 9851/10000 (99%)\n",
      "\n",
      "Training - Epoch: 44 loss: 0.03392879303296407 accuracy: 98.88666666666667\n",
      "\n",
      "Test set: Average loss: 0.0451, Accuracy: 9846/10000 (98%)\n",
      "\n",
      "Training - Epoch: 45 loss: 0.033735739932457605 accuracy: 98.92666666666666\n",
      "\n",
      "Test set: Average loss: 0.0449, Accuracy: 9860/10000 (99%)\n",
      "\n",
      "Training - Epoch: 46 loss: 0.032298964635531106 accuracy: 98.93333333333334\n",
      "\n",
      "Test set: Average loss: 0.0461, Accuracy: 9846/10000 (98%)\n",
      "\n",
      "Training - Epoch: 47 loss: 0.033735575878620146 accuracy: 98.96166666666667\n",
      "\n",
      "Test set: Average loss: 0.0456, Accuracy: 9854/10000 (99%)\n",
      "\n",
      "Training - Epoch: 48 loss: 0.032932390824953715 accuracy: 98.99666666666667\n",
      "\n",
      "Test set: Average loss: 0.0465, Accuracy: 9857/10000 (99%)\n",
      "\n",
      "Training - Epoch: 49 loss: 0.031653428762654465 accuracy: 98.95666666666666\n",
      "\n",
      "Test set: Average loss: 0.0467, Accuracy: 9854/10000 (99%)\n",
      "\n",
      "Training - Epoch: 50 loss: 0.032927199109395344 accuracy: 98.85166666666667\n",
      "\n",
      "Test set: Average loss: 0.0450, Accuracy: 9854/10000 (99%)\n",
      "\n",
      "Training - Epoch: 51 loss: 0.030563653786977132 accuracy: 99.00666666666666\n",
      "\n",
      "Test set: Average loss: 0.0464, Accuracy: 9858/10000 (99%)\n",
      "\n",
      "Training - Epoch: 52 loss: 0.03253435329198837 accuracy: 98.95\n",
      "\n",
      "Test set: Average loss: 0.0466, Accuracy: 9851/10000 (99%)\n",
      "\n",
      "Training - Epoch: 53 loss: 0.03129181131521861 accuracy: 98.96833333333333\n",
      "\n",
      "Test set: Average loss: 0.0456, Accuracy: 9860/10000 (99%)\n",
      "\n",
      "Training - Epoch: 54 loss: 0.030683123403787614 accuracy: 99.00333333333333\n",
      "\n",
      "Test set: Average loss: 0.0459, Accuracy: 9856/10000 (99%)\n",
      "\n",
      "Training - Epoch: 55 loss: 0.03165432036320368 accuracy: 98.985\n",
      "\n",
      "Test set: Average loss: 0.0453, Accuracy: 9860/10000 (99%)\n",
      "\n",
      "Training - Epoch: 56 loss: 0.031172340885798136 accuracy: 98.97833333333334\n",
      "\n",
      "Test set: Average loss: 0.0464, Accuracy: 9857/10000 (99%)\n",
      "\n",
      "Training - Epoch: 57 loss: 0.030541110296050708 accuracy: 99.01833333333333\n",
      "\n",
      "Test set: Average loss: 0.0465, Accuracy: 9848/10000 (98%)\n",
      "\n",
      "Training - Epoch: 58 loss: 0.030561678385734557 accuracy: 99.01\n",
      "\n",
      "Test set: Average loss: 0.0469, Accuracy: 9856/10000 (99%)\n",
      "\n",
      "Training - Epoch: 59 loss: 0.029533942763010662 accuracy: 99.02166666666666\n",
      "\n",
      "Test set: Average loss: 0.0460, Accuracy: 9850/10000 (98%)\n",
      "\n",
      "Training - Epoch: 60 loss: 0.030188646757602692 accuracy: 99.05166666666666\n",
      "\n",
      "Test set: Average loss: 0.0466, Accuracy: 9851/10000 (99%)\n",
      "\n",
      "Training - Epoch: 61 loss: 0.029405465720097224 accuracy: 99.045\n",
      "\n",
      "Test set: Average loss: 0.0470, Accuracy: 9850/10000 (98%)\n",
      "\n",
      "Training - Epoch: 62 loss: 0.029652434517939884 accuracy: 99.03166666666667\n",
      "\n",
      "Test set: Average loss: 0.0463, Accuracy: 9857/10000 (99%)\n",
      "\n",
      "Training - Epoch: 63 loss: 0.029236521220207215 accuracy: 99.03833333333333\n",
      "\n",
      "Test set: Average loss: 0.0468, Accuracy: 9848/10000 (98%)\n",
      "\n",
      "Training - Epoch: 64 loss: 0.029602594663699466 accuracy: 99.02333333333333\n",
      "\n",
      "Test set: Average loss: 0.0464, Accuracy: 9864/10000 (99%)\n",
      "\n",
      "Training - Epoch: 65 loss: 0.028435765159130096 accuracy: 99.07833333333333\n",
      "\n",
      "Test set: Average loss: 0.0470, Accuracy: 9852/10000 (99%)\n",
      "\n",
      "Training - Epoch: 66 loss: 0.0295898983279864 accuracy: 99.03\n",
      "\n",
      "Test set: Average loss: 0.0472, Accuracy: 9852/10000 (99%)\n",
      "\n",
      "Training - Epoch: 67 loss: 0.028485714399814607 accuracy: 99.10166666666667\n",
      "\n",
      "Test set: Average loss: 0.0486, Accuracy: 9857/10000 (99%)\n",
      "\n",
      "Training - Epoch: 68 loss: 0.02763902444044749 accuracy: 99.09666666666666\n",
      "\n",
      "Test set: Average loss: 0.0489, Accuracy: 9855/10000 (99%)\n",
      "\n",
      "Training - Epoch: 69 loss: 0.027739477026462556 accuracy: 99.13333333333334\n",
      "\n",
      "Test set: Average loss: 0.0476, Accuracy: 9852/10000 (99%)\n",
      "\n",
      "Training - Epoch: 70 loss: 0.027266462775071464 accuracy: 99.105\n",
      "\n",
      "Test set: Average loss: 0.0479, Accuracy: 9853/10000 (99%)\n",
      "\n",
      "Training - Epoch: 71 loss: 0.027549273931980135 accuracy: 99.07166666666667\n",
      "\n",
      "Test set: Average loss: 0.0482, Accuracy: 9847/10000 (98%)\n",
      "\n",
      "Training - Epoch: 72 loss: 0.0274683958530426 accuracy: 99.1\n",
      "\n",
      "Test set: Average loss: 0.0497, Accuracy: 9851/10000 (99%)\n",
      "\n",
      "Training - Epoch: 73 loss: 0.02816176343758901 accuracy: 99.08\n",
      "\n",
      "Test set: Average loss: 0.0496, Accuracy: 9854/10000 (99%)\n",
      "\n",
      "Training - Epoch: 74 loss: 0.026877421215176583 accuracy: 99.12\n",
      "\n",
      "Test set: Average loss: 0.0476, Accuracy: 9856/10000 (99%)\n",
      "\n",
      "Training - Epoch: 75 loss: 0.026106911158561705 accuracy: 99.17833333333333\n",
      "\n",
      "Test set: Average loss: 0.0485, Accuracy: 9852/10000 (99%)\n",
      "\n",
      "Training - Epoch: 76 loss: 0.02630257665316264 accuracy: 99.13666666666667\n",
      "\n",
      "Test set: Average loss: 0.0478, Accuracy: 9856/10000 (99%)\n",
      "\n",
      "Training - Epoch: 77 loss: 0.0256756761799256 accuracy: 99.175\n",
      "\n",
      "Test set: Average loss: 0.0485, Accuracy: 9852/10000 (99%)\n",
      "\n",
      "Training - Epoch: 78 loss: 0.026416213957468668 accuracy: 99.135\n",
      "\n",
      "Test set: Average loss: 0.0491, Accuracy: 9861/10000 (99%)\n",
      "\n",
      "Training - Epoch: 79 loss: 0.026502076502641043 accuracy: 99.155\n",
      "\n",
      "Test set: Average loss: 0.0487, Accuracy: 9856/10000 (99%)\n",
      "\n",
      "Training - Epoch: 80 loss: 0.026522698307037355 accuracy: 99.13833333333334\n",
      "\n",
      "Test set: Average loss: 0.0488, Accuracy: 9855/10000 (99%)\n",
      "\n",
      "Training - Epoch: 81 loss: 0.025902113020420073 accuracy: 99.13\n",
      "\n",
      "Test set: Average loss: 0.0490, Accuracy: 9856/10000 (99%)\n",
      "\n",
      "Training - Epoch: 82 loss: 0.026346123874684175 accuracy: 99.15166666666667\n",
      "\n",
      "Test set: Average loss: 0.0488, Accuracy: 9853/10000 (99%)\n",
      "\n",
      "Training - Epoch: 83 loss: 0.026390081477165223 accuracy: 99.11\n",
      "\n",
      "Test set: Average loss: 0.0493, Accuracy: 9854/10000 (99%)\n",
      "\n",
      "Training - Epoch: 84 loss: 0.0263256023367246 accuracy: 99.135\n",
      "\n",
      "Test set: Average loss: 0.0475, Accuracy: 9863/10000 (99%)\n",
      "\n",
      "Training - Epoch: 85 loss: 0.02621463497877121 accuracy: 99.10833333333333\n",
      "\n",
      "Test set: Average loss: 0.0478, Accuracy: 9860/10000 (99%)\n",
      "\n",
      "Training - Epoch: 86 loss: 0.02617566406528155 accuracy: 99.175\n",
      "\n",
      "Test set: Average loss: 0.0482, Accuracy: 9862/10000 (99%)\n",
      "\n",
      "Training - Epoch: 87 loss: 0.024607228150963785 accuracy: 99.19666666666667\n",
      "\n",
      "Test set: Average loss: 0.0495, Accuracy: 9862/10000 (99%)\n",
      "\n",
      "Training - Epoch: 88 loss: 0.025847781825065613 accuracy: 99.135\n",
      "\n",
      "Test set: Average loss: 0.0496, Accuracy: 9855/10000 (99%)\n",
      "\n",
      "Training - Epoch: 89 loss: 0.0259057740042607 accuracy: 99.11666666666666\n",
      "\n",
      "Test set: Average loss: 0.0484, Accuracy: 9855/10000 (99%)\n",
      "\n",
      "Training - Epoch: 90 loss: 0.026071759853760403 accuracy: 99.15166666666667\n",
      "\n",
      "Test set: Average loss: 0.0491, Accuracy: 9858/10000 (99%)\n",
      "\n",
      "Training - Epoch: 91 loss: 0.025445820554097495 accuracy: 99.16\n",
      "\n",
      "Test set: Average loss: 0.0490, Accuracy: 9863/10000 (99%)\n",
      "\n",
      "Training - Epoch: 92 loss: 0.024693090014159678 accuracy: 99.175\n",
      "\n",
      "Test set: Average loss: 0.0489, Accuracy: 9856/10000 (99%)\n",
      "\n",
      "Training - Epoch: 93 loss: 0.02365691258907318 accuracy: 99.22833333333334\n",
      "\n",
      "Test set: Average loss: 0.0502, Accuracy: 9852/10000 (99%)\n",
      "\n",
      "Training - Epoch: 94 loss: 0.02398725203871727 accuracy: 99.2\n",
      "\n",
      "Test set: Average loss: 0.0495, Accuracy: 9864/10000 (99%)\n",
      "\n",
      "Training - Epoch: 95 loss: 0.025290883070230485 accuracy: 99.17333333333333\n",
      "\n",
      "Test set: Average loss: 0.0486, Accuracy: 9853/10000 (99%)\n",
      "\n",
      "Training - Epoch: 96 loss: 0.02445994428793589 accuracy: 99.20333333333333\n",
      "\n",
      "Test set: Average loss: 0.0490, Accuracy: 9856/10000 (99%)\n",
      "\n",
      "Training - Epoch: 97 loss: 0.02570615226427714 accuracy: 99.18\n",
      "\n",
      "Test set: Average loss: 0.0494, Accuracy: 9858/10000 (99%)\n",
      "\n",
      "Training - Epoch: 98 loss: 0.024658669300874073 accuracy: 99.18166666666667\n",
      "\n",
      "Test set: Average loss: 0.0496, Accuracy: 9852/10000 (99%)\n",
      "\n",
      "Training - Epoch: 99 loss: 0.02319172476331393 accuracy: 99.25\n",
      "\n",
      "Test set: Average loss: 0.0514, Accuracy: 9855/10000 (99%)\n",
      "\n",
      "Training - Epoch: 100 loss: 0.024357542470594247 accuracy: 99.22\n",
      "\n",
      "Test set: Average loss: 0.0498, Accuracy: 9855/10000 (99%)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "epoch=100\n",
    "model = CnnNet3().to(device)\n",
    "model3_params = sum(p.numel() for p in model.parameters())\n",
    "optimizer = optim.Adadelta(model.parameters(), lr=1)\n",
    "scheduler = StepLR(optimizer, step_size=1, gamma=0.1)\n",
    "loss = nn.CrossEntropyLoss()\n",
    "train_loss_3, accuracy_train_3, test_loss_3, test_accuracy_3 = returnModelAccAndLoss(model, device, train_loader, test_loader, optimizer, loss, epoch, scheduler)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "43c40c78",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training - Epoch: 1 loss: 0.5980066206296285 accuracy: 80.57333333333334\n",
      "\n",
      "Test set: Average loss: 0.1276, Accuracy: 9614/10000 (96%)\n",
      "\n",
      "Training - Epoch: 2 loss: 0.16549663623174032 accuracy: 94.91333333333333\n",
      "\n",
      "Test set: Average loss: 0.0738, Accuracy: 9758/10000 (98%)\n",
      "\n",
      "Training - Epoch: 3 loss: 0.10732036418914795 accuracy: 96.775\n",
      "\n",
      "Test set: Average loss: 0.0571, Accuracy: 9817/10000 (98%)\n",
      "\n",
      "Training - Epoch: 4 loss: 0.08221105495889981 accuracy: 97.525\n",
      "\n",
      "Test set: Average loss: 0.0474, Accuracy: 9836/10000 (98%)\n",
      "\n",
      "Training - Epoch: 5 loss: 0.06837663218975067 accuracy: 97.90166666666667\n",
      "\n",
      "Test set: Average loss: 0.0397, Accuracy: 9871/10000 (99%)\n",
      "\n",
      "Training - Epoch: 6 loss: 0.055878142638504505 accuracy: 98.285\n",
      "\n",
      "Test set: Average loss: 0.0369, Accuracy: 9876/10000 (99%)\n",
      "\n",
      "Training - Epoch: 7 loss: 0.04839512972831726 accuracy: 98.495\n",
      "\n",
      "Test set: Average loss: 0.0348, Accuracy: 9889/10000 (99%)\n",
      "\n",
      "Training - Epoch: 8 loss: 0.04306376005411148 accuracy: 98.665\n",
      "\n",
      "Test set: Average loss: 0.0361, Accuracy: 9875/10000 (99%)\n",
      "\n",
      "Training - Epoch: 9 loss: 0.03799578773180644 accuracy: 98.79833333333333\n",
      "\n",
      "Test set: Average loss: 0.0301, Accuracy: 9902/10000 (99%)\n",
      "\n",
      "Training - Epoch: 10 loss: 0.0346313325881958 accuracy: 98.91666666666667\n",
      "\n",
      "Test set: Average loss: 0.0336, Accuracy: 9892/10000 (99%)\n",
      "\n",
      "Training - Epoch: 11 loss: 0.031607895441850026 accuracy: 99.02\n",
      "\n",
      "Test set: Average loss: 0.0313, Accuracy: 9901/10000 (99%)\n",
      "\n",
      "Training - Epoch: 12 loss: 0.028130688627560935 accuracy: 99.08\n",
      "\n",
      "Test set: Average loss: 0.0343, Accuracy: 9892/10000 (99%)\n",
      "\n",
      "Training - Epoch: 13 loss: 0.02637235178053379 accuracy: 99.14166666666667\n",
      "\n",
      "Test set: Average loss: 0.0297, Accuracy: 9903/10000 (99%)\n",
      "\n",
      "Training - Epoch: 14 loss: 0.024994129618008933 accuracy: 99.22\n",
      "\n",
      "Test set: Average loss: 0.0306, Accuracy: 9911/10000 (99%)\n",
      "\n",
      "Training - Epoch: 15 loss: 0.024677813363075257 accuracy: 99.2\n",
      "\n",
      "Test set: Average loss: 0.0311, Accuracy: 9906/10000 (99%)\n",
      "\n",
      "Training - Epoch: 16 loss: 0.022993475274244946 accuracy: 99.26333333333334\n",
      "\n",
      "Test set: Average loss: 0.0329, Accuracy: 9893/10000 (99%)\n",
      "\n",
      "Training - Epoch: 17 loss: 0.021021569561958314 accuracy: 99.35166666666667\n",
      "\n",
      "Test set: Average loss: 0.0344, Accuracy: 9896/10000 (99%)\n",
      "\n",
      "Training - Epoch: 18 loss: 0.02113667845726013 accuracy: 99.30333333333333\n",
      "\n",
      "Test set: Average loss: 0.0297, Accuracy: 9912/10000 (99%)\n",
      "\n",
      "Training - Epoch: 19 loss: 0.021119960757096608 accuracy: 99.33666666666667\n",
      "\n",
      "Test set: Average loss: 0.0324, Accuracy: 9907/10000 (99%)\n",
      "\n",
      "Training - Epoch: 20 loss: 0.01979821012665828 accuracy: 99.35166666666667\n",
      "\n",
      "Test set: Average loss: 0.0324, Accuracy: 9909/10000 (99%)\n",
      "\n",
      "Training - Epoch: 21 loss: 0.020706178828080495 accuracy: 99.34\n",
      "\n",
      "Test set: Average loss: 0.0321, Accuracy: 9903/10000 (99%)\n",
      "\n",
      "Training - Epoch: 22 loss: 0.019750011066595713 accuracy: 99.365\n",
      "\n",
      "Test set: Average loss: 0.0303, Accuracy: 9911/10000 (99%)\n",
      "\n",
      "Training - Epoch: 23 loss: 0.01789253006192545 accuracy: 99.42166666666667\n",
      "\n",
      "Test set: Average loss: 0.0330, Accuracy: 9904/10000 (99%)\n",
      "\n",
      "Training - Epoch: 24 loss: 0.019072318922479948 accuracy: 99.42\n",
      "\n",
      "Test set: Average loss: 0.0312, Accuracy: 9911/10000 (99%)\n",
      "\n",
      "Training - Epoch: 25 loss: 0.016788885811964673 accuracy: 99.48333333333333\n",
      "\n",
      "Test set: Average loss: 0.0375, Accuracy: 9897/10000 (99%)\n",
      "\n",
      "Training - Epoch: 26 loss: 0.016423540118336678 accuracy: 99.43833333333333\n",
      "\n",
      "Test set: Average loss: 0.0333, Accuracy: 9907/10000 (99%)\n",
      "\n",
      "Training - Epoch: 27 loss: 0.01789480486754328 accuracy: 99.38666666666667\n",
      "\n",
      "Test set: Average loss: 0.0370, Accuracy: 9903/10000 (99%)\n",
      "\n",
      "Training - Epoch: 28 loss: 0.015294727195302645 accuracy: 99.49833333333333\n",
      "\n",
      "Test set: Average loss: 0.0354, Accuracy: 9905/10000 (99%)\n",
      "\n",
      "Training - Epoch: 29 loss: 0.017222660466035208 accuracy: 99.44666666666667\n",
      "\n",
      "Test set: Average loss: 0.0340, Accuracy: 9908/10000 (99%)\n",
      "\n",
      "Training - Epoch: 30 loss: 0.017244598732391994 accuracy: 99.44666666666667\n",
      "\n",
      "Test set: Average loss: 0.0365, Accuracy: 9913/10000 (99%)\n",
      "\n",
      "Training - Epoch: 31 loss: 0.017505707134803136 accuracy: 99.44333333333333\n",
      "\n",
      "Test set: Average loss: 0.0343, Accuracy: 9904/10000 (99%)\n",
      "\n",
      "Training - Epoch: 32 loss: 0.014821151630580426 accuracy: 99.5\n",
      "\n",
      "Test set: Average loss: 0.0337, Accuracy: 9908/10000 (99%)\n",
      "\n",
      "Training - Epoch: 33 loss: 0.01727984176501632 accuracy: 99.46\n",
      "\n",
      "Test set: Average loss: 0.0340, Accuracy: 9915/10000 (99%)\n",
      "\n",
      "Training - Epoch: 34 loss: 0.01563813642623524 accuracy: 99.52833333333334\n",
      "\n",
      "Test set: Average loss: 0.0352, Accuracy: 9911/10000 (99%)\n",
      "\n",
      "Training - Epoch: 35 loss: 0.016811480736732484 accuracy: 99.46\n",
      "\n",
      "Test set: Average loss: 0.0347, Accuracy: 9903/10000 (99%)\n",
      "\n",
      "Training - Epoch: 36 loss: 0.017441104557116827 accuracy: 99.45166666666667\n",
      "\n",
      "Test set: Average loss: 0.0397, Accuracy: 9905/10000 (99%)\n",
      "\n",
      "Training - Epoch: 37 loss: 0.016361664574282866 accuracy: 99.475\n",
      "\n",
      "Test set: Average loss: 0.0393, Accuracy: 9911/10000 (99%)\n",
      "\n",
      "Training - Epoch: 38 loss: 0.01826868801911672 accuracy: 99.47333333333333\n",
      "\n",
      "Test set: Average loss: 0.0376, Accuracy: 9906/10000 (99%)\n",
      "\n",
      "Training - Epoch: 39 loss: 0.018186983724435172 accuracy: 99.42166666666667\n",
      "\n",
      "Test set: Average loss: 0.0371, Accuracy: 9903/10000 (99%)\n",
      "\n",
      "Training - Epoch: 40 loss: 0.016583610193928084 accuracy: 99.485\n",
      "\n",
      "Test set: Average loss: 0.0355, Accuracy: 9916/10000 (99%)\n",
      "\n",
      "Training - Epoch: 41 loss: 0.017541582611948252 accuracy: 99.48666666666666\n",
      "\n",
      "Test set: Average loss: 0.0389, Accuracy: 9905/10000 (99%)\n",
      "\n",
      "Training - Epoch: 42 loss: 0.017187803070743878 accuracy: 99.50333333333333\n",
      "\n",
      "Test set: Average loss: 0.0412, Accuracy: 9912/10000 (99%)\n",
      "\n",
      "Training - Epoch: 43 loss: 0.017312377446889876 accuracy: 99.47666666666667\n",
      "\n",
      "Test set: Average loss: 0.0388, Accuracy: 9888/10000 (99%)\n",
      "\n",
      "Training - Epoch: 44 loss: 0.01805269668661058 accuracy: 99.48666666666666\n",
      "\n",
      "Test set: Average loss: 0.0384, Accuracy: 9905/10000 (99%)\n",
      "\n",
      "Training - Epoch: 45 loss: 0.01852659798959891 accuracy: 99.485\n",
      "\n",
      "Test set: Average loss: 0.0397, Accuracy: 9902/10000 (99%)\n",
      "\n",
      "Training - Epoch: 46 loss: 0.018294411473472914 accuracy: 99.50166666666667\n",
      "\n",
      "Test set: Average loss: 0.0399, Accuracy: 9903/10000 (99%)\n",
      "\n",
      "Training - Epoch: 47 loss: 0.018556836734712125 accuracy: 99.43666666666667\n",
      "\n",
      "Test set: Average loss: 0.0389, Accuracy: 9906/10000 (99%)\n",
      "\n",
      "Training - Epoch: 48 loss: 0.018335903032496572 accuracy: 99.43166666666667\n",
      "\n",
      "Test set: Average loss: 0.0443, Accuracy: 9907/10000 (99%)\n",
      "\n",
      "Training - Epoch: 49 loss: 0.01866208998411894 accuracy: 99.48666666666666\n",
      "\n",
      "Test set: Average loss: 0.0472, Accuracy: 9899/10000 (99%)\n",
      "\n",
      "Training - Epoch: 50 loss: 0.01790378411213557 accuracy: 99.475\n",
      "\n",
      "Test set: Average loss: 0.0418, Accuracy: 9908/10000 (99%)\n",
      "\n",
      "Training - Epoch: 51 loss: 0.018386789164940517 accuracy: 99.43833333333333\n",
      "\n",
      "Test set: Average loss: 0.0447, Accuracy: 9900/10000 (99%)\n",
      "\n",
      "Training - Epoch: 52 loss: 0.017919357619682948 accuracy: 99.43666666666667\n",
      "\n",
      "Test set: Average loss: 0.0434, Accuracy: 9901/10000 (99%)\n",
      "\n",
      "Training - Epoch: 53 loss: 0.021007695229848227 accuracy: 99.395\n",
      "\n",
      "Test set: Average loss: 0.0412, Accuracy: 9903/10000 (99%)\n",
      "\n",
      "Training - Epoch: 54 loss: 0.016564078078667322 accuracy: 99.48833333333333\n",
      "\n",
      "Test set: Average loss: 0.0434, Accuracy: 9891/10000 (99%)\n",
      "\n",
      "Training - Epoch: 55 loss: 0.0196540928820769 accuracy: 99.4\n",
      "\n",
      "Test set: Average loss: 0.0431, Accuracy: 9905/10000 (99%)\n",
      "\n",
      "Training - Epoch: 56 loss: 0.018542542269391317 accuracy: 99.47666666666667\n",
      "\n",
      "Test set: Average loss: 0.0440, Accuracy: 9910/10000 (99%)\n",
      "\n",
      "Training - Epoch: 57 loss: 0.019114554673433305 accuracy: 99.44333333333333\n",
      "\n",
      "Test set: Average loss: 0.0455, Accuracy: 9902/10000 (99%)\n",
      "\n",
      "Training - Epoch: 58 loss: 0.018084028789401053 accuracy: 99.5\n",
      "\n",
      "Test set: Average loss: 0.0517, Accuracy: 9908/10000 (99%)\n",
      "\n",
      "Training - Epoch: 59 loss: 0.02000257273738583 accuracy: 99.39166666666667\n",
      "\n",
      "Test set: Average loss: 0.0440, Accuracy: 9897/10000 (99%)\n",
      "\n",
      "Training - Epoch: 60 loss: 0.018822238169113794 accuracy: 99.405\n",
      "\n",
      "Test set: Average loss: 0.0451, Accuracy: 9904/10000 (99%)\n",
      "\n",
      "Training - Epoch: 61 loss: 0.019510253473122914 accuracy: 99.425\n",
      "\n",
      "Test set: Average loss: 0.0411, Accuracy: 9894/10000 (99%)\n",
      "\n",
      "Training - Epoch: 62 loss: 0.018579558637738226 accuracy: 99.48\n",
      "\n",
      "Test set: Average loss: 0.0472, Accuracy: 9902/10000 (99%)\n",
      "\n",
      "Training - Epoch: 63 loss: 0.019989268589019775 accuracy: 99.415\n",
      "\n",
      "Test set: Average loss: 0.0457, Accuracy: 9901/10000 (99%)\n",
      "\n",
      "Training - Epoch: 64 loss: 0.020435796822110813 accuracy: 99.44\n",
      "\n",
      "Test set: Average loss: 0.0419, Accuracy: 9910/10000 (99%)\n",
      "\n",
      "Training - Epoch: 65 loss: 0.01898693917095661 accuracy: 99.485\n",
      "\n",
      "Test set: Average loss: 0.0440, Accuracy: 9904/10000 (99%)\n",
      "\n",
      "Training - Epoch: 66 loss: 0.020541274120658638 accuracy: 99.43166666666667\n",
      "\n",
      "Test set: Average loss: 0.0498, Accuracy: 9901/10000 (99%)\n",
      "\n",
      "Training - Epoch: 67 loss: 0.018590081912527482 accuracy: 99.46166666666667\n",
      "\n",
      "Test set: Average loss: 0.0455, Accuracy: 9912/10000 (99%)\n",
      "\n",
      "Training - Epoch: 68 loss: 0.0182360710656542 accuracy: 99.46833333333333\n",
      "\n",
      "Test set: Average loss: 0.0465, Accuracy: 9907/10000 (99%)\n",
      "\n",
      "Training - Epoch: 69 loss: 0.020228431639571984 accuracy: 99.41666666666667\n",
      "\n",
      "Test set: Average loss: 0.0494, Accuracy: 9908/10000 (99%)\n",
      "\n",
      "Training - Epoch: 70 loss: 0.0188709699511528 accuracy: 99.45333333333333\n",
      "\n",
      "Test set: Average loss: 0.0442, Accuracy: 9911/10000 (99%)\n",
      "\n",
      "Training - Epoch: 71 loss: 0.019726680201043685 accuracy: 99.46\n",
      "\n",
      "Test set: Average loss: 0.0501, Accuracy: 9903/10000 (99%)\n",
      "\n",
      "Training - Epoch: 72 loss: 0.019556043885151544 accuracy: 99.44666666666667\n",
      "\n",
      "Test set: Average loss: 0.0449, Accuracy: 9896/10000 (99%)\n",
      "\n",
      "Training - Epoch: 73 loss: 0.020401256013909977 accuracy: 99.41166666666666\n",
      "\n",
      "Test set: Average loss: 0.0483, Accuracy: 9888/10000 (99%)\n",
      "\n",
      "Training - Epoch: 74 loss: 0.020886031882961592 accuracy: 99.42333333333333\n",
      "\n",
      "Test set: Average loss: 0.0484, Accuracy: 9902/10000 (99%)\n",
      "\n",
      "Training - Epoch: 75 loss: 0.018640835418924688 accuracy: 99.43666666666667\n",
      "\n",
      "Test set: Average loss: 0.0480, Accuracy: 9897/10000 (99%)\n",
      "\n",
      "Training - Epoch: 76 loss: 0.01964642595971624 accuracy: 99.47\n",
      "\n",
      "Test set: Average loss: 0.0575, Accuracy: 9907/10000 (99%)\n",
      "\n",
      "Training - Epoch: 77 loss: 0.02040138653417428 accuracy: 99.41166666666666\n",
      "\n",
      "Test set: Average loss: 0.0484, Accuracy: 9908/10000 (99%)\n",
      "\n",
      "Training - Epoch: 78 loss: 0.019212236635386945 accuracy: 99.43833333333333\n",
      "\n",
      "Test set: Average loss: 0.0519, Accuracy: 9893/10000 (99%)\n",
      "\n",
      "Training - Epoch: 79 loss: 0.019613900701204934 accuracy: 99.47166666666666\n",
      "\n",
      "Test set: Average loss: 0.0462, Accuracy: 9894/10000 (99%)\n",
      "\n",
      "Training - Epoch: 80 loss: 0.02020334374671802 accuracy: 99.42666666666666\n",
      "\n",
      "Test set: Average loss: 0.0533, Accuracy: 9899/10000 (99%)\n",
      "\n",
      "Training - Epoch: 81 loss: 0.021103374015788236 accuracy: 99.4\n",
      "\n",
      "Test set: Average loss: 0.0428, Accuracy: 9904/10000 (99%)\n",
      "\n",
      "Training - Epoch: 82 loss: 0.02098338962843021 accuracy: 99.40833333333333\n",
      "\n",
      "Test set: Average loss: 0.0453, Accuracy: 9897/10000 (99%)\n",
      "\n",
      "Training - Epoch: 83 loss: 0.019042269856731096 accuracy: 99.52333333333333\n",
      "\n",
      "Test set: Average loss: 0.0441, Accuracy: 9889/10000 (99%)\n",
      "\n",
      "Training - Epoch: 84 loss: 0.01930754492680232 accuracy: 99.46666666666667\n",
      "\n",
      "Test set: Average loss: 0.0410, Accuracy: 9905/10000 (99%)\n",
      "\n",
      "Training - Epoch: 85 loss: 0.0182302457049489 accuracy: 99.47\n",
      "\n",
      "Test set: Average loss: 0.0462, Accuracy: 9895/10000 (99%)\n",
      "\n",
      "Training - Epoch: 86 loss: 0.019690217850605648 accuracy: 99.44666666666667\n",
      "\n",
      "Test set: Average loss: 0.0515, Accuracy: 9904/10000 (99%)\n",
      "\n",
      "Training - Epoch: 87 loss: 0.022775543287396432 accuracy: 99.37166666666667\n",
      "\n",
      "Test set: Average loss: 0.0523, Accuracy: 9889/10000 (99%)\n",
      "\n",
      "Training - Epoch: 88 loss: 0.021719154690702756 accuracy: 99.38\n",
      "\n",
      "Test set: Average loss: 0.0420, Accuracy: 9905/10000 (99%)\n",
      "\n",
      "Training - Epoch: 89 loss: 0.02104116004407406 accuracy: 99.40666666666667\n",
      "\n",
      "Test set: Average loss: 0.0450, Accuracy: 9913/10000 (99%)\n",
      "\n",
      "Training - Epoch: 90 loss: 0.01883218950331211 accuracy: 99.48\n",
      "\n",
      "Test set: Average loss: 0.0443, Accuracy: 9909/10000 (99%)\n",
      "\n",
      "Training - Epoch: 91 loss: 0.02071316929856936 accuracy: 99.42833333333333\n",
      "\n",
      "Test set: Average loss: 0.0439, Accuracy: 9902/10000 (99%)\n",
      "\n",
      "Training - Epoch: 92 loss: 0.020281456944843134 accuracy: 99.44833333333334\n",
      "\n",
      "Test set: Average loss: 0.0487, Accuracy: 9890/10000 (99%)\n",
      "\n",
      "Training - Epoch: 93 loss: 0.02259663217018048 accuracy: 99.40166666666667\n",
      "\n",
      "Test set: Average loss: 0.0508, Accuracy: 9909/10000 (99%)\n",
      "\n",
      "Training - Epoch: 94 loss: 0.019655141772826512 accuracy: 99.47333333333333\n",
      "\n",
      "Test set: Average loss: 0.0514, Accuracy: 9908/10000 (99%)\n",
      "\n",
      "Training - Epoch: 95 loss: 0.019571354034543038 accuracy: 99.45666666666666\n",
      "\n",
      "Test set: Average loss: 0.0442, Accuracy: 9910/10000 (99%)\n",
      "\n",
      "Training - Epoch: 96 loss: 0.02320327254533768 accuracy: 99.33833333333334\n",
      "\n",
      "Test set: Average loss: 0.0385, Accuracy: 9907/10000 (99%)\n",
      "\n",
      "Training - Epoch: 97 loss: 0.020633265119791033 accuracy: 99.43666666666667\n",
      "\n",
      "Test set: Average loss: 0.0606, Accuracy: 9903/10000 (99%)\n",
      "\n",
      "Training - Epoch: 98 loss: 0.02099434044659138 accuracy: 99.42333333333333\n",
      "\n",
      "Test set: Average loss: 0.0518, Accuracy: 9891/10000 (99%)\n",
      "\n",
      "Training - Epoch: 99 loss: 0.021659483379187685 accuracy: 99.41166666666666\n",
      "\n",
      "Test set: Average loss: 0.0589, Accuracy: 9896/10000 (99%)\n",
      "\n",
      "Training - Epoch: 100 loss: 0.021233269529044627 accuracy: 99.43\n",
      "\n",
      "Test set: Average loss: 0.0485, Accuracy: 9901/10000 (99%)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "epoch=100\n",
    "model = CnnNet4().to(device)\n",
    "model4_params = sum(p.numel() for p in model.parameters())\n",
    "optimizer = optim.Adadelta(model.parameters(), lr=1)\n",
    "scheduler = StepLR(optimizer, step_size=1, gamma=0.1)\n",
    "loss = nn.CrossEntropyLoss()\n",
    "train_loss_4, accuracy_train_4, test_loss_4, test_accuracy_4 = returnModelAccAndLoss(model, device, train_loader, test_loader, optimizer, loss, epoch, scheduler)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "13031d17",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training - Epoch: 1 loss: 0.4065330081701279 accuracy: 87.17833333333333\n",
      "\n",
      "Test set: Average loss: 0.0641, Accuracy: 9792/10000 (98%)\n",
      "\n",
      "Training - Epoch: 2 loss: 0.081998522011439 accuracy: 97.53666666666666\n",
      "\n",
      "Test set: Average loss: 0.0403, Accuracy: 9862/10000 (99%)\n",
      "\n",
      "Training - Epoch: 3 loss: 0.055378232089678445 accuracy: 98.32833333333333\n",
      "\n",
      "Test set: Average loss: 0.0326, Accuracy: 9883/10000 (99%)\n",
      "\n",
      "Training - Epoch: 4 loss: 0.04654177349408468 accuracy: 98.61833333333334\n",
      "\n",
      "Test set: Average loss: 0.0316, Accuracy: 9893/10000 (99%)\n",
      "\n",
      "Training - Epoch: 5 loss: 0.03767487826347351 accuracy: 98.82666666666667\n",
      "\n",
      "Test set: Average loss: 0.0297, Accuracy: 9895/10000 (99%)\n",
      "\n",
      "Training - Epoch: 6 loss: 0.03344199840227763 accuracy: 98.95166666666667\n",
      "\n",
      "Test set: Average loss: 0.0275, Accuracy: 9910/10000 (99%)\n",
      "\n",
      "Training - Epoch: 7 loss: 0.028206373765071232 accuracy: 99.15166666666667\n",
      "\n",
      "Test set: Average loss: 0.0252, Accuracy: 9909/10000 (99%)\n",
      "\n",
      "Training - Epoch: 8 loss: 0.02575851695140203 accuracy: 99.18666666666667\n",
      "\n",
      "Test set: Average loss: 0.0276, Accuracy: 9924/10000 (99%)\n",
      "\n",
      "Training - Epoch: 9 loss: 0.02205428006251653 accuracy: 99.28666666666666\n",
      "\n",
      "Test set: Average loss: 0.0275, Accuracy: 9916/10000 (99%)\n",
      "\n",
      "Training - Epoch: 10 loss: 0.02143873690168063 accuracy: 99.36\n",
      "\n",
      "Test set: Average loss: 0.0262, Accuracy: 9915/10000 (99%)\n",
      "\n",
      "Training - Epoch: 11 loss: 0.01945736025373141 accuracy: 99.35166666666667\n",
      "\n",
      "Test set: Average loss: 0.0284, Accuracy: 9912/10000 (99%)\n",
      "\n",
      "Training - Epoch: 12 loss: 0.01624104018211365 accuracy: 99.46166666666667\n",
      "\n",
      "Test set: Average loss: 0.0265, Accuracy: 9918/10000 (99%)\n",
      "\n",
      "Training - Epoch: 13 loss: 0.01638182696501414 accuracy: 99.48666666666666\n",
      "\n",
      "Test set: Average loss: 0.0272, Accuracy: 9923/10000 (99%)\n",
      "\n",
      "Training - Epoch: 14 loss: 0.014456184418002765 accuracy: 99.54833333333333\n",
      "\n",
      "Test set: Average loss: 0.0254, Accuracy: 9928/10000 (99%)\n",
      "\n",
      "Training - Epoch: 15 loss: 0.013333749737342198 accuracy: 99.56333333333333\n",
      "\n",
      "Test set: Average loss: 0.0276, Accuracy: 9923/10000 (99%)\n",
      "\n",
      "Training - Epoch: 16 loss: 0.011813714878757795 accuracy: 99.62166666666667\n",
      "\n",
      "Test set: Average loss: 0.0299, Accuracy: 9925/10000 (99%)\n",
      "\n",
      "Training - Epoch: 17 loss: 0.012839766339461009 accuracy: 99.605\n",
      "\n",
      "Test set: Average loss: 0.0267, Accuracy: 9927/10000 (99%)\n",
      "\n",
      "Training - Epoch: 18 loss: 0.011442477695581814 accuracy: 99.62166666666667\n",
      "\n",
      "Test set: Average loss: 0.0291, Accuracy: 9917/10000 (99%)\n",
      "\n",
      "Training - Epoch: 19 loss: 0.011437644404359161 accuracy: 99.62333333333333\n",
      "\n",
      "Test set: Average loss: 0.0292, Accuracy: 9922/10000 (99%)\n",
      "\n",
      "Training - Epoch: 20 loss: 0.011072079230348269 accuracy: 99.62666666666667\n",
      "\n",
      "Test set: Average loss: 0.0339, Accuracy: 9921/10000 (99%)\n",
      "\n",
      "Training - Epoch: 21 loss: 0.010543397892763217 accuracy: 99.655\n",
      "\n",
      "Test set: Average loss: 0.0289, Accuracy: 9915/10000 (99%)\n",
      "\n",
      "Training - Epoch: 22 loss: 0.00986304988314708 accuracy: 99.66666666666667\n",
      "\n",
      "Test set: Average loss: 0.0301, Accuracy: 9923/10000 (99%)\n",
      "\n",
      "Training - Epoch: 23 loss: 0.010409066738436619 accuracy: 99.65166666666667\n",
      "\n",
      "Test set: Average loss: 0.0322, Accuracy: 9921/10000 (99%)\n",
      "\n",
      "Training - Epoch: 24 loss: 0.009565090375145276 accuracy: 99.68\n",
      "\n",
      "Test set: Average loss: 0.0308, Accuracy: 9927/10000 (99%)\n",
      "\n",
      "Training - Epoch: 25 loss: 0.010454307150964936 accuracy: 99.67\n",
      "\n",
      "Test set: Average loss: 0.0315, Accuracy: 9927/10000 (99%)\n",
      "\n",
      "Training - Epoch: 26 loss: 0.00944464670419693 accuracy: 99.69333333333333\n",
      "\n",
      "Test set: Average loss: 0.0275, Accuracy: 9928/10000 (99%)\n",
      "\n",
      "Training - Epoch: 27 loss: 0.009887590220818917 accuracy: 99.68166666666667\n",
      "\n",
      "Test set: Average loss: 0.0344, Accuracy: 9926/10000 (99%)\n",
      "\n",
      "Training - Epoch: 28 loss: 0.00897950474669536 accuracy: 99.69833333333334\n",
      "\n",
      "Test set: Average loss: 0.0283, Accuracy: 9926/10000 (99%)\n",
      "\n",
      "Training - Epoch: 29 loss: 0.008040993452223484 accuracy: 99.72666666666667\n",
      "\n",
      "Test set: Average loss: 0.0371, Accuracy: 9926/10000 (99%)\n",
      "\n",
      "Training - Epoch: 30 loss: 0.009800823447604974 accuracy: 99.67166666666667\n",
      "\n",
      "Test set: Average loss: 0.0306, Accuracy: 9921/10000 (99%)\n",
      "\n",
      "Training - Epoch: 31 loss: 0.010712295542905727 accuracy: 99.64\n",
      "\n",
      "Test set: Average loss: 0.0312, Accuracy: 9929/10000 (99%)\n",
      "\n",
      "Training - Epoch: 32 loss: 0.009360379522076497 accuracy: 99.69333333333333\n",
      "\n",
      "Test set: Average loss: 0.0354, Accuracy: 9924/10000 (99%)\n",
      "\n",
      "Training - Epoch: 33 loss: 0.009117702677442382 accuracy: 99.685\n",
      "\n",
      "Test set: Average loss: 0.0315, Accuracy: 9930/10000 (99%)\n",
      "\n",
      "Training - Epoch: 34 loss: 0.010476799497908602 accuracy: 99.685\n",
      "\n",
      "Test set: Average loss: 0.0342, Accuracy: 9922/10000 (99%)\n",
      "\n",
      "Training - Epoch: 35 loss: 0.00994889843997856 accuracy: 99.68833333333333\n",
      "\n",
      "Test set: Average loss: 0.0354, Accuracy: 9927/10000 (99%)\n",
      "\n",
      "Training - Epoch: 36 loss: 0.009525276920731024 accuracy: 99.71166666666667\n",
      "\n",
      "Test set: Average loss: 0.0395, Accuracy: 9918/10000 (99%)\n",
      "\n",
      "Training - Epoch: 37 loss: 0.010505212223033111 accuracy: 99.67\n",
      "\n",
      "Test set: Average loss: 0.0416, Accuracy: 9916/10000 (99%)\n",
      "\n",
      "Training - Epoch: 38 loss: 0.010591514567534129 accuracy: 99.68666666666667\n",
      "\n",
      "Test set: Average loss: 0.0405, Accuracy: 9927/10000 (99%)\n",
      "\n",
      "Training - Epoch: 39 loss: 0.010797780654827754 accuracy: 99.67666666666666\n",
      "\n",
      "Test set: Average loss: 0.0447, Accuracy: 9918/10000 (99%)\n",
      "\n",
      "Training - Epoch: 40 loss: 0.0113200199679782 accuracy: 99.66166666666666\n",
      "\n",
      "Test set: Average loss: 0.0404, Accuracy: 9916/10000 (99%)\n",
      "\n",
      "Training - Epoch: 41 loss: 0.010939084700743357 accuracy: 99.67166666666667\n",
      "\n",
      "Test set: Average loss: 0.0321, Accuracy: 9928/10000 (99%)\n",
      "\n",
      "Training - Epoch: 42 loss: 0.009244039009837434 accuracy: 99.7\n",
      "\n",
      "Test set: Average loss: 0.0405, Accuracy: 9926/10000 (99%)\n",
      "\n",
      "Training - Epoch: 43 loss: 0.011039314775665601 accuracy: 99.65166666666667\n",
      "\n",
      "Test set: Average loss: 0.0395, Accuracy: 9921/10000 (99%)\n",
      "\n",
      "Training - Epoch: 44 loss: 0.011147749445059647 accuracy: 99.65333333333334\n",
      "\n",
      "Test set: Average loss: 0.0428, Accuracy: 9914/10000 (99%)\n",
      "\n",
      "Training - Epoch: 45 loss: 0.011738921431700388 accuracy: 99.665\n",
      "\n",
      "Test set: Average loss: 0.0390, Accuracy: 9921/10000 (99%)\n",
      "\n",
      "Training - Epoch: 46 loss: 0.011964277127260963 accuracy: 99.66333333333333\n",
      "\n",
      "Test set: Average loss: 0.0348, Accuracy: 9923/10000 (99%)\n",
      "\n",
      "Training - Epoch: 47 loss: 0.011245405935247739 accuracy: 99.63833333333334\n",
      "\n",
      "Test set: Average loss: 0.0417, Accuracy: 9924/10000 (99%)\n",
      "\n",
      "Training - Epoch: 48 loss: 0.012996146661539873 accuracy: 99.62833333333333\n",
      "\n",
      "Test set: Average loss: 0.0388, Accuracy: 9928/10000 (99%)\n",
      "\n",
      "Training - Epoch: 49 loss: 0.012274671691221496 accuracy: 99.64166666666667\n",
      "\n",
      "Test set: Average loss: 0.0419, Accuracy: 9930/10000 (99%)\n",
      "\n",
      "Training - Epoch: 50 loss: 0.013861778863271078 accuracy: 99.61666666666666\n",
      "\n",
      "Test set: Average loss: 0.0333, Accuracy: 9922/10000 (99%)\n",
      "\n",
      "Training - Epoch: 51 loss: 0.013519616442918778 accuracy: 99.635\n",
      "\n",
      "Test set: Average loss: 0.0338, Accuracy: 9913/10000 (99%)\n",
      "\n",
      "Training - Epoch: 52 loss: 0.013812180598576864 accuracy: 99.60666666666667\n",
      "\n",
      "Test set: Average loss: 0.0411, Accuracy: 9928/10000 (99%)\n",
      "\n",
      "Training - Epoch: 53 loss: 0.013896587271305423 accuracy: 99.57166666666667\n",
      "\n",
      "Test set: Average loss: 0.0403, Accuracy: 9924/10000 (99%)\n",
      "\n",
      "Training - Epoch: 54 loss: 0.01388334810535113 accuracy: 99.57166666666667\n",
      "\n",
      "Test set: Average loss: 0.0290, Accuracy: 9925/10000 (99%)\n",
      "\n",
      "Training - Epoch: 55 loss: 0.01418324542293946 accuracy: 99.605\n",
      "\n",
      "Test set: Average loss: 0.0445, Accuracy: 9915/10000 (99%)\n",
      "\n",
      "Training - Epoch: 56 loss: 0.014814872339367867 accuracy: 99.58\n",
      "\n",
      "Test set: Average loss: 0.0335, Accuracy: 9934/10000 (99%)\n",
      "\n",
      "Training - Epoch: 57 loss: 0.015498052388553818 accuracy: 99.55666666666667\n",
      "\n",
      "Test set: Average loss: 0.0418, Accuracy: 9914/10000 (99%)\n",
      "\n",
      "Training - Epoch: 58 loss: 0.014633616554170536 accuracy: 99.58333333333333\n",
      "\n",
      "Test set: Average loss: 0.0465, Accuracy: 9917/10000 (99%)\n",
      "\n",
      "Training - Epoch: 59 loss: 0.013968430304030577 accuracy: 99.60833333333333\n",
      "\n",
      "Test set: Average loss: 0.0507, Accuracy: 9919/10000 (99%)\n",
      "\n",
      "Training - Epoch: 60 loss: 0.01524661030670007 accuracy: 99.57166666666667\n",
      "\n",
      "Test set: Average loss: 0.0331, Accuracy: 9913/10000 (99%)\n",
      "\n",
      "Training - Epoch: 61 loss: 0.014790633077671131 accuracy: 99.59\n",
      "\n",
      "Test set: Average loss: 0.0425, Accuracy: 9924/10000 (99%)\n",
      "\n",
      "Training - Epoch: 62 loss: 0.015273658286035061 accuracy: 99.59166666666667\n",
      "\n",
      "Test set: Average loss: 0.0522, Accuracy: 9919/10000 (99%)\n",
      "\n",
      "Training - Epoch: 63 loss: 0.014122155193984508 accuracy: 99.57166666666667\n",
      "\n",
      "Test set: Average loss: 0.0455, Accuracy: 9918/10000 (99%)\n",
      "\n",
      "Training - Epoch: 64 loss: 0.013800370394686857 accuracy: 99.60666666666667\n",
      "\n",
      "Test set: Average loss: 0.0393, Accuracy: 9926/10000 (99%)\n",
      "\n",
      "Training - Epoch: 65 loss: 0.015298059308032195 accuracy: 99.57333333333334\n",
      "\n",
      "Test set: Average loss: 0.0418, Accuracy: 9928/10000 (99%)\n",
      "\n",
      "Training - Epoch: 66 loss: 0.015590119562546412 accuracy: 99.56833333333333\n",
      "\n",
      "Test set: Average loss: 0.0411, Accuracy: 9920/10000 (99%)\n",
      "\n",
      "Training - Epoch: 67 loss: 0.014166188404957453 accuracy: 99.58333333333333\n",
      "\n",
      "Test set: Average loss: 0.0397, Accuracy: 9923/10000 (99%)\n",
      "\n",
      "Training - Epoch: 68 loss: 0.014906444142758846 accuracy: 99.60833333333333\n",
      "\n",
      "Test set: Average loss: 0.0485, Accuracy: 9918/10000 (99%)\n",
      "\n",
      "Training - Epoch: 69 loss: 0.015447958971684177 accuracy: 99.595\n",
      "\n",
      "Test set: Average loss: 0.0436, Accuracy: 9928/10000 (99%)\n",
      "\n",
      "Training - Epoch: 70 loss: 0.017929093419512113 accuracy: 99.55166666666666\n",
      "\n",
      "Test set: Average loss: 0.0425, Accuracy: 9920/10000 (99%)\n",
      "\n",
      "Training - Epoch: 71 loss: 0.017897414260109264 accuracy: 99.52\n",
      "\n",
      "Test set: Average loss: 0.0404, Accuracy: 9918/10000 (99%)\n",
      "\n",
      "Training - Epoch: 72 loss: 0.015922394918402034 accuracy: 99.51666666666667\n",
      "\n",
      "Test set: Average loss: 0.0354, Accuracy: 9918/10000 (99%)\n",
      "\n",
      "Training - Epoch: 73 loss: 0.014666201104968786 accuracy: 99.58166666666666\n",
      "\n",
      "Test set: Average loss: 0.0419, Accuracy: 9917/10000 (99%)\n",
      "\n",
      "Training - Epoch: 74 loss: 0.016664802899087467 accuracy: 99.55833333333334\n",
      "\n",
      "Test set: Average loss: 0.0373, Accuracy: 9927/10000 (99%)\n",
      "\n",
      "Training - Epoch: 75 loss: 0.016100028587381045 accuracy: 99.55833333333334\n",
      "\n",
      "Test set: Average loss: 0.0402, Accuracy: 9934/10000 (99%)\n",
      "\n",
      "Training - Epoch: 76 loss: 0.016643142204607525 accuracy: 99.60166666666667\n",
      "\n",
      "Test set: Average loss: 0.0523, Accuracy: 9920/10000 (99%)\n",
      "\n",
      "Training - Epoch: 77 loss: 0.016452257783214252 accuracy: 99.605\n",
      "\n",
      "Test set: Average loss: 0.0389, Accuracy: 9930/10000 (99%)\n",
      "\n",
      "Training - Epoch: 78 loss: 0.014744907007118066 accuracy: 99.61666666666666\n",
      "\n",
      "Test set: Average loss: 0.0440, Accuracy: 9922/10000 (99%)\n",
      "\n",
      "Training - Epoch: 79 loss: 0.015971605017284552 accuracy: 99.56\n",
      "\n",
      "Test set: Average loss: 0.0483, Accuracy: 9915/10000 (99%)\n",
      "\n",
      "Training - Epoch: 80 loss: 0.01661041818757852 accuracy: 99.59\n",
      "\n",
      "Test set: Average loss: 0.0430, Accuracy: 9919/10000 (99%)\n",
      "\n",
      "Training - Epoch: 81 loss: 0.01804677696675062 accuracy: 99.515\n",
      "\n",
      "Test set: Average loss: 0.0533, Accuracy: 9919/10000 (99%)\n",
      "\n",
      "Training - Epoch: 82 loss: 0.015593489817778269 accuracy: 99.59\n",
      "\n",
      "Test set: Average loss: 0.0314, Accuracy: 9927/10000 (99%)\n",
      "\n",
      "Training - Epoch: 83 loss: 0.016695623389755686 accuracy: 99.57166666666667\n",
      "\n",
      "Test set: Average loss: 0.0485, Accuracy: 9919/10000 (99%)\n",
      "\n",
      "Training - Epoch: 84 loss: 0.01706497624690334 accuracy: 99.58166666666666\n",
      "\n",
      "Test set: Average loss: 0.0492, Accuracy: 9915/10000 (99%)\n",
      "\n",
      "Training - Epoch: 85 loss: 0.01644682766199112 accuracy: 99.55833333333334\n",
      "\n",
      "Test set: Average loss: 0.0444, Accuracy: 9928/10000 (99%)\n",
      "\n",
      "Training - Epoch: 86 loss: 0.016522392605245115 accuracy: 99.54166666666667\n",
      "\n",
      "Test set: Average loss: 0.0485, Accuracy: 9914/10000 (99%)\n",
      "\n",
      "Training - Epoch: 87 loss: 0.01691606200610598 accuracy: 99.57666666666667\n",
      "\n",
      "Test set: Average loss: 0.0434, Accuracy: 9921/10000 (99%)\n",
      "\n",
      "Training - Epoch: 88 loss: 0.01807833395351966 accuracy: 99.535\n",
      "\n",
      "Test set: Average loss: 0.0396, Accuracy: 9914/10000 (99%)\n",
      "\n",
      "Training - Epoch: 89 loss: 0.018351328602650513 accuracy: 99.53666666666666\n",
      "\n",
      "Test set: Average loss: 0.0443, Accuracy: 9918/10000 (99%)\n",
      "\n",
      "Training - Epoch: 90 loss: 0.015928320859620967 accuracy: 99.56\n",
      "\n",
      "Test set: Average loss: 0.0360, Accuracy: 9919/10000 (99%)\n",
      "\n",
      "Training - Epoch: 91 loss: 0.0163607408542186 accuracy: 99.57166666666667\n",
      "\n",
      "Test set: Average loss: 0.0477, Accuracy: 9923/10000 (99%)\n",
      "\n",
      "Training - Epoch: 92 loss: 0.019042731527859964 accuracy: 99.50166666666667\n",
      "\n",
      "Test set: Average loss: 0.0578, Accuracy: 9917/10000 (99%)\n",
      "\n",
      "Training - Epoch: 93 loss: 0.016060171039402486 accuracy: 99.57833333333333\n",
      "\n",
      "Test set: Average loss: 0.0488, Accuracy: 9923/10000 (99%)\n",
      "\n",
      "Training - Epoch: 94 loss: 0.01971748531080472 accuracy: 99.51\n",
      "\n",
      "Test set: Average loss: 0.0567, Accuracy: 9909/10000 (99%)\n",
      "\n",
      "Training - Epoch: 95 loss: 0.019989999263733627 accuracy: 99.46666666666667\n",
      "\n",
      "Test set: Average loss: 0.0406, Accuracy: 9927/10000 (99%)\n",
      "\n",
      "Training - Epoch: 96 loss: 0.01604324403673236 accuracy: 99.55333333333333\n",
      "\n",
      "Test set: Average loss: 0.0554, Accuracy: 9924/10000 (99%)\n",
      "\n",
      "Training - Epoch: 97 loss: 0.018562660090012166 accuracy: 99.505\n",
      "\n",
      "Test set: Average loss: 0.0459, Accuracy: 9917/10000 (99%)\n",
      "\n",
      "Training - Epoch: 98 loss: 0.016447299522409836 accuracy: 99.58333333333333\n",
      "\n",
      "Test set: Average loss: 0.0394, Accuracy: 9918/10000 (99%)\n",
      "\n",
      "Training - Epoch: 99 loss: 0.018476497933268547 accuracy: 99.545\n",
      "\n",
      "Test set: Average loss: 0.0380, Accuracy: 9917/10000 (99%)\n",
      "\n",
      "Training - Epoch: 100 loss: 0.016087578043341638 accuracy: 99.59333333333333\n",
      "\n",
      "Test set: Average loss: 0.0458, Accuracy: 9920/10000 (99%)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "epoch=100\n",
    "model = CnnNet5().to(device)\n",
    "model5_params = sum(p.numel() for p in model.parameters())\n",
    "optimizer = optim.Adadelta(model.parameters(), lr=1)\n",
    "scheduler = StepLR(optimizer, step_size=1, gamma=0.1)\n",
    "loss = nn.CrossEntropyLoss()\n",
    "train_loss_5, accuracy_train_5, test_loss_5, test_accuracy_5 = returnModelAccAndLoss(model, device, train_loader, test_loader, optimizer, loss, epoch, scheduler)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "bc71aff1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training - Epoch: 1 loss: 0.4230681296666463 accuracy: 86.86666666666666\n",
      "\n",
      "Test set: Average loss: 0.0687, Accuracy: 9778/10000 (98%)\n",
      "\n",
      "Training - Epoch: 2 loss: 0.07892294031778971 accuracy: 97.58333333333333\n",
      "\n",
      "Test set: Average loss: 0.0438, Accuracy: 9849/10000 (98%)\n",
      "\n",
      "Training - Epoch: 3 loss: 0.05457811172008514 accuracy: 98.27833333333334\n",
      "\n",
      "Test set: Average loss: 0.0316, Accuracy: 9896/10000 (99%)\n",
      "\n",
      "Training - Epoch: 4 loss: 0.03883043236732483 accuracy: 98.73833333333333\n",
      "\n",
      "Test set: Average loss: 0.0323, Accuracy: 9890/10000 (99%)\n",
      "\n",
      "Training - Epoch: 5 loss: 0.03354357192516327 accuracy: 98.98333333333333\n",
      "\n",
      "Test set: Average loss: 0.0259, Accuracy: 9913/10000 (99%)\n",
      "\n",
      "Training - Epoch: 6 loss: 0.02808244196251035 accuracy: 99.12333333333333\n",
      "\n",
      "Test set: Average loss: 0.0266, Accuracy: 9908/10000 (99%)\n",
      "\n",
      "Training - Epoch: 7 loss: 0.023107887424031893 accuracy: 99.27\n",
      "\n",
      "Test set: Average loss: 0.0276, Accuracy: 9915/10000 (99%)\n",
      "\n",
      "Training - Epoch: 8 loss: 0.019807343762119612 accuracy: 99.34666666666666\n",
      "\n",
      "Test set: Average loss: 0.0245, Accuracy: 9926/10000 (99%)\n",
      "\n",
      "Training - Epoch: 9 loss: 0.017170316872000695 accuracy: 99.48\n",
      "\n",
      "Test set: Average loss: 0.0234, Accuracy: 9925/10000 (99%)\n",
      "\n",
      "Training - Epoch: 10 loss: 0.016333543720841406 accuracy: 99.48\n",
      "\n",
      "Test set: Average loss: 0.0269, Accuracy: 9913/10000 (99%)\n",
      "\n",
      "Training - Epoch: 11 loss: 0.013339232162634532 accuracy: 99.59666666666666\n",
      "\n",
      "Test set: Average loss: 0.0288, Accuracy: 9923/10000 (99%)\n",
      "\n",
      "Training - Epoch: 12 loss: 0.012647294151782989 accuracy: 99.585\n",
      "\n",
      "Test set: Average loss: 0.0251, Accuracy: 9924/10000 (99%)\n",
      "\n",
      "Training - Epoch: 13 loss: 0.010888656914047896 accuracy: 99.64166666666667\n",
      "\n",
      "Test set: Average loss: 0.0260, Accuracy: 9923/10000 (99%)\n",
      "\n",
      "Training - Epoch: 14 loss: 0.010129627598325412 accuracy: 99.68166666666667\n",
      "\n",
      "Test set: Average loss: 0.0255, Accuracy: 9935/10000 (99%)\n",
      "\n",
      "Training - Epoch: 15 loss: 0.010384963379303615 accuracy: 99.68\n",
      "\n",
      "Test set: Average loss: 0.0277, Accuracy: 9923/10000 (99%)\n",
      "\n",
      "Training - Epoch: 16 loss: 0.008453815129896005 accuracy: 99.71666666666667\n",
      "\n",
      "Test set: Average loss: 0.0291, Accuracy: 9926/10000 (99%)\n",
      "\n",
      "Training - Epoch: 17 loss: 0.008152288658171893 accuracy: 99.74\n",
      "\n",
      "Test set: Average loss: 0.0278, Accuracy: 9919/10000 (99%)\n",
      "\n",
      "Training - Epoch: 18 loss: 0.007082632153481245 accuracy: 99.775\n",
      "\n",
      "Test set: Average loss: 0.0299, Accuracy: 9929/10000 (99%)\n",
      "\n",
      "Training - Epoch: 19 loss: 0.00650448434005181 accuracy: 99.775\n",
      "\n",
      "Test set: Average loss: 0.0270, Accuracy: 9930/10000 (99%)\n",
      "\n",
      "Training - Epoch: 20 loss: 0.006618946852783362 accuracy: 99.79666666666667\n",
      "\n",
      "Test set: Average loss: 0.0283, Accuracy: 9929/10000 (99%)\n",
      "\n",
      "Training - Epoch: 21 loss: 0.006066832186033329 accuracy: 99.77333333333333\n",
      "\n",
      "Test set: Average loss: 0.0323, Accuracy: 9928/10000 (99%)\n",
      "\n",
      "Training - Epoch: 22 loss: 0.006330302697171768 accuracy: 99.78666666666666\n",
      "\n",
      "Test set: Average loss: 0.0252, Accuracy: 9932/10000 (99%)\n",
      "\n",
      "Training - Epoch: 23 loss: 0.005226636479174098 accuracy: 99.83666666666667\n",
      "\n",
      "Test set: Average loss: 0.0317, Accuracy: 9932/10000 (99%)\n",
      "\n",
      "Training - Epoch: 24 loss: 0.005066336894532045 accuracy: 99.82666666666667\n",
      "\n",
      "Test set: Average loss: 0.0277, Accuracy: 9937/10000 (99%)\n",
      "\n",
      "Training - Epoch: 25 loss: 0.004593477133164803 accuracy: 99.81666666666666\n",
      "\n",
      "Test set: Average loss: 0.0387, Accuracy: 9924/10000 (99%)\n",
      "\n",
      "Training - Epoch: 26 loss: 0.005362147104740143 accuracy: 99.84666666666666\n",
      "\n",
      "Test set: Average loss: 0.0326, Accuracy: 9933/10000 (99%)\n",
      "\n",
      "Training - Epoch: 27 loss: 0.0041462112540611995 accuracy: 99.85666666666667\n",
      "\n",
      "Test set: Average loss: 0.0321, Accuracy: 9933/10000 (99%)\n",
      "\n",
      "Training - Epoch: 28 loss: 0.0048180535569166145 accuracy: 99.86166666666666\n",
      "\n",
      "Test set: Average loss: 0.0351, Accuracy: 9934/10000 (99%)\n",
      "\n",
      "Training - Epoch: 29 loss: 0.004798292729483607 accuracy: 99.85333333333334\n",
      "\n",
      "Test set: Average loss: 0.0319, Accuracy: 9932/10000 (99%)\n",
      "\n",
      "Training - Epoch: 30 loss: 0.0044435994637198745 accuracy: 99.855\n",
      "\n",
      "Test set: Average loss: 0.0317, Accuracy: 9922/10000 (99%)\n",
      "\n",
      "Training - Epoch: 31 loss: 0.004074129828748604 accuracy: 99.86666666666666\n",
      "\n",
      "Test set: Average loss: 0.0347, Accuracy: 9929/10000 (99%)\n",
      "\n",
      "Training - Epoch: 32 loss: 0.004111017470868925 accuracy: 99.87333333333333\n",
      "\n",
      "Test set: Average loss: 0.0349, Accuracy: 9930/10000 (99%)\n",
      "\n",
      "Training - Epoch: 33 loss: 0.003574701022737039 accuracy: 99.88333333333334\n",
      "\n",
      "Test set: Average loss: 0.0380, Accuracy: 9926/10000 (99%)\n",
      "\n",
      "Training - Epoch: 34 loss: 0.0035078597887108725 accuracy: 99.87666666666667\n",
      "\n",
      "Test set: Average loss: 0.0334, Accuracy: 9926/10000 (99%)\n",
      "\n",
      "Training - Epoch: 35 loss: 0.002950571476611852 accuracy: 99.90166666666667\n",
      "\n",
      "Test set: Average loss: 0.0350, Accuracy: 9930/10000 (99%)\n",
      "\n",
      "Training - Epoch: 36 loss: 0.003692964785011524 accuracy: 99.85333333333334\n",
      "\n",
      "Test set: Average loss: 0.0334, Accuracy: 9938/10000 (99%)\n",
      "\n",
      "Training - Epoch: 37 loss: 0.003259196644493689 accuracy: 99.88166666666666\n",
      "\n",
      "Test set: Average loss: 0.0382, Accuracy: 9934/10000 (99%)\n",
      "\n",
      "Training - Epoch: 38 loss: 0.0031302989925878744 accuracy: 99.9\n",
      "\n",
      "Test set: Average loss: 0.0338, Accuracy: 9936/10000 (99%)\n",
      "\n",
      "Training - Epoch: 39 loss: 0.003233098076027818 accuracy: 99.9\n",
      "\n",
      "Test set: Average loss: 0.0313, Accuracy: 9935/10000 (99%)\n",
      "\n",
      "Training - Epoch: 40 loss: 0.002298077204975137 accuracy: 99.92166666666667\n",
      "\n",
      "Test set: Average loss: 0.0366, Accuracy: 9934/10000 (99%)\n",
      "\n",
      "Training - Epoch: 41 loss: 0.003160238848285129 accuracy: 99.89666666666666\n",
      "\n",
      "Test set: Average loss: 0.0347, Accuracy: 9933/10000 (99%)\n",
      "\n",
      "Training - Epoch: 42 loss: 0.0033827988917570716 accuracy: 99.89\n",
      "\n",
      "Test set: Average loss: 0.0333, Accuracy: 9932/10000 (99%)\n",
      "\n",
      "Training - Epoch: 43 loss: 0.002744012131448835 accuracy: 99.89833333333333\n",
      "\n",
      "Test set: Average loss: 0.0387, Accuracy: 9930/10000 (99%)\n",
      "\n",
      "Training - Epoch: 44 loss: 0.0027121429657253125 accuracy: 99.90666666666667\n",
      "\n",
      "Test set: Average loss: 0.0380, Accuracy: 9931/10000 (99%)\n",
      "\n",
      "Training - Epoch: 45 loss: 0.002393607272130127 accuracy: 99.91666666666667\n",
      "\n",
      "Test set: Average loss: 0.0361, Accuracy: 9936/10000 (99%)\n",
      "\n",
      "Training - Epoch: 46 loss: 0.0021489236921886914 accuracy: 99.93166666666667\n",
      "\n",
      "Test set: Average loss: 0.0359, Accuracy: 9931/10000 (99%)\n",
      "\n",
      "Training - Epoch: 47 loss: 0.0030398435219811896 accuracy: 99.89666666666666\n",
      "\n",
      "Test set: Average loss: 0.0377, Accuracy: 9928/10000 (99%)\n",
      "\n",
      "Training - Epoch: 48 loss: 0.0019214218793436886 accuracy: 99.925\n",
      "\n",
      "Test set: Average loss: 0.0362, Accuracy: 9929/10000 (99%)\n",
      "\n",
      "Training - Epoch: 49 loss: 0.003048471969878301 accuracy: 99.91333333333333\n",
      "\n",
      "Test set: Average loss: 0.0387, Accuracy: 9926/10000 (99%)\n",
      "\n",
      "Training - Epoch: 50 loss: 0.003080426514761833 accuracy: 99.90333333333334\n",
      "\n",
      "Test set: Average loss: 0.0429, Accuracy: 9922/10000 (99%)\n",
      "\n",
      "Training - Epoch: 51 loss: 0.0017906567674013786 accuracy: 99.93666666666667\n",
      "\n",
      "Test set: Average loss: 0.0427, Accuracy: 9935/10000 (99%)\n",
      "\n",
      "Training - Epoch: 52 loss: 0.0025513371073175224 accuracy: 99.93166666666667\n",
      "\n",
      "Test set: Average loss: 0.0366, Accuracy: 9933/10000 (99%)\n",
      "\n",
      "Training - Epoch: 53 loss: 0.0019317429681347373 accuracy: 99.93\n",
      "\n",
      "Test set: Average loss: 0.0435, Accuracy: 9936/10000 (99%)\n",
      "\n",
      "Training - Epoch: 54 loss: 0.0019833451741966806 accuracy: 99.93166666666667\n",
      "\n",
      "Test set: Average loss: 0.0485, Accuracy: 9928/10000 (99%)\n",
      "\n",
      "Training - Epoch: 55 loss: 0.0023330598572278783 accuracy: 99.925\n",
      "\n",
      "Test set: Average loss: 0.0471, Accuracy: 9930/10000 (99%)\n",
      "\n",
      "Training - Epoch: 56 loss: 0.0021945989595357485 accuracy: 99.92833333333333\n",
      "\n",
      "Test set: Average loss: 0.0450, Accuracy: 9924/10000 (99%)\n",
      "\n",
      "Training - Epoch: 57 loss: 0.00203293279891368 accuracy: 99.925\n",
      "\n",
      "Test set: Average loss: 0.0427, Accuracy: 9925/10000 (99%)\n",
      "\n",
      "Training - Epoch: 58 loss: 0.002387433767502322 accuracy: 99.925\n",
      "\n",
      "Test set: Average loss: 0.0398, Accuracy: 9931/10000 (99%)\n",
      "\n",
      "Training - Epoch: 59 loss: 0.0022402182273877163 accuracy: 99.92833333333333\n",
      "\n",
      "Test set: Average loss: 0.0350, Accuracy: 9941/10000 (99%)\n",
      "\n",
      "Training - Epoch: 60 loss: 0.0025482614308498645 accuracy: 99.91166666666666\n",
      "\n",
      "Test set: Average loss: 0.0423, Accuracy: 9930/10000 (99%)\n",
      "\n",
      "Training - Epoch: 61 loss: 0.002115133244598595 accuracy: 99.925\n",
      "\n",
      "Test set: Average loss: 0.0436, Accuracy: 9933/10000 (99%)\n",
      "\n",
      "Training - Epoch: 62 loss: 0.002045743723977163 accuracy: 99.93666666666667\n",
      "\n",
      "Test set: Average loss: 0.0455, Accuracy: 9929/10000 (99%)\n",
      "\n",
      "Training - Epoch: 63 loss: 0.0020407963085977824 accuracy: 99.945\n",
      "\n",
      "Test set: Average loss: 0.0397, Accuracy: 9928/10000 (99%)\n",
      "\n",
      "Training - Epoch: 64 loss: 0.001936367053373639 accuracy: 99.93\n",
      "\n",
      "Test set: Average loss: 0.0382, Accuracy: 9940/10000 (99%)\n",
      "\n",
      "Training - Epoch: 65 loss: 0.0017053328112272236 accuracy: 99.94166666666666\n",
      "\n",
      "Test set: Average loss: 0.0419, Accuracy: 9940/10000 (99%)\n",
      "\n",
      "Training - Epoch: 66 loss: 0.0021008939853015664 accuracy: 99.93\n",
      "\n",
      "Test set: Average loss: 0.0375, Accuracy: 9937/10000 (99%)\n",
      "\n",
      "Training - Epoch: 67 loss: 0.0020833463694202997 accuracy: 99.935\n",
      "\n",
      "Test set: Average loss: 0.0411, Accuracy: 9933/10000 (99%)\n",
      "\n",
      "Training - Epoch: 68 loss: 0.0021065722642544036 accuracy: 99.93166666666667\n",
      "\n",
      "Test set: Average loss: 0.0414, Accuracy: 9937/10000 (99%)\n",
      "\n",
      "Training - Epoch: 69 loss: 0.0014462652482946092 accuracy: 99.94833333333334\n",
      "\n",
      "Test set: Average loss: 0.0451, Accuracy: 9940/10000 (99%)\n",
      "\n",
      "Training - Epoch: 70 loss: 0.002550479488172035 accuracy: 99.91666666666667\n",
      "\n",
      "Test set: Average loss: 0.0366, Accuracy: 9940/10000 (99%)\n",
      "\n",
      "Training - Epoch: 71 loss: 0.001990402216383033 accuracy: 99.92833333333333\n",
      "\n",
      "Test set: Average loss: 0.0385, Accuracy: 9937/10000 (99%)\n",
      "\n",
      "Training - Epoch: 72 loss: 0.0021556627639355915 accuracy: 99.94\n",
      "\n",
      "Test set: Average loss: 0.0452, Accuracy: 9936/10000 (99%)\n",
      "\n",
      "Training - Epoch: 73 loss: 0.001217901692156253 accuracy: 99.96333333333334\n",
      "\n",
      "Test set: Average loss: 0.0416, Accuracy: 9936/10000 (99%)\n",
      "\n",
      "Training - Epoch: 74 loss: 0.0016565780133746253 accuracy: 99.955\n",
      "\n",
      "Test set: Average loss: 0.0407, Accuracy: 9931/10000 (99%)\n",
      "\n",
      "Training - Epoch: 75 loss: 0.0018967181981119211 accuracy: 99.945\n",
      "\n",
      "Test set: Average loss: 0.0451, Accuracy: 9935/10000 (99%)\n",
      "\n",
      "Training - Epoch: 76 loss: 0.0019126933398345878 accuracy: 99.93833333333333\n",
      "\n",
      "Test set: Average loss: 0.0439, Accuracy: 9933/10000 (99%)\n",
      "\n",
      "Training - Epoch: 77 loss: 0.0019974951673221463 accuracy: 99.93166666666667\n",
      "\n",
      "Test set: Average loss: 0.0399, Accuracy: 9934/10000 (99%)\n",
      "\n",
      "Training - Epoch: 78 loss: 0.0017662095468070522 accuracy: 99.93333333333334\n",
      "\n",
      "Test set: Average loss: 0.0403, Accuracy: 9934/10000 (99%)\n",
      "\n",
      "Training - Epoch: 79 loss: 0.0017847786066549208 accuracy: 99.94833333333334\n",
      "\n",
      "Test set: Average loss: 0.0421, Accuracy: 9941/10000 (99%)\n",
      "\n",
      "Training - Epoch: 80 loss: 0.0022184196421449693 accuracy: 99.935\n",
      "\n",
      "Test set: Average loss: 0.0465, Accuracy: 9935/10000 (99%)\n",
      "\n",
      "Training - Epoch: 81 loss: 0.0013630116302791673 accuracy: 99.945\n",
      "\n",
      "Test set: Average loss: 0.0473, Accuracy: 9928/10000 (99%)\n",
      "\n",
      "Training - Epoch: 82 loss: 0.0018838972364047853 accuracy: 99.93166666666667\n",
      "\n",
      "Test set: Average loss: 0.0449, Accuracy: 9933/10000 (99%)\n",
      "\n",
      "Training - Epoch: 83 loss: 0.0015159297537000384 accuracy: 99.96\n",
      "\n",
      "Test set: Average loss: 0.0501, Accuracy: 9933/10000 (99%)\n",
      "\n",
      "Training - Epoch: 84 loss: 0.00189828513127383 accuracy: 99.93166666666667\n",
      "\n",
      "Test set: Average loss: 0.0494, Accuracy: 9929/10000 (99%)\n",
      "\n",
      "Training - Epoch: 85 loss: 0.0024591194593172987 accuracy: 99.92666666666666\n",
      "\n",
      "Test set: Average loss: 0.0424, Accuracy: 9935/10000 (99%)\n",
      "\n",
      "Training - Epoch: 86 loss: 0.0015852287053799955 accuracy: 99.95166666666667\n",
      "\n",
      "Test set: Average loss: 0.0449, Accuracy: 9932/10000 (99%)\n",
      "\n",
      "Training - Epoch: 87 loss: 0.0017798303912990377 accuracy: 99.93333333333334\n",
      "\n",
      "Test set: Average loss: 0.0395, Accuracy: 9936/10000 (99%)\n",
      "\n",
      "Training - Epoch: 88 loss: 0.0019203512245905587 accuracy: 99.93\n",
      "\n",
      "Test set: Average loss: 0.0539, Accuracy: 9929/10000 (99%)\n",
      "\n",
      "Training - Epoch: 89 loss: 0.0016648704888531938 accuracy: 99.94166666666666\n",
      "\n",
      "Test set: Average loss: 0.0476, Accuracy: 9934/10000 (99%)\n",
      "\n",
      "Training - Epoch: 90 loss: 0.0015669551783124916 accuracy: 99.94\n",
      "\n",
      "Test set: Average loss: 0.0433, Accuracy: 9933/10000 (99%)\n",
      "\n",
      "Training - Epoch: 91 loss: 0.0016017175875216103 accuracy: 99.94166666666666\n",
      "\n",
      "Test set: Average loss: 0.0493, Accuracy: 9930/10000 (99%)\n",
      "\n",
      "Training - Epoch: 92 loss: 0.0013631060414229675 accuracy: 99.95166666666667\n",
      "\n",
      "Test set: Average loss: 0.0485, Accuracy: 9937/10000 (99%)\n",
      "\n",
      "Training - Epoch: 93 loss: 0.0015967068016703705 accuracy: 99.94\n",
      "\n",
      "Test set: Average loss: 0.0474, Accuracy: 9924/10000 (99%)\n",
      "\n",
      "Training - Epoch: 94 loss: 0.0015503609770217736 accuracy: 99.94666666666667\n",
      "\n",
      "Test set: Average loss: 0.0465, Accuracy: 9931/10000 (99%)\n",
      "\n",
      "Training - Epoch: 95 loss: 0.001268176426510278 accuracy: 99.95333333333333\n",
      "\n",
      "Test set: Average loss: 0.0493, Accuracy: 9928/10000 (99%)\n",
      "\n",
      "Training - Epoch: 96 loss: 0.0013493848336366984 accuracy: 99.94833333333334\n",
      "\n",
      "Test set: Average loss: 0.0449, Accuracy: 9932/10000 (99%)\n",
      "\n",
      "Training - Epoch: 97 loss: 0.001900145634838888 accuracy: 99.93666666666667\n",
      "\n",
      "Test set: Average loss: 0.0442, Accuracy: 9933/10000 (99%)\n",
      "\n",
      "Training - Epoch: 98 loss: 0.00179864601350273 accuracy: 99.955\n",
      "\n",
      "Test set: Average loss: 0.0471, Accuracy: 9935/10000 (99%)\n",
      "\n",
      "Training - Epoch: 99 loss: 0.0019471162048278832 accuracy: 99.945\n",
      "\n",
      "Test set: Average loss: 0.0443, Accuracy: 9932/10000 (99%)\n",
      "\n",
      "Training - Epoch: 100 loss: 0.0013091702309267323 accuracy: 99.95166666666667\n",
      "\n",
      "Test set: Average loss: 0.0463, Accuracy: 9927/10000 (99%)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "epoch=100\n",
    "model = CnnNet6().to(device)\n",
    "model6_params = sum(p.numel() for p in model.parameters())\n",
    "optimizer = optim.Adadelta(model.parameters(), lr=1)\n",
    "scheduler = StepLR(optimizer, step_size=1, gamma=0.1)\n",
    "loss = nn.CrossEntropyLoss()\n",
    "train_loss_6, accuracy_train_6, test_loss_6, test_accuracy_6 = returnModelAccAndLoss(model, device, train_loader, test_loader, optimizer, loss, epoch, scheduler)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "cf509388",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_parameters=dict()\n",
    "sum(p.numel() for p in model.parameters())\n",
    "model_parameters=[ sum(p.numel() for p in CnnNet().parameters()), sum(p.numel() for p in CnnNet2().parameters()),\n",
    "                  sum(p.numel() for p in CnnNet3().parameters()), sum(p.numel() for p in CnnNet4().parameters()),\n",
    "                  sum(p.numel() for p in CnnNet5().parameters()), sum(p.numel() for p in CnnNet6().parameters())]\n",
    "train_parameters[model_parameters[0]]=[accuracy_train_1[-1], test_accuracy_1[-1]]\n",
    "train_parameters[model_parameters[1]]=[accuracy_train_2[-1], test_accuracy_2[-1]]\n",
    "train_parameters[model_parameters[2]]= [accuracy_train_3[-1], test_accuracy_3[-1]]\n",
    "train_parameters[model_parameters[3]]= [accuracy_train_4[-1], test_accuracy_4[-1]]\n",
    "train_parameters[model_parameters[4]]= [accuracy_train_5[-1], test_accuracy_5[-1]]\n",
    "train_parameters[model_parameters[5]]= [accuracy_train_6[-1], test_accuracy_6[-1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "b5f67fc8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "693962\n",
      "[0.05098609790007273, 0.05023577457666397, 0.05204939595858256, 0.04862858276367187, 0.049204388658205665, 0.05224372126261393]\n",
      "[3469810]\n",
      "[(27210, 0.02319172476331393), (347690, 0.05224372126261393), (1187274, 0.018476497933268547), (1390250, 0.021659483379187685), (2369738, 0.0019471162048278832)]\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x432 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "train_loss=[ train_loss_1[-1], train_loss_2[-1], train_loss_3[-1],train_loss_4[-1], train_loss_5[-1]]\n",
    "test_loss = [ test_loss_1[-1], test_loss_2[-1],test_loss_3[-1], test_loss_4[-1], test_loss_5[-1]]\n",
    "print(model_parameters[0])\n",
    "import matplotlib.pyplot as plt\n",
    "plt1=plt.figure(figsize=(10, 6))\n",
    "\n",
    "print(train_loss_2[-7:-1])\n",
    "print([model_parameters[0]*5])\n",
    "n=1\n",
    "parameters=[model_parameters[1]]*n + [model_parameters[2]]*n + [model_parameters[3]]*n+ [model_parameters[4]]*n + [model_parameters[5]]*n\n",
    "#parameters\n",
    "train_losses=train_loss_2[-n-1:-1] + train_loss_3[-n-1:-1] + train_loss_4[-n-1:-1] + train_loss_5[-n-1:-1] + train_loss_6[-n-1:-1]\n",
    "#train_losses\n",
    "train_param_zip=zip(parameters, train_losses)\n",
    "train_parm= list(train_param_zip)\n",
    "train_parm.sort(key=lambda x: x[0])\n",
    "print(train_parm)\n",
    "\n",
    "test_parmaters=[model_parameters[1]]*n + [model_parameters[2]]*n +[model_parameters[3]]*n+ [model_parameters[4]]*n + [model_parameters[5]]*n\n",
    "test_losses=test_loss_2[-n-1:-1] + test_loss_3[-n-1:-1] + test_loss_4[-n-1:-1] + test_loss_5[-n-1:-1] +  test_loss_6[-n-1:-1]\n",
    "test_param_zip=zip(test_parmaters, test_losses)\n",
    "test_param=list(test_param_zip)\n",
    "test_param.sort(key=lambda x: x[0])\n",
    "\n",
    "plt.plot(np.array( [ t[0] for t in train_parm]),\n",
    "            np.array( [ t[1] for t in train_parm] ), label='training')\n",
    "plt.plot(np.array( [ t[0] for t in test_param]),\n",
    "            np.array( [ t[1] for t in test_param] ), label='test')\n",
    "\n",
    "# naming the x axis\n",
    "plt.xlabel('parameters')\n",
    "# naming the y axis\n",
    "plt.ylabel('Loss') \n",
    "plt.legend()\n",
    "plt.title(\"Loss function For MNIST\")\n",
    "plt.show()\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "61945507",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[99.22, 98.61166666666666, 99.59333333333333, 99.43, 99.95166666666667]\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x432 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "train_parameters_sort=[]\n",
    "train_acc=[]\n",
    "test_acc=[]\n",
    "train_parameters\n",
    "for p in sorted(train_parameters):\n",
    "    train_parameters_sort.append(p)\n",
    "    train_acc.append(train_parameters[p][0])\n",
    "    test_acc.append(train_parameters[p][1])\n",
    "train_acc.pop(2)\n",
    "test_acc.pop(2)\n",
    "train_parameters_sort.pop(2)\n",
    "print(train_acc)\n",
    "plt1=plt.figure(figsize=(10, 6))\n",
    "plt.plot(np.array(train_parameters_sort),np.array(train_acc), label='training')\n",
    "plt.plot(np.array(train_parameters_sort),np.array(test_acc), label='test')\n",
    "\n",
    "# naming the x axis\n",
    "plt.xlabel('Parameters')\n",
    "# naming the y axis\n",
    "plt.ylabel('Accuracy') \n",
    "plt.legend()\n",
    "plt.title(\"Accuracy Vs Parameters\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87da8bd4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b49c886-3001-4b75-a0cd-0c31ef1439d8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
