{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "54f45fad",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import argparse\n",
    "import torch\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "from torch.optim.lr_scheduler import StepLR\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "52c352ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CnnNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CnnNet, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 16, 3, 1)\n",
    "        self.fc1 = nn.Linear(2704, 128)\n",
    "        self.fc2 = nn.Linear(128, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = F.relu(x)\n",
    "        x = F.max_pool2d(x, 2)\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = self.fc1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        output = F.log_softmax(x, dim=1)\n",
    "        return output\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "efacf574",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train( model, device, train_loader, optimizer,loss_fn, epoch):\n",
    "    train_loss=0\n",
    "    model.train()\n",
    "    correct = 0\n",
    "    loss_fn = nn.CrossEntropyLoss(reduction='sum')\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        loss = loss_fn(output, target)\n",
    "        train_loss+=loss.item()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        pred = output.argmax(dim=1, keepdim=True)  # get the index of the max log-probability\n",
    "        correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "    train_loss /= len(train_loader.dataset)\n",
    "    print(\"Training - Epoch:\", epoch, \"loss:\", train_loss, \"accuracy:\", 100. * correct / len(train_loader.dataset))  \n",
    "    return train_loss, 100. * correct / len(train_loader.dataset)\n",
    "\n",
    "def test(model, device, test_loader):\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    loss_fn = nn.CrossEntropyLoss(reduction='sum')\n",
    "    with torch.no_grad():\n",
    "        for data, target in test_loader:\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            output = model(data)\n",
    "            test_loss += loss_fn(output, target)  # sum up batch loss\n",
    "            pred = output.argmax(dim=1, keepdim=True)  # get the index of the max log-probability\n",
    "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "\n",
    "    test_loss /= len(test_loader.dataset)\n",
    "\n",
    "    print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
    "        test_loss, correct, len(test_loader.dataset),\n",
    "        100. * correct / len(test_loader.dataset)))\n",
    "    return test_loss, 100. * correct / len(test_loader.dataset)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "9a287dac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torchvision.datasets.mnist.MNIST\n"
     ]
    }
   ],
   "source": [
    "use_cuda = torch.cuda.is_available()\n",
    "torch.manual_seed(1)\n",
    "\n",
    "device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
    "\n",
    "train_kwargs = {'batch_size': 64}\n",
    "test_kwargs = {'batch_size': 64}\n",
    "if use_cuda:\n",
    "    cuda_kwargs = {'num_workers': 1,\n",
    "                   'pin_memory': True,\n",
    "                   'shuffle': True}\n",
    "    train_kwargs.update(cuda_kwargs)\n",
    "    test_kwargs.update(cuda_kwargs)\n",
    "\n",
    "transform=transforms.Compose([\n",
    "        transforms.ToTensor()\n",
    "        ])\n",
    "dataset1 = datasets.MNIST('../data', train=True, download=True, transform=transform)\n",
    "dataset2 = datasets.MNIST('../data', train=False, transform=transform)\n",
    "print(type(dataset1))\n",
    "train_loader = torch.utils.data.DataLoader(dataset1,**train_kwargs)\n",
    "test_loader = torch.utils.data.DataLoader(dataset2, **test_kwargs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "6333413f",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data=[]\n",
    "train_labels=[]\n",
    "for data, label in train_loader:\n",
    "    for dt in data: \n",
    "        train_data.append(dt)\n",
    "    for lb in label:\n",
    "        train_labels.append(lb)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "88a94d28",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "random.shuffle(train_labels)\n",
    "\n",
    "train_data_shuffle = []\n",
    "for i in range(len(train_data)):\n",
    "    train_data_shuffle.append([train_data[i], train_labels[i]])\n",
    "\n",
    "train_loader_shuffle = torch.utils.data.DataLoader(train_data_shuffle, shuffle=True, batch_size=512)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "59cce36b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def returnModelAccAndLoss(model, device, train_loader, test_loader, optimizer, loss, epoch, scheduler):\n",
    "    train_loss=[]\n",
    "    test_loss=[]\n",
    "    accuracy_train=[]\n",
    "    accuracy_test=[]\n",
    "    for epoch in range(1, epoch+1):\n",
    "        tr_loss, tr_accuracy=train(model, device, train_loader, optimizer, loss, epoch)\n",
    "        train_loss.append(tr_loss)\n",
    "        accuracy_train.append(tr_accuracy)\n",
    "        tst_loss, tst_accuracy = test(model, device, test_loader)\n",
    "        test_loss.append(tst_loss)\n",
    "        accuracy_test.append(tst_accuracy)\n",
    "        scheduler.step()\n",
    "    return train_loss, accuracy_train, test_loss, accuracy_test\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "bf869b0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training - Epoch: 1 loss: 2.3075510897318523 accuracy: 11.035\n",
      "\n",
      "Test set: Average loss: 2.3012, Accuracy: 1135/10000 (11%)\n",
      "\n",
      "Training - Epoch: 2 loss: 2.3012968325297036 accuracy: 11.236666666666666\n",
      "\n",
      "Test set: Average loss: 2.3012, Accuracy: 1135/10000 (11%)\n",
      "\n",
      "Training - Epoch: 3 loss: 2.3012559949239093 accuracy: 11.236666666666666\n",
      "\n",
      "Test set: Average loss: 2.3012, Accuracy: 1135/10000 (11%)\n",
      "\n",
      "Training - Epoch: 4 loss: 2.301252320098877 accuracy: 11.236666666666666\n",
      "\n",
      "Test set: Average loss: 2.3012, Accuracy: 1135/10000 (11%)\n",
      "\n",
      "Training - Epoch: 5 loss: 2.3012519421895345 accuracy: 11.236666666666666\n",
      "\n",
      "Test set: Average loss: 2.3012, Accuracy: 1135/10000 (11%)\n",
      "\n",
      "Training - Epoch: 6 loss: 2.3012518549601237 accuracy: 11.236666666666666\n",
      "\n",
      "Test set: Average loss: 2.3012, Accuracy: 1135/10000 (11%)\n",
      "\n",
      "Training - Epoch: 7 loss: 2.3012517690022785 accuracy: 11.236666666666666\n",
      "\n",
      "Test set: Average loss: 2.3012, Accuracy: 1135/10000 (11%)\n",
      "\n",
      "Training - Epoch: 8 loss: 2.301251767730713 accuracy: 11.236666666666666\n",
      "\n",
      "Test set: Average loss: 2.3012, Accuracy: 1135/10000 (11%)\n",
      "\n",
      "Training - Epoch: 9 loss: 2.301251756286621 accuracy: 11.236666666666666\n",
      "\n",
      "Test set: Average loss: 2.3012, Accuracy: 1135/10000 (11%)\n",
      "\n",
      "Training - Epoch: 10 loss: 2.301251739501953 accuracy: 11.236666666666666\n",
      "\n",
      "Test set: Average loss: 2.3012, Accuracy: 1135/10000 (11%)\n",
      "\n",
      "Training - Epoch: 11 loss: 2.301251708730062 accuracy: 11.236666666666666\n",
      "\n",
      "Test set: Average loss: 2.3012, Accuracy: 1135/10000 (11%)\n",
      "\n",
      "Training - Epoch: 12 loss: 2.3012517832438153 accuracy: 11.236666666666666\n",
      "\n",
      "Test set: Average loss: 2.3012, Accuracy: 1135/10000 (11%)\n",
      "\n",
      "Training - Epoch: 13 loss: 2.301251761372884 accuracy: 11.236666666666666\n",
      "\n",
      "Test set: Average loss: 2.3012, Accuracy: 1135/10000 (11%)\n",
      "\n",
      "Training - Epoch: 14 loss: 2.301251777648926 accuracy: 11.236666666666666\n",
      "\n",
      "Test set: Average loss: 2.3012, Accuracy: 1135/10000 (11%)\n",
      "\n",
      "Training - Epoch: 15 loss: 2.3012517537434896 accuracy: 11.236666666666666\n",
      "\n",
      "Test set: Average loss: 2.3012, Accuracy: 1135/10000 (11%)\n",
      "\n",
      "Training - Epoch: 16 loss: 2.301251776123047 accuracy: 11.236666666666666\n",
      "\n",
      "Test set: Average loss: 2.3012, Accuracy: 1135/10000 (11%)\n",
      "\n",
      "Training - Epoch: 17 loss: 2.3012517440795897 accuracy: 11.236666666666666\n",
      "\n",
      "Test set: Average loss: 2.3012, Accuracy: 1135/10000 (11%)\n",
      "\n",
      "Training - Epoch: 18 loss: 2.3012517389933267 accuracy: 11.236666666666666\n",
      "\n",
      "Test set: Average loss: 2.3012, Accuracy: 1135/10000 (11%)\n",
      "\n",
      "Training - Epoch: 19 loss: 2.3012517837524413 accuracy: 11.236666666666666\n",
      "\n",
      "Test set: Average loss: 2.3012, Accuracy: 1135/10000 (11%)\n",
      "\n",
      "Training - Epoch: 20 loss: 2.301251756032308 accuracy: 11.236666666666666\n",
      "\n",
      "Test set: Average loss: 2.3012, Accuracy: 1135/10000 (11%)\n",
      "\n",
      "Training - Epoch: 21 loss: 2.3012517300923667 accuracy: 11.236666666666666\n",
      "\n",
      "Test set: Average loss: 2.3012, Accuracy: 1135/10000 (11%)\n",
      "\n",
      "Training - Epoch: 22 loss: 2.3012517822265623 accuracy: 11.236666666666666\n",
      "\n",
      "Test set: Average loss: 2.3012, Accuracy: 1135/10000 (11%)\n",
      "\n",
      "Training - Epoch: 23 loss: 2.301251718393962 accuracy: 11.236666666666666\n",
      "\n",
      "Test set: Average loss: 2.3012, Accuracy: 1135/10000 (11%)\n",
      "\n",
      "Training - Epoch: 24 loss: 2.3012517618815105 accuracy: 11.236666666666666\n",
      "\n",
      "Test set: Average loss: 2.3012, Accuracy: 1135/10000 (11%)\n",
      "\n",
      "Training - Epoch: 25 loss: 2.301251769765218 accuracy: 11.236666666666666\n",
      "\n",
      "Test set: Average loss: 2.3012, Accuracy: 1135/10000 (11%)\n",
      "\n",
      "Training - Epoch: 26 loss: 2.3012517801920573 accuracy: 11.236666666666666\n",
      "\n",
      "Test set: Average loss: 2.3012, Accuracy: 1135/10000 (11%)\n",
      "\n",
      "Training - Epoch: 27 loss: 2.3012517717997234 accuracy: 11.236666666666666\n",
      "\n",
      "Test set: Average loss: 2.3012, Accuracy: 1135/10000 (11%)\n",
      "\n",
      "Training - Epoch: 28 loss: 2.3012517453511556 accuracy: 11.236666666666666\n",
      "\n",
      "Test set: Average loss: 2.3012, Accuracy: 1135/10000 (11%)\n",
      "\n",
      "Training - Epoch: 29 loss: 2.3012517433166506 accuracy: 11.236666666666666\n",
      "\n",
      "Test set: Average loss: 2.3012, Accuracy: 1135/10000 (11%)\n",
      "\n",
      "Training - Epoch: 30 loss: 2.301251754506429 accuracy: 11.236666666666666\n",
      "\n",
      "Test set: Average loss: 2.3012, Accuracy: 1135/10000 (11%)\n",
      "\n",
      "Training - Epoch: 31 loss: 2.301251741027832 accuracy: 11.236666666666666\n",
      "\n",
      "Test set: Average loss: 2.3012, Accuracy: 1135/10000 (11%)\n",
      "\n",
      "Training - Epoch: 32 loss: 2.30125171585083 accuracy: 11.236666666666666\n",
      "\n",
      "Test set: Average loss: 2.3012, Accuracy: 1135/10000 (11%)\n",
      "\n",
      "Training - Epoch: 33 loss: 2.3012518056233726 accuracy: 11.236666666666666\n",
      "\n",
      "Test set: Average loss: 2.3012, Accuracy: 1135/10000 (11%)\n",
      "\n",
      "Training - Epoch: 34 loss: 2.3012517972310382 accuracy: 11.236666666666666\n",
      "\n",
      "Test set: Average loss: 2.3012, Accuracy: 1135/10000 (11%)\n",
      "\n",
      "Training - Epoch: 35 loss: 2.3012517682393394 accuracy: 11.236666666666666\n",
      "\n",
      "Test set: Average loss: 2.3012, Accuracy: 1135/10000 (11%)\n",
      "\n",
      "Training - Epoch: 36 loss: 2.301251744588216 accuracy: 11.236666666666666\n",
      "\n",
      "Test set: Average loss: 2.3012, Accuracy: 1135/10000 (11%)\n",
      "\n",
      "Training - Epoch: 37 loss: 2.3012517387390137 accuracy: 11.236666666666666\n",
      "\n",
      "Test set: Average loss: 2.3012, Accuracy: 1135/10000 (11%)\n",
      "\n",
      "Training - Epoch: 38 loss: 2.301251737976074 accuracy: 11.236666666666666\n",
      "\n",
      "Test set: Average loss: 2.3012, Accuracy: 1135/10000 (11%)\n",
      "\n",
      "Training - Epoch: 39 loss: 2.301251735941569 accuracy: 11.236666666666666\n",
      "\n",
      "Test set: Average loss: 2.3012, Accuracy: 1135/10000 (11%)\n",
      "\n",
      "Training - Epoch: 40 loss: 2.3012517753601074 accuracy: 11.236666666666666\n",
      "\n",
      "Test set: Average loss: 2.3012, Accuracy: 1135/10000 (11%)\n",
      "\n",
      "Training - Epoch: 41 loss: 2.3012517957051597 accuracy: 11.236666666666666\n",
      "\n",
      "Test set: Average loss: 2.3012, Accuracy: 1135/10000 (11%)\n",
      "\n",
      "Training - Epoch: 42 loss: 2.3012517837524413 accuracy: 11.236666666666666\n",
      "\n",
      "Test set: Average loss: 2.3012, Accuracy: 1135/10000 (11%)\n",
      "\n",
      "Training - Epoch: 43 loss: 2.301251763153076 accuracy: 11.236666666666666\n",
      "\n",
      "Test set: Average loss: 2.3012, Accuracy: 1135/10000 (11%)\n",
      "\n",
      "Training - Epoch: 44 loss: 2.301251784261068 accuracy: 11.236666666666666\n",
      "\n",
      "Test set: Average loss: 2.3012, Accuracy: 1135/10000 (11%)\n",
      "\n",
      "Training - Epoch: 45 loss: 2.301251758066813 accuracy: 11.236666666666666\n",
      "\n",
      "Test set: Average loss: 2.3012, Accuracy: 1135/10000 (11%)\n",
      "\n",
      "Training - Epoch: 46 loss: 2.3012517283121743 accuracy: 11.236666666666666\n",
      "\n",
      "Test set: Average loss: 2.3012, Accuracy: 1135/10000 (11%)\n",
      "\n",
      "Training - Epoch: 47 loss: 2.301251777648926 accuracy: 11.236666666666666\n",
      "\n",
      "Test set: Average loss: 2.3012, Accuracy: 1135/10000 (11%)\n",
      "\n",
      "Training - Epoch: 48 loss: 2.3012517735799154 accuracy: 11.236666666666666\n",
      "\n",
      "Test set: Average loss: 2.3012, Accuracy: 1135/10000 (11%)\n",
      "\n",
      "Training - Epoch: 49 loss: 2.3012517471313476 accuracy: 11.236666666666666\n",
      "\n",
      "Test set: Average loss: 2.3012, Accuracy: 1135/10000 (11%)\n",
      "\n",
      "Training - Epoch: 50 loss: 2.301251788075765 accuracy: 11.236666666666666\n",
      "\n",
      "Test set: Average loss: 2.3012, Accuracy: 1135/10000 (11%)\n",
      "\n",
      "Training - Epoch: 51 loss: 2.301251753234863 accuracy: 11.236666666666666\n",
      "\n",
      "Test set: Average loss: 2.3012, Accuracy: 1135/10000 (11%)\n",
      "\n",
      "Training - Epoch: 52 loss: 2.3012517621358235 accuracy: 11.236666666666666\n",
      "\n",
      "Test set: Average loss: 2.3012, Accuracy: 1135/10000 (11%)\n",
      "\n",
      "Training - Epoch: 53 loss: 2.301251772816976 accuracy: 11.236666666666666\n",
      "\n",
      "Test set: Average loss: 2.3012, Accuracy: 1135/10000 (11%)\n",
      "\n",
      "Training - Epoch: 54 loss: 2.3012517288208008 accuracy: 11.236666666666666\n",
      "\n",
      "Test set: Average loss: 2.3012, Accuracy: 1135/10000 (11%)\n",
      "\n",
      "Training - Epoch: 55 loss: 2.3012517702738444 accuracy: 11.236666666666666\n",
      "\n",
      "Test set: Average loss: 2.3012, Accuracy: 1135/10000 (11%)\n",
      "\n",
      "Training - Epoch: 56 loss: 2.3012517911275228 accuracy: 11.236666666666666\n",
      "\n",
      "Test set: Average loss: 2.3012, Accuracy: 1135/10000 (11%)\n",
      "\n",
      "Training - Epoch: 57 loss: 2.301251750946045 accuracy: 11.236666666666666\n",
      "\n",
      "Test set: Average loss: 2.3012, Accuracy: 1135/10000 (11%)\n",
      "\n",
      "Training - Epoch: 58 loss: 2.301251741027832 accuracy: 11.236666666666666\n",
      "\n",
      "Test set: Average loss: 2.3012, Accuracy: 1135/10000 (11%)\n",
      "\n",
      "Training - Epoch: 59 loss: 2.301251749420166 accuracy: 11.236666666666666\n",
      "\n",
      "Test set: Average loss: 2.3012, Accuracy: 1135/10000 (11%)\n",
      "\n",
      "Training - Epoch: 60 loss: 2.3012517621358235 accuracy: 11.236666666666666\n",
      "\n",
      "Test set: Average loss: 2.3012, Accuracy: 1135/10000 (11%)\n",
      "\n",
      "Training - Epoch: 61 loss: 2.3012517992655437 accuracy: 11.236666666666666\n",
      "\n",
      "Test set: Average loss: 2.3012, Accuracy: 1135/10000 (11%)\n",
      "\n",
      "Training - Epoch: 62 loss: 2.3012517283121743 accuracy: 11.236666666666666\n",
      "\n",
      "Test set: Average loss: 2.3012, Accuracy: 1135/10000 (11%)\n",
      "\n",
      "Training - Epoch: 63 loss: 2.3012517450968426 accuracy: 11.236666666666666\n",
      "\n",
      "Test set: Average loss: 2.3012, Accuracy: 1135/10000 (11%)\n",
      "\n",
      "Training - Epoch: 64 loss: 2.301251741536458 accuracy: 11.236666666666666\n",
      "\n",
      "Test set: Average loss: 2.3012, Accuracy: 1135/10000 (11%)\n",
      "\n",
      "Training - Epoch: 65 loss: 2.301251747894287 accuracy: 11.236666666666666\n",
      "\n",
      "Test set: Average loss: 2.3012, Accuracy: 1135/10000 (11%)\n",
      "\n",
      "Training - Epoch: 66 loss: 2.3012517349243162 accuracy: 11.236666666666666\n",
      "\n",
      "Test set: Average loss: 2.3012, Accuracy: 1135/10000 (11%)\n",
      "\n",
      "Training - Epoch: 67 loss: 2.301251763153076 accuracy: 11.236666666666666\n",
      "\n",
      "Test set: Average loss: 2.3012, Accuracy: 1135/10000 (11%)\n",
      "\n",
      "Training - Epoch: 68 loss: 2.3012517654418945 accuracy: 11.236666666666666\n",
      "\n",
      "Test set: Average loss: 2.3012, Accuracy: 1135/10000 (11%)\n",
      "\n",
      "Training - Epoch: 69 loss: 2.301251752471924 accuracy: 11.236666666666666\n",
      "\n",
      "Test set: Average loss: 2.3012, Accuracy: 1135/10000 (11%)\n",
      "\n",
      "Training - Epoch: 70 loss: 2.3012517684936524 accuracy: 11.236666666666666\n",
      "\n",
      "Test set: Average loss: 2.3012, Accuracy: 1135/10000 (11%)\n",
      "\n",
      "Training - Epoch: 71 loss: 2.3012517199198403 accuracy: 11.236666666666666\n",
      "\n",
      "Test set: Average loss: 2.3012, Accuracy: 1135/10000 (11%)\n",
      "\n",
      "Training - Epoch: 72 loss: 2.301251744333903 accuracy: 11.236666666666666\n",
      "\n",
      "Test set: Average loss: 2.3012, Accuracy: 1135/10000 (11%)\n",
      "\n",
      "Training - Epoch: 73 loss: 2.30125177154541 accuracy: 11.236666666666666\n",
      "\n",
      "Test set: Average loss: 2.3012, Accuracy: 1135/10000 (11%)\n",
      "\n",
      "Training - Epoch: 74 loss: 2.301251754760742 accuracy: 11.236666666666666\n",
      "\n",
      "Test set: Average loss: 2.3012, Accuracy: 1135/10000 (11%)\n",
      "\n",
      "Training - Epoch: 75 loss: 2.3012517367045087 accuracy: 11.236666666666666\n",
      "\n",
      "Test set: Average loss: 2.3012, Accuracy: 1135/10000 (11%)\n",
      "\n",
      "Training - Epoch: 76 loss: 2.3012518074035646 accuracy: 11.236666666666666\n",
      "\n",
      "Test set: Average loss: 2.3012, Accuracy: 1135/10000 (11%)\n",
      "\n",
      "Training - Epoch: 77 loss: 2.301251737721761 accuracy: 11.236666666666666\n",
      "\n",
      "Test set: Average loss: 2.3012, Accuracy: 1135/10000 (11%)\n",
      "\n",
      "Training - Epoch: 78 loss: 2.3012517926534017 accuracy: 11.236666666666666\n",
      "\n",
      "Test set: Average loss: 2.3012, Accuracy: 1135/10000 (11%)\n",
      "\n",
      "Training - Epoch: 79 loss: 2.3012517433166506 accuracy: 11.236666666666666\n",
      "\n",
      "Test set: Average loss: 2.3012, Accuracy: 1135/10000 (11%)\n",
      "\n",
      "Training - Epoch: 80 loss: 2.301251776123047 accuracy: 11.236666666666666\n",
      "\n",
      "Test set: Average loss: 2.3012, Accuracy: 1135/10000 (11%)\n",
      "\n",
      "Training - Epoch: 81 loss: 2.3012517252604168 accuracy: 11.236666666666666\n",
      "\n",
      "Test set: Average loss: 2.3012, Accuracy: 1135/10000 (11%)\n",
      "\n",
      "Training - Epoch: 82 loss: 2.30125173924764 accuracy: 11.236666666666666\n",
      "\n",
      "Test set: Average loss: 2.3012, Accuracy: 1135/10000 (11%)\n",
      "\n",
      "Training - Epoch: 83 loss: 2.3012517618815105 accuracy: 11.236666666666666\n",
      "\n",
      "Test set: Average loss: 2.3012, Accuracy: 1135/10000 (11%)\n",
      "\n",
      "Training - Epoch: 84 loss: 2.301251738230387 accuracy: 11.236666666666666\n",
      "\n",
      "Test set: Average loss: 2.3012, Accuracy: 1135/10000 (11%)\n",
      "\n",
      "Training - Epoch: 85 loss: 2.301251750946045 accuracy: 11.236666666666666\n",
      "\n",
      "Test set: Average loss: 2.3012, Accuracy: 1135/10000 (11%)\n",
      "\n",
      "Training - Epoch: 86 loss: 2.3012517227172853 accuracy: 11.236666666666666\n",
      "\n",
      "Test set: Average loss: 2.3012, Accuracy: 1135/10000 (11%)\n",
      "\n",
      "Training - Epoch: 87 loss: 2.3012517504374186 accuracy: 11.236666666666666\n",
      "\n",
      "Test set: Average loss: 2.3012, Accuracy: 1135/10000 (11%)\n",
      "\n",
      "Training - Epoch: 88 loss: 2.3012517267862957 accuracy: 11.236666666666666\n",
      "\n",
      "Test set: Average loss: 2.3012, Accuracy: 1135/10000 (11%)\n",
      "\n",
      "Training - Epoch: 89 loss: 2.301251751454671 accuracy: 11.236666666666666\n",
      "\n",
      "Test set: Average loss: 2.3012, Accuracy: 1135/10000 (11%)\n",
      "\n",
      "Training - Epoch: 90 loss: 2.3012517822265623 accuracy: 11.236666666666666\n",
      "\n",
      "Test set: Average loss: 2.3012, Accuracy: 1135/10000 (11%)\n",
      "\n",
      "Training - Epoch: 91 loss: 2.3012517822265623 accuracy: 11.236666666666666\n",
      "\n",
      "Test set: Average loss: 2.3012, Accuracy: 1135/10000 (11%)\n",
      "\n",
      "Training - Epoch: 92 loss: 2.3012517870585123 accuracy: 11.236666666666666\n",
      "\n",
      "Test set: Average loss: 2.3012, Accuracy: 1135/10000 (11%)\n",
      "\n",
      "Training - Epoch: 93 loss: 2.301251744588216 accuracy: 11.236666666666666\n",
      "\n",
      "Test set: Average loss: 2.3012, Accuracy: 1135/10000 (11%)\n",
      "\n",
      "Training - Epoch: 94 loss: 2.301251788075765 accuracy: 11.236666666666666\n",
      "\n",
      "Test set: Average loss: 2.3012, Accuracy: 1135/10000 (11%)\n",
      "\n",
      "Training - Epoch: 95 loss: 2.301251749420166 accuracy: 11.236666666666666\n",
      "\n",
      "Test set: Average loss: 2.3012, Accuracy: 1135/10000 (11%)\n",
      "\n",
      "Training - Epoch: 96 loss: 2.301251754252116 accuracy: 11.236666666666666\n",
      "\n",
      "Test set: Average loss: 2.3012, Accuracy: 1135/10000 (11%)\n",
      "\n",
      "Training - Epoch: 97 loss: 2.3012517649332684 accuracy: 11.236666666666666\n",
      "\n",
      "Test set: Average loss: 2.3012, Accuracy: 1135/10000 (11%)\n",
      "\n",
      "Training - Epoch: 98 loss: 2.3012517618815105 accuracy: 11.236666666666666\n",
      "\n",
      "Test set: Average loss: 2.3012, Accuracy: 1135/10000 (11%)\n",
      "\n",
      "Training - Epoch: 99 loss: 2.301251784515381 accuracy: 11.236666666666666\n",
      "\n",
      "Test set: Average loss: 2.3012, Accuracy: 1135/10000 (11%)\n",
      "\n",
      "Training - Epoch: 100 loss: 2.30125176264445 accuracy: 11.236666666666666\n",
      "\n",
      "Test set: Average loss: 2.3012, Accuracy: 1135/10000 (11%)\n",
      "\n",
      "Training - Epoch: 101 loss: 2.301251787821452 accuracy: 11.236666666666666\n",
      "\n",
      "Test set: Average loss: 2.3012, Accuracy: 1135/10000 (11%)\n",
      "\n",
      "Training - Epoch: 102 loss: 2.301251764170329 accuracy: 11.236666666666666\n",
      "\n",
      "Test set: Average loss: 2.3012, Accuracy: 1135/10000 (11%)\n",
      "\n",
      "Training - Epoch: 103 loss: 2.3012517804463704 accuracy: 11.236666666666666\n",
      "\n",
      "Test set: Average loss: 2.3012, Accuracy: 1135/10000 (11%)\n",
      "\n",
      "Training - Epoch: 104 loss: 2.3012517926534017 accuracy: 11.236666666666666\n",
      "\n",
      "Test set: Average loss: 2.3012, Accuracy: 1135/10000 (11%)\n",
      "\n",
      "Training - Epoch: 105 loss: 2.301251786295573 accuracy: 11.236666666666666\n",
      "\n",
      "Test set: Average loss: 2.3012, Accuracy: 1135/10000 (11%)\n",
      "\n",
      "Training - Epoch: 106 loss: 2.30125175298055 accuracy: 11.236666666666666\n",
      "\n",
      "Test set: Average loss: 2.3012, Accuracy: 1135/10000 (11%)\n",
      "\n",
      "Training - Epoch: 107 loss: 2.3012517753601074 accuracy: 11.236666666666666\n",
      "\n",
      "Test set: Average loss: 2.3012, Accuracy: 1135/10000 (11%)\n",
      "\n",
      "Training - Epoch: 108 loss: 2.301251746368408 accuracy: 11.236666666666666\n",
      "\n",
      "Test set: Average loss: 2.3012, Accuracy: 1135/10000 (11%)\n",
      "\n",
      "Training - Epoch: 109 loss: 2.3012517583211265 accuracy: 11.236666666666666\n",
      "\n",
      "Test set: Average loss: 2.3012, Accuracy: 1135/10000 (11%)\n",
      "\n",
      "Training - Epoch: 110 loss: 2.3012517567952475 accuracy: 11.236666666666666\n",
      "\n",
      "Test set: Average loss: 2.3012, Accuracy: 1135/10000 (11%)\n",
      "\n",
      "Training - Epoch: 111 loss: 2.30125173924764 accuracy: 11.236666666666666\n",
      "\n",
      "Test set: Average loss: 2.3012, Accuracy: 1135/10000 (11%)\n",
      "\n",
      "Training - Epoch: 112 loss: 2.3012517583211265 accuracy: 11.236666666666666\n",
      "\n",
      "Test set: Average loss: 2.3012, Accuracy: 1135/10000 (11%)\n",
      "\n",
      "Training - Epoch: 113 loss: 2.3012517603556315 accuracy: 11.236666666666666\n",
      "\n",
      "Test set: Average loss: 2.3012, Accuracy: 1135/10000 (11%)\n",
      "\n",
      "Training - Epoch: 114 loss: 2.301251766459147 accuracy: 11.236666666666666\n",
      "\n",
      "Test set: Average loss: 2.3012, Accuracy: 1135/10000 (11%)\n",
      "\n",
      "Training - Epoch: 115 loss: 2.3012517921447753 accuracy: 11.236666666666666\n",
      "\n",
      "Test set: Average loss: 2.3012, Accuracy: 1135/10000 (11%)\n",
      "\n",
      "Training - Epoch: 116 loss: 2.301251761372884 accuracy: 11.236666666666666\n",
      "\n",
      "Test set: Average loss: 2.3012, Accuracy: 1135/10000 (11%)\n",
      "\n",
      "Training - Epoch: 117 loss: 2.301251741027832 accuracy: 11.236666666666666\n",
      "\n",
      "Test set: Average loss: 2.3012, Accuracy: 1135/10000 (11%)\n",
      "\n",
      "Training - Epoch: 118 loss: 2.301251777903239 accuracy: 11.236666666666666\n",
      "\n",
      "Test set: Average loss: 2.3012, Accuracy: 1135/10000 (11%)\n",
      "\n",
      "Training - Epoch: 119 loss: 2.3012517504374186 accuracy: 11.236666666666666\n",
      "\n",
      "Test set: Average loss: 2.3012, Accuracy: 1135/10000 (11%)\n",
      "\n",
      "Training - Epoch: 120 loss: 2.301251752471924 accuracy: 11.236666666666666\n",
      "\n",
      "Test set: Average loss: 2.3012, Accuracy: 1135/10000 (11%)\n",
      "\n",
      "Training - Epoch: 121 loss: 2.3012517280578613 accuracy: 11.236666666666666\n",
      "\n",
      "Test set: Average loss: 2.3012, Accuracy: 1135/10000 (11%)\n",
      "\n",
      "Training - Epoch: 122 loss: 2.3012517883300783 accuracy: 11.236666666666666\n",
      "\n",
      "Test set: Average loss: 2.3012, Accuracy: 1135/10000 (11%)\n",
      "\n",
      "Training - Epoch: 123 loss: 2.301251758066813 accuracy: 11.236666666666666\n",
      "\n",
      "Test set: Average loss: 2.3012, Accuracy: 1135/10000 (11%)\n",
      "\n",
      "Training - Epoch: 124 loss: 2.3012517786661784 accuracy: 11.236666666666666\n",
      "\n",
      "Test set: Average loss: 2.3012, Accuracy: 1135/10000 (11%)\n",
      "\n",
      "Training - Epoch: 125 loss: 2.301251759592692 accuracy: 11.236666666666666\n",
      "\n",
      "Test set: Average loss: 2.3012, Accuracy: 1135/10000 (11%)\n",
      "\n",
      "Training - Epoch: 126 loss: 2.3012517634073895 accuracy: 11.236666666666666\n",
      "\n",
      "Test set: Average loss: 2.3012, Accuracy: 1135/10000 (11%)\n",
      "\n",
      "Training - Epoch: 127 loss: 2.301251741536458 accuracy: 11.236666666666666\n",
      "\n",
      "Test set: Average loss: 2.3012, Accuracy: 1135/10000 (11%)\n",
      "\n",
      "Training - Epoch: 128 loss: 2.3012517578125 accuracy: 11.236666666666666\n",
      "\n",
      "Test set: Average loss: 2.3012, Accuracy: 1135/10000 (11%)\n",
      "\n",
      "Training - Epoch: 129 loss: 2.3012517621358235 accuracy: 11.236666666666666\n",
      "\n",
      "Test set: Average loss: 2.3012, Accuracy: 1135/10000 (11%)\n",
      "\n",
      "Training - Epoch: 130 loss: 2.3012517651875815 accuracy: 11.236666666666666\n",
      "\n",
      "Test set: Average loss: 2.3012, Accuracy: 1135/10000 (11%)\n",
      "\n",
      "Training - Epoch: 131 loss: 2.3012517667134604 accuracy: 11.236666666666666\n",
      "\n",
      "Test set: Average loss: 2.3012, Accuracy: 1135/10000 (11%)\n",
      "\n",
      "Training - Epoch: 132 loss: 2.301251774342855 accuracy: 11.236666666666666\n",
      "\n",
      "Test set: Average loss: 2.3012, Accuracy: 1135/10000 (11%)\n",
      "\n",
      "Training - Epoch: 133 loss: 2.3012517341613767 accuracy: 11.236666666666666\n",
      "\n",
      "Test set: Average loss: 2.3012, Accuracy: 1135/10000 (11%)\n",
      "\n",
      "Training - Epoch: 134 loss: 2.3012517634073895 accuracy: 11.236666666666666\n",
      "\n",
      "Test set: Average loss: 2.3012, Accuracy: 1135/10000 (11%)\n",
      "\n",
      "Training - Epoch: 135 loss: 2.3012517639160155 accuracy: 11.236666666666666\n",
      "\n",
      "Test set: Average loss: 2.3012, Accuracy: 1135/10000 (11%)\n",
      "\n",
      "Training - Epoch: 136 loss: 2.3012517440795897 accuracy: 11.236666666666666\n",
      "\n",
      "Test set: Average loss: 2.3012, Accuracy: 1135/10000 (11%)\n",
      "\n",
      "Training - Epoch: 137 loss: 2.3012517789204914 accuracy: 11.236666666666666\n",
      "\n",
      "Test set: Average loss: 2.3012, Accuracy: 1135/10000 (11%)\n",
      "\n",
      "Training - Epoch: 138 loss: 2.3012517753601074 accuracy: 11.236666666666666\n",
      "\n",
      "Test set: Average loss: 2.3012, Accuracy: 1135/10000 (11%)\n",
      "\n",
      "Training - Epoch: 139 loss: 2.301251730855306 accuracy: 11.236666666666666\n",
      "\n",
      "Test set: Average loss: 2.3012, Accuracy: 1135/10000 (11%)\n",
      "\n",
      "Training - Epoch: 140 loss: 2.3012517374674477 accuracy: 11.236666666666666\n",
      "\n",
      "Test set: Average loss: 2.3012, Accuracy: 1135/10000 (11%)\n",
      "\n",
      "Training - Epoch: 141 loss: 2.30125179494222 accuracy: 11.236666666666666\n",
      "\n",
      "Test set: Average loss: 2.3012, Accuracy: 1135/10000 (11%)\n",
      "\n",
      "Training - Epoch: 142 loss: 2.3012517651875815 accuracy: 11.236666666666666\n",
      "\n",
      "Test set: Average loss: 2.3012, Accuracy: 1135/10000 (11%)\n",
      "\n",
      "Training - Epoch: 143 loss: 2.3012517789204914 accuracy: 11.236666666666666\n",
      "\n",
      "Test set: Average loss: 2.3012, Accuracy: 1135/10000 (11%)\n",
      "\n",
      "Training - Epoch: 144 loss: 2.3012517918904623 accuracy: 11.236666666666666\n",
      "\n",
      "Test set: Average loss: 2.3012, Accuracy: 1135/10000 (11%)\n",
      "\n",
      "Training - Epoch: 145 loss: 2.3012517311096192 accuracy: 11.236666666666666\n",
      "\n",
      "Test set: Average loss: 2.3012, Accuracy: 1135/10000 (11%)\n",
      "\n",
      "Training - Epoch: 146 loss: 2.3012517400105796 accuracy: 11.236666666666666\n",
      "\n",
      "Test set: Average loss: 2.3012, Accuracy: 1135/10000 (11%)\n",
      "\n",
      "Training - Epoch: 147 loss: 2.3012517519632976 accuracy: 11.236666666666666\n",
      "\n",
      "Test set: Average loss: 2.3012, Accuracy: 1135/10000 (11%)\n",
      "\n",
      "Training - Epoch: 148 loss: 2.301251780700684 accuracy: 11.236666666666666\n",
      "\n",
      "Test set: Average loss: 2.3012, Accuracy: 1135/10000 (11%)\n",
      "\n",
      "Training - Epoch: 149 loss: 2.301251756032308 accuracy: 11.236666666666666\n",
      "\n",
      "Test set: Average loss: 2.3012, Accuracy: 1135/10000 (11%)\n",
      "\n",
      "Training - Epoch: 150 loss: 2.30125175298055 accuracy: 11.236666666666666\n",
      "\n",
      "Test set: Average loss: 2.3012, Accuracy: 1135/10000 (11%)\n",
      "\n",
      "Training - Epoch: 151 loss: 2.3012517784118653 accuracy: 11.236666666666666\n",
      "\n",
      "Test set: Average loss: 2.3012, Accuracy: 1135/10000 (11%)\n",
      "\n",
      "Training - Epoch: 152 loss: 2.301251780954997 accuracy: 11.236666666666666\n",
      "\n",
      "Test set: Average loss: 2.3012, Accuracy: 1135/10000 (11%)\n",
      "\n",
      "Training - Epoch: 153 loss: 2.301251778157552 accuracy: 11.236666666666666\n",
      "\n",
      "Test set: Average loss: 2.3012, Accuracy: 1135/10000 (11%)\n",
      "\n",
      "Training - Epoch: 154 loss: 2.3012517250061033 accuracy: 11.236666666666666\n",
      "\n",
      "Test set: Average loss: 2.3012, Accuracy: 1135/10000 (11%)\n",
      "\n",
      "Training - Epoch: 155 loss: 2.3012517293294272 accuracy: 11.236666666666666\n",
      "\n",
      "Test set: Average loss: 2.3012, Accuracy: 1135/10000 (11%)\n",
      "\n",
      "Training - Epoch: 156 loss: 2.301251785786947 accuracy: 11.236666666666666\n",
      "\n",
      "Test set: Average loss: 2.3012, Accuracy: 1135/10000 (11%)\n",
      "\n",
      "Training - Epoch: 157 loss: 2.3012517290751138 accuracy: 11.236666666666666\n",
      "\n",
      "Test set: Average loss: 2.3012, Accuracy: 1135/10000 (11%)\n",
      "\n",
      "Training - Epoch: 158 loss: 2.301251757558187 accuracy: 11.236666666666666\n",
      "\n",
      "Test set: Average loss: 2.3012, Accuracy: 1135/10000 (11%)\n",
      "\n",
      "Training - Epoch: 159 loss: 2.3012517473856606 accuracy: 11.236666666666666\n",
      "\n",
      "Test set: Average loss: 2.3012, Accuracy: 1135/10000 (11%)\n",
      "\n",
      "Training - Epoch: 160 loss: 2.3012517351786297 accuracy: 11.236666666666666\n",
      "\n",
      "Test set: Average loss: 2.3012, Accuracy: 1135/10000 (11%)\n",
      "\n",
      "Training - Epoch: 161 loss: 2.30125177637736 accuracy: 11.236666666666666\n",
      "\n",
      "Test set: Average loss: 2.3012, Accuracy: 1135/10000 (11%)\n",
      "\n",
      "Training - Epoch: 162 loss: 2.3012517850240073 accuracy: 11.236666666666666\n",
      "\n",
      "Test set: Average loss: 2.3012, Accuracy: 1135/10000 (11%)\n",
      "\n",
      "Training - Epoch: 163 loss: 2.3012517473856606 accuracy: 11.236666666666666\n",
      "\n",
      "Test set: Average loss: 2.3012, Accuracy: 1135/10000 (11%)\n",
      "\n",
      "Training - Epoch: 164 loss: 2.3012517570495605 accuracy: 11.236666666666666\n",
      "\n",
      "Test set: Average loss: 2.3012, Accuracy: 1135/10000 (11%)\n",
      "\n",
      "Training - Epoch: 165 loss: 2.3012517817179363 accuracy: 11.236666666666666\n",
      "\n",
      "Test set: Average loss: 2.3012, Accuracy: 1135/10000 (11%)\n",
      "\n",
      "Training - Epoch: 166 loss: 2.301251760864258 accuracy: 11.236666666666666\n",
      "\n",
      "Test set: Average loss: 2.3012, Accuracy: 1135/10000 (11%)\n",
      "\n",
      "Training - Epoch: 167 loss: 2.3012517573038735 accuracy: 11.236666666666666\n",
      "\n",
      "Test set: Average loss: 2.3012, Accuracy: 1135/10000 (11%)\n",
      "\n",
      "Training - Epoch: 168 loss: 2.301251746368408 accuracy: 11.236666666666666\n",
      "\n",
      "Test set: Average loss: 2.3012, Accuracy: 1135/10000 (11%)\n",
      "\n",
      "Training - Epoch: 169 loss: 2.301251746368408 accuracy: 11.236666666666666\n",
      "\n",
      "Test set: Average loss: 2.3012, Accuracy: 1135/10000 (11%)\n",
      "\n",
      "Training - Epoch: 170 loss: 2.3012517801920573 accuracy: 11.236666666666666\n",
      "\n",
      "Test set: Average loss: 2.3012, Accuracy: 1135/10000 (11%)\n",
      "\n",
      "Training - Epoch: 171 loss: 2.3012517672220865 accuracy: 11.236666666666666\n",
      "\n",
      "Test set: Average loss: 2.3012, Accuracy: 1135/10000 (11%)\n",
      "\n",
      "Training - Epoch: 172 loss: 2.3012517567952475 accuracy: 11.236666666666666\n",
      "\n",
      "Test set: Average loss: 2.3012, Accuracy: 1135/10000 (11%)\n",
      "\n",
      "Training - Epoch: 173 loss: 2.3012517690022785 accuracy: 11.236666666666666\n",
      "\n",
      "Test set: Average loss: 2.3012, Accuracy: 1135/10000 (11%)\n",
      "\n",
      "Training - Epoch: 174 loss: 2.30125179494222 accuracy: 11.236666666666666\n",
      "\n",
      "Test set: Average loss: 2.3012, Accuracy: 1135/10000 (11%)\n",
      "\n",
      "Training - Epoch: 175 loss: 2.301251744588216 accuracy: 11.236666666666666\n",
      "\n",
      "Test set: Average loss: 2.3012, Accuracy: 1135/10000 (11%)\n",
      "\n",
      "Training - Epoch: 176 loss: 2.3012517298380533 accuracy: 11.236666666666666\n",
      "\n",
      "Test set: Average loss: 2.3012, Accuracy: 1135/10000 (11%)\n",
      "\n",
      "Training - Epoch: 177 loss: 2.301251752471924 accuracy: 11.236666666666666\n",
      "\n",
      "Test set: Average loss: 2.3012, Accuracy: 1135/10000 (11%)\n",
      "\n",
      "Training - Epoch: 178 loss: 2.301251765950521 accuracy: 11.236666666666666\n",
      "\n",
      "Test set: Average loss: 2.3012, Accuracy: 1135/10000 (11%)\n",
      "\n",
      "Training - Epoch: 179 loss: 2.3012517573038735 accuracy: 11.236666666666666\n",
      "\n",
      "Test set: Average loss: 2.3012, Accuracy: 1135/10000 (11%)\n",
      "\n",
      "Training - Epoch: 180 loss: 2.30125177154541 accuracy: 11.236666666666666\n",
      "\n",
      "Test set: Average loss: 2.3012, Accuracy: 1135/10000 (11%)\n",
      "\n",
      "Training - Epoch: 181 loss: 2.301251745859782 accuracy: 11.236666666666666\n",
      "\n",
      "Test set: Average loss: 2.3012, Accuracy: 1135/10000 (11%)\n",
      "\n",
      "Training - Epoch: 182 loss: 2.3012517583211265 accuracy: 11.236666666666666\n",
      "\n",
      "Test set: Average loss: 2.3012, Accuracy: 1135/10000 (11%)\n",
      "\n",
      "Training - Epoch: 183 loss: 2.3012517603556315 accuracy: 11.236666666666666\n",
      "\n",
      "Test set: Average loss: 2.3012, Accuracy: 1135/10000 (11%)\n",
      "\n",
      "Training - Epoch: 184 loss: 2.301251800282796 accuracy: 11.236666666666666\n",
      "\n",
      "Test set: Average loss: 2.3012, Accuracy: 1135/10000 (11%)\n",
      "\n",
      "Training - Epoch: 185 loss: 2.301251772562663 accuracy: 11.236666666666666\n",
      "\n",
      "Test set: Average loss: 2.3012, Accuracy: 1135/10000 (11%)\n",
      "\n",
      "Training - Epoch: 186 loss: 2.301251774088542 accuracy: 11.236666666666666\n",
      "\n",
      "Test set: Average loss: 2.3012, Accuracy: 1135/10000 (11%)\n",
      "\n",
      "Training - Epoch: 187 loss: 2.3012517585754395 accuracy: 11.236666666666666\n",
      "\n",
      "Test set: Average loss: 2.3012, Accuracy: 1135/10000 (11%)\n",
      "\n",
      "Training - Epoch: 188 loss: 2.3012517700195314 accuracy: 11.236666666666666\n",
      "\n",
      "Test set: Average loss: 2.3012, Accuracy: 1135/10000 (11%)\n",
      "\n",
      "Training - Epoch: 189 loss: 2.301251777648926 accuracy: 11.236666666666666\n",
      "\n",
      "Test set: Average loss: 2.3012, Accuracy: 1135/10000 (11%)\n",
      "\n",
      "Training - Epoch: 190 loss: 2.3012517654418945 accuracy: 11.236666666666666\n",
      "\n",
      "Test set: Average loss: 2.3012, Accuracy: 1135/10000 (11%)\n",
      "\n",
      "Training - Epoch: 191 loss: 2.3012517669677734 accuracy: 11.236666666666666\n",
      "\n",
      "Test set: Average loss: 2.3012, Accuracy: 1135/10000 (11%)\n",
      "\n",
      "Training - Epoch: 192 loss: 2.3012517799377443 accuracy: 11.236666666666666\n",
      "\n",
      "Test set: Average loss: 2.3012, Accuracy: 1135/10000 (11%)\n",
      "\n",
      "Training - Epoch: 193 loss: 2.3012517333984377 accuracy: 11.236666666666666\n",
      "\n",
      "Test set: Average loss: 2.3012, Accuracy: 1135/10000 (11%)\n",
      "\n",
      "Training - Epoch: 194 loss: 2.3012517799377443 accuracy: 11.236666666666666\n",
      "\n",
      "Test set: Average loss: 2.3012, Accuracy: 1135/10000 (11%)\n",
      "\n",
      "Training - Epoch: 195 loss: 2.301251764170329 accuracy: 11.236666666666666\n",
      "\n",
      "Test set: Average loss: 2.3012, Accuracy: 1135/10000 (11%)\n",
      "\n",
      "Training - Epoch: 196 loss: 2.3012517735799154 accuracy: 11.236666666666666\n",
      "\n",
      "Test set: Average loss: 2.3012, Accuracy: 1135/10000 (11%)\n",
      "\n",
      "Training - Epoch: 197 loss: 2.3012517374674477 accuracy: 11.236666666666666\n",
      "\n",
      "Test set: Average loss: 2.3012, Accuracy: 1135/10000 (11%)\n",
      "\n",
      "Training - Epoch: 198 loss: 2.3012517435709636 accuracy: 11.236666666666666\n",
      "\n",
      "Test set: Average loss: 2.3012, Accuracy: 1135/10000 (11%)\n",
      "\n",
      "Training - Epoch: 199 loss: 2.301251759847005 accuracy: 11.236666666666666\n",
      "\n",
      "Test set: Average loss: 2.3012, Accuracy: 1135/10000 (11%)\n",
      "\n",
      "Training - Epoch: 200 loss: 2.3012517964680987 accuracy: 11.236666666666666\n",
      "\n",
      "Test set: Average loss: 2.3012, Accuracy: 1135/10000 (11%)\n",
      "\n",
      "Training - Epoch: 201 loss: 2.301251749165853 accuracy: 11.236666666666666\n",
      "\n",
      "Test set: Average loss: 2.3012, Accuracy: 1135/10000 (11%)\n",
      "\n",
      "Training - Epoch: 202 loss: 2.3012517402648927 accuracy: 11.236666666666666\n",
      "\n",
      "Test set: Average loss: 2.3012, Accuracy: 1135/10000 (11%)\n",
      "\n",
      "Training - Epoch: 203 loss: 2.301251767985026 accuracy: 11.236666666666666\n",
      "\n",
      "Test set: Average loss: 2.3012, Accuracy: 1135/10000 (11%)\n",
      "\n",
      "Training - Epoch: 204 loss: 2.301251756286621 accuracy: 11.236666666666666\n",
      "\n",
      "Test set: Average loss: 2.3012, Accuracy: 1135/10000 (11%)\n",
      "\n",
      "Training - Epoch: 205 loss: 2.3012517700195314 accuracy: 11.236666666666666\n",
      "\n",
      "Test set: Average loss: 2.3012, Accuracy: 1135/10000 (11%)\n",
      "\n",
      "Training - Epoch: 206 loss: 2.3012517016092935 accuracy: 11.236666666666666\n",
      "\n",
      "Test set: Average loss: 2.3012, Accuracy: 1135/10000 (11%)\n",
      "\n",
      "Training - Epoch: 207 loss: 2.301251758066813 accuracy: 11.236666666666666\n",
      "\n",
      "Test set: Average loss: 2.3012, Accuracy: 1135/10000 (11%)\n",
      "\n",
      "Training - Epoch: 208 loss: 2.301251767985026 accuracy: 11.236666666666666\n",
      "\n",
      "Test set: Average loss: 2.3012, Accuracy: 1135/10000 (11%)\n",
      "\n",
      "Training - Epoch: 209 loss: 2.3012517267862957 accuracy: 11.236666666666666\n",
      "\n",
      "Test set: Average loss: 2.3012, Accuracy: 1135/10000 (11%)\n",
      "\n",
      "Training - Epoch: 210 loss: 2.301251749674479 accuracy: 11.236666666666666\n",
      "\n",
      "Test set: Average loss: 2.3012, Accuracy: 1135/10000 (11%)\n",
      "\n",
      "Training - Epoch: 211 loss: 2.301251788075765 accuracy: 11.236666666666666\n",
      "\n",
      "Test set: Average loss: 2.3012, Accuracy: 1135/10000 (11%)\n",
      "\n",
      "Training - Epoch: 212 loss: 2.3012517293294272 accuracy: 11.236666666666666\n",
      "\n",
      "Test set: Average loss: 2.3012, Accuracy: 1135/10000 (11%)\n",
      "\n",
      "Training - Epoch: 213 loss: 2.3012517384847007 accuracy: 11.236666666666666\n",
      "\n",
      "Test set: Average loss: 2.3012, Accuracy: 1135/10000 (11%)\n",
      "\n",
      "Training - Epoch: 214 loss: 2.301251757558187 accuracy: 11.236666666666666\n",
      "\n",
      "Test set: Average loss: 2.3012, Accuracy: 1135/10000 (11%)\n",
      "\n",
      "Training - Epoch: 215 loss: 2.3012517539978026 accuracy: 11.236666666666666\n",
      "\n",
      "Test set: Average loss: 2.3012, Accuracy: 1135/10000 (11%)\n",
      "\n",
      "Training - Epoch: 216 loss: 2.301251763153076 accuracy: 11.236666666666666\n",
      "\n",
      "Test set: Average loss: 2.3012, Accuracy: 1135/10000 (11%)\n",
      "\n",
      "Training - Epoch: 217 loss: 2.301251766204834 accuracy: 11.236666666666666\n",
      "\n",
      "Test set: Average loss: 2.3012, Accuracy: 1135/10000 (11%)\n",
      "\n",
      "Training - Epoch: 218 loss: 2.301251763153076 accuracy: 11.236666666666666\n",
      "\n",
      "Test set: Average loss: 2.3012, Accuracy: 1135/10000 (11%)\n",
      "\n",
      "Training - Epoch: 219 loss: 2.301251769256592 accuracy: 11.236666666666666\n",
      "\n",
      "Test set: Average loss: 2.3012, Accuracy: 1135/10000 (11%)\n",
      "\n",
      "Training - Epoch: 220 loss: 2.301251764424642 accuracy: 11.236666666666666\n",
      "\n",
      "Test set: Average loss: 2.3012, Accuracy: 1135/10000 (11%)\n",
      "\n",
      "Training - Epoch: 221 loss: 2.301251810201009 accuracy: 11.236666666666666\n",
      "\n",
      "Test set: Average loss: 2.3012, Accuracy: 1135/10000 (11%)\n",
      "\n",
      "Training - Epoch: 222 loss: 2.3012517786661784 accuracy: 11.236666666666666\n",
      "\n",
      "Test set: Average loss: 2.3012, Accuracy: 1135/10000 (11%)\n",
      "\n",
      "Training - Epoch: 223 loss: 2.301251751200358 accuracy: 11.236666666666666\n",
      "\n",
      "Test set: Average loss: 2.3012, Accuracy: 1135/10000 (11%)\n",
      "\n",
      "Training - Epoch: 224 loss: 2.3012517634073895 accuracy: 11.236666666666666\n",
      "\n",
      "Test set: Average loss: 2.3012, Accuracy: 1135/10000 (11%)\n",
      "\n",
      "Training - Epoch: 225 loss: 2.301251754252116 accuracy: 11.236666666666666\n",
      "\n",
      "Test set: Average loss: 2.3012, Accuracy: 1135/10000 (11%)\n",
      "\n",
      "Training - Epoch: 226 loss: 2.3012517484029136 accuracy: 11.236666666666666\n",
      "\n",
      "Test set: Average loss: 2.3012, Accuracy: 1135/10000 (11%)\n",
      "\n",
      "Training - Epoch: 227 loss: 2.3012517667134604 accuracy: 11.236666666666666\n",
      "\n",
      "Test set: Average loss: 2.3012, Accuracy: 1135/10000 (11%)\n",
      "\n",
      "Training - Epoch: 228 loss: 2.3012517435709636 accuracy: 11.236666666666666\n",
      "\n",
      "Test set: Average loss: 2.3012, Accuracy: 1135/10000 (11%)\n",
      "\n",
      "Training - Epoch: 229 loss: 2.3012517621358235 accuracy: 11.236666666666666\n",
      "\n",
      "Test set: Average loss: 2.3012, Accuracy: 1135/10000 (11%)\n",
      "\n",
      "Training - Epoch: 230 loss: 2.3012517517089845 accuracy: 11.236666666666666\n",
      "\n",
      "Test set: Average loss: 2.3012, Accuracy: 1135/10000 (11%)\n",
      "\n",
      "Training - Epoch: 231 loss: 2.301251752471924 accuracy: 11.236666666666666\n",
      "\n",
      "Test set: Average loss: 2.3012, Accuracy: 1135/10000 (11%)\n",
      "\n",
      "Training - Epoch: 232 loss: 2.3012517918904623 accuracy: 11.236666666666666\n",
      "\n",
      "Test set: Average loss: 2.3012, Accuracy: 1135/10000 (11%)\n",
      "\n",
      "Training - Epoch: 233 loss: 2.3012517522176106 accuracy: 11.236666666666666\n",
      "\n",
      "Test set: Average loss: 2.3012, Accuracy: 1135/10000 (11%)\n",
      "\n",
      "Training - Epoch: 234 loss: 2.3012517723083494 accuracy: 11.236666666666666\n",
      "\n",
      "Test set: Average loss: 2.3012, Accuracy: 1135/10000 (11%)\n",
      "\n",
      "Training - Epoch: 235 loss: 2.3012517400105796 accuracy: 11.236666666666666\n",
      "\n",
      "Test set: Average loss: 2.3012, Accuracy: 1135/10000 (11%)\n",
      "\n",
      "Training - Epoch: 236 loss: 2.3012517801920573 accuracy: 11.236666666666666\n",
      "\n",
      "Test set: Average loss: 2.3012, Accuracy: 1135/10000 (11%)\n",
      "\n",
      "Training - Epoch: 237 loss: 2.301251736195882 accuracy: 11.236666666666666\n",
      "\n",
      "Test set: Average loss: 2.3012, Accuracy: 1135/10000 (11%)\n",
      "\n",
      "Training - Epoch: 238 loss: 2.30125173441569 accuracy: 11.236666666666666\n",
      "\n",
      "Test set: Average loss: 2.3012, Accuracy: 1135/10000 (11%)\n",
      "\n",
      "Training - Epoch: 239 loss: 2.301251753234863 accuracy: 11.236666666666666\n",
      "\n",
      "Test set: Average loss: 2.3012, Accuracy: 1135/10000 (11%)\n",
      "\n",
      "Training - Epoch: 240 loss: 2.301251774088542 accuracy: 11.236666666666666\n",
      "\n",
      "Test set: Average loss: 2.3012, Accuracy: 1135/10000 (11%)\n",
      "\n",
      "Training - Epoch: 241 loss: 2.301251779683431 accuracy: 11.236666666666666\n",
      "\n",
      "Test set: Average loss: 2.3012, Accuracy: 1135/10000 (11%)\n",
      "\n",
      "Training - Epoch: 242 loss: 2.30125178120931 accuracy: 11.236666666666666\n",
      "\n",
      "Test set: Average loss: 2.3012, Accuracy: 1135/10000 (11%)\n",
      "\n",
      "Training - Epoch: 243 loss: 2.3012517603556315 accuracy: 11.236666666666666\n",
      "\n",
      "Test set: Average loss: 2.3012, Accuracy: 1135/10000 (11%)\n",
      "\n",
      "Training - Epoch: 244 loss: 2.301251767730713 accuracy: 11.236666666666666\n",
      "\n",
      "Test set: Average loss: 2.3012, Accuracy: 1135/10000 (11%)\n",
      "\n",
      "Training - Epoch: 245 loss: 2.301251826731364 accuracy: 11.236666666666666\n",
      "\n",
      "Test set: Average loss: 2.3012, Accuracy: 1135/10000 (11%)\n",
      "\n",
      "Training - Epoch: 246 loss: 2.301251778157552 accuracy: 11.236666666666666\n",
      "\n",
      "Test set: Average loss: 2.3012, Accuracy: 1135/10000 (11%)\n",
      "\n",
      "Training - Epoch: 247 loss: 2.3012517682393394 accuracy: 11.236666666666666\n",
      "\n",
      "Test set: Average loss: 2.3012, Accuracy: 1135/10000 (11%)\n",
      "\n",
      "Training - Epoch: 248 loss: 2.301251747894287 accuracy: 11.236666666666666\n",
      "\n",
      "Test set: Average loss: 2.3012, Accuracy: 1135/10000 (11%)\n",
      "\n",
      "Training - Epoch: 249 loss: 2.3012517651875815 accuracy: 11.236666666666666\n",
      "\n",
      "Test set: Average loss: 2.3012, Accuracy: 1135/10000 (11%)\n",
      "\n",
      "Training - Epoch: 250 loss: 2.3012517850240073 accuracy: 11.236666666666666\n",
      "\n",
      "Test set: Average loss: 2.3012, Accuracy: 1135/10000 (11%)\n",
      "\n",
      "Training - Epoch: 251 loss: 2.3012517923990887 accuracy: 11.236666666666666\n",
      "\n",
      "Test set: Average loss: 2.3012, Accuracy: 1135/10000 (11%)\n",
      "\n",
      "Training - Epoch: 252 loss: 2.30125172068278 accuracy: 11.236666666666666\n",
      "\n",
      "Test set: Average loss: 2.3012, Accuracy: 1135/10000 (11%)\n",
      "\n",
      "Training - Epoch: 253 loss: 2.3012517456054686 accuracy: 11.236666666666666\n",
      "\n",
      "Test set: Average loss: 2.3012, Accuracy: 1135/10000 (11%)\n",
      "\n",
      "Training - Epoch: 254 loss: 2.3012517669677734 accuracy: 11.236666666666666\n",
      "\n",
      "Test set: Average loss: 2.3012, Accuracy: 1135/10000 (11%)\n",
      "\n",
      "Training - Epoch: 255 loss: 2.301251774851481 accuracy: 11.236666666666666\n",
      "\n",
      "Test set: Average loss: 2.3012, Accuracy: 1135/10000 (11%)\n",
      "\n",
      "Training - Epoch: 256 loss: 2.301251754506429 accuracy: 11.236666666666666\n",
      "\n",
      "Test set: Average loss: 2.3012, Accuracy: 1135/10000 (11%)\n",
      "\n",
      "Training - Epoch: 257 loss: 2.3012517501831056 accuracy: 11.236666666666666\n",
      "\n",
      "Test set: Average loss: 2.3012, Accuracy: 1135/10000 (11%)\n",
      "\n",
      "Training - Epoch: 258 loss: 2.3012517606099445 accuracy: 11.236666666666666\n",
      "\n",
      "Test set: Average loss: 2.3012, Accuracy: 1135/10000 (11%)\n",
      "\n",
      "Training - Epoch: 259 loss: 2.301251779429118 accuracy: 11.236666666666666\n",
      "\n",
      "Test set: Average loss: 2.3012, Accuracy: 1135/10000 (11%)\n",
      "\n",
      "Training - Epoch: 260 loss: 2.3012517481486 accuracy: 11.236666666666666\n",
      "\n",
      "Test set: Average loss: 2.3012, Accuracy: 1135/10000 (11%)\n",
      "\n",
      "Training - Epoch: 261 loss: 2.3012517801920573 accuracy: 11.236666666666666\n",
      "\n",
      "Test set: Average loss: 2.3012, Accuracy: 1135/10000 (11%)\n",
      "\n",
      "Training - Epoch: 262 loss: 2.3012517623901365 accuracy: 11.236666666666666\n",
      "\n",
      "Test set: Average loss: 2.3012, Accuracy: 1135/10000 (11%)\n",
      "\n",
      "Training - Epoch: 263 loss: 2.3012517801920573 accuracy: 11.236666666666666\n",
      "\n",
      "Test set: Average loss: 2.3012, Accuracy: 1135/10000 (11%)\n",
      "\n",
      "Training - Epoch: 264 loss: 2.301251773071289 accuracy: 11.236666666666666\n",
      "\n",
      "Test set: Average loss: 2.3012, Accuracy: 1135/10000 (11%)\n",
      "\n",
      "Training - Epoch: 265 loss: 2.3012517890930178 accuracy: 11.236666666666666\n",
      "\n",
      "Test set: Average loss: 2.3012, Accuracy: 1135/10000 (11%)\n",
      "\n",
      "Training - Epoch: 266 loss: 2.3012517621358235 accuracy: 11.236666666666666\n",
      "\n",
      "Test set: Average loss: 2.3012, Accuracy: 1135/10000 (11%)\n",
      "\n",
      "Training - Epoch: 267 loss: 2.301251769256592 accuracy: 11.236666666666666\n",
      "\n",
      "Test set: Average loss: 2.3012, Accuracy: 1135/10000 (11%)\n",
      "\n",
      "Training - Epoch: 268 loss: 2.301251759592692 accuracy: 11.236666666666666\n",
      "\n",
      "Test set: Average loss: 2.3012, Accuracy: 1135/10000 (11%)\n",
      "\n",
      "Training - Epoch: 269 loss: 2.301251775868734 accuracy: 11.236666666666666\n",
      "\n",
      "Test set: Average loss: 2.3012, Accuracy: 1135/10000 (11%)\n",
      "\n",
      "Training - Epoch: 270 loss: 2.301251765950521 accuracy: 11.236666666666666\n",
      "\n",
      "Test set: Average loss: 2.3012, Accuracy: 1135/10000 (11%)\n",
      "\n",
      "Training - Epoch: 271 loss: 2.3012517389933267 accuracy: 11.236666666666666\n",
      "\n",
      "Test set: Average loss: 2.3012, Accuracy: 1135/10000 (11%)\n",
      "\n",
      "Training - Epoch: 272 loss: 2.3012517440795897 accuracy: 11.236666666666666\n",
      "\n",
      "Test set: Average loss: 2.3012, Accuracy: 1135/10000 (11%)\n",
      "\n",
      "Training - Epoch: 273 loss: 2.3012517517089845 accuracy: 11.236666666666666\n",
      "\n",
      "Test set: Average loss: 2.3012, Accuracy: 1135/10000 (11%)\n",
      "\n",
      "Training - Epoch: 274 loss: 2.3012517733256024 accuracy: 11.236666666666666\n",
      "\n",
      "Test set: Average loss: 2.3012, Accuracy: 1135/10000 (11%)\n",
      "\n",
      "Training - Epoch: 275 loss: 2.3012517585754395 accuracy: 11.236666666666666\n",
      "\n",
      "Test set: Average loss: 2.3012, Accuracy: 1135/10000 (11%)\n",
      "\n",
      "Training - Epoch: 276 loss: 2.301251756540934 accuracy: 11.236666666666666\n",
      "\n",
      "Test set: Average loss: 2.3012, Accuracy: 1135/10000 (11%)\n",
      "\n",
      "Training - Epoch: 277 loss: 2.3012517684936524 accuracy: 11.236666666666666\n",
      "\n",
      "Test set: Average loss: 2.3012, Accuracy: 1135/10000 (11%)\n",
      "\n",
      "Training - Epoch: 278 loss: 2.3012517567952475 accuracy: 11.236666666666666\n",
      "\n",
      "Test set: Average loss: 2.3012, Accuracy: 1135/10000 (11%)\n",
      "\n",
      "Training - Epoch: 279 loss: 2.3012517903645833 accuracy: 11.236666666666666\n",
      "\n",
      "Test set: Average loss: 2.3012, Accuracy: 1135/10000 (11%)\n",
      "\n",
      "Training - Epoch: 280 loss: 2.3012517601013185 accuracy: 11.236666666666666\n",
      "\n",
      "Test set: Average loss: 2.3012, Accuracy: 1135/10000 (11%)\n",
      "\n",
      "Training - Epoch: 281 loss: 2.301251774342855 accuracy: 11.236666666666666\n",
      "\n",
      "Test set: Average loss: 2.3012, Accuracy: 1135/10000 (11%)\n",
      "\n",
      "Training - Epoch: 282 loss: 2.3012517616271975 accuracy: 11.236666666666666\n",
      "\n",
      "Test set: Average loss: 2.3012, Accuracy: 1135/10000 (11%)\n",
      "\n",
      "Training - Epoch: 283 loss: 2.301251774342855 accuracy: 11.236666666666666\n",
      "\n",
      "Test set: Average loss: 2.3012, Accuracy: 1135/10000 (11%)\n",
      "\n",
      "Training - Epoch: 284 loss: 2.301251777394613 accuracy: 11.236666666666666\n",
      "\n",
      "Test set: Average loss: 2.3012, Accuracy: 1135/10000 (11%)\n",
      "\n",
      "Training - Epoch: 285 loss: 2.301251766459147 accuracy: 11.236666666666666\n",
      "\n",
      "Test set: Average loss: 2.3012, Accuracy: 1135/10000 (11%)\n",
      "\n",
      "Training - Epoch: 286 loss: 2.301251774851481 accuracy: 11.236666666666666\n",
      "\n",
      "Test set: Average loss: 2.3012, Accuracy: 1135/10000 (11%)\n",
      "\n",
      "Training - Epoch: 287 loss: 2.3012517654418945 accuracy: 11.236666666666666\n",
      "\n",
      "Test set: Average loss: 2.3012, Accuracy: 1135/10000 (11%)\n",
      "\n",
      "Training - Epoch: 288 loss: 2.301251741027832 accuracy: 11.236666666666666\n",
      "\n",
      "Test set: Average loss: 2.3012, Accuracy: 1135/10000 (11%)\n",
      "\n",
      "Training - Epoch: 289 loss: 2.301251761118571 accuracy: 11.236666666666666\n",
      "\n",
      "Test set: Average loss: 2.3012, Accuracy: 1135/10000 (11%)\n",
      "\n",
      "Training - Epoch: 290 loss: 2.3012517636617025 accuracy: 11.236666666666666\n",
      "\n",
      "Test set: Average loss: 2.3012, Accuracy: 1135/10000 (11%)\n",
      "\n",
      "Training - Epoch: 291 loss: 2.301251749165853 accuracy: 11.236666666666666\n",
      "\n",
      "Test set: Average loss: 2.3012, Accuracy: 1135/10000 (11%)\n",
      "\n",
      "Training - Epoch: 292 loss: 2.3012517672220865 accuracy: 11.236666666666666\n",
      "\n",
      "Test set: Average loss: 2.3012, Accuracy: 1135/10000 (11%)\n",
      "\n",
      "Training - Epoch: 293 loss: 2.301251780700684 accuracy: 11.236666666666666\n",
      "\n",
      "Test set: Average loss: 2.3012, Accuracy: 1135/10000 (11%)\n",
      "\n",
      "Training - Epoch: 294 loss: 2.3012517311096192 accuracy: 11.236666666666666\n",
      "\n",
      "Test set: Average loss: 2.3012, Accuracy: 1135/10000 (11%)\n",
      "\n",
      "Training - Epoch: 295 loss: 2.3012517573038735 accuracy: 11.236666666666666\n",
      "\n",
      "Test set: Average loss: 2.3012, Accuracy: 1135/10000 (11%)\n",
      "\n",
      "Training - Epoch: 296 loss: 2.3012517979939777 accuracy: 11.236666666666666\n",
      "\n",
      "Test set: Average loss: 2.3012, Accuracy: 1135/10000 (11%)\n",
      "\n",
      "Training - Epoch: 297 loss: 2.3012517649332684 accuracy: 11.236666666666666\n",
      "\n",
      "Test set: Average loss: 2.3012, Accuracy: 1135/10000 (11%)\n",
      "\n",
      "Training - Epoch: 298 loss: 2.3012517588297525 accuracy: 11.236666666666666\n",
      "\n",
      "Test set: Average loss: 2.3012, Accuracy: 1135/10000 (11%)\n",
      "\n",
      "Training - Epoch: 299 loss: 2.30125173441569 accuracy: 11.236666666666666\n",
      "\n",
      "Test set: Average loss: 2.3012, Accuracy: 1135/10000 (11%)\n",
      "\n",
      "Training - Epoch: 300 loss: 2.3012517682393394 accuracy: 11.236666666666666\n",
      "\n",
      "Test set: Average loss: 2.3012, Accuracy: 1135/10000 (11%)\n",
      "\n",
      "Training - Epoch: 301 loss: 2.3012517903645833 accuracy: 11.236666666666666\n",
      "\n",
      "Test set: Average loss: 2.3012, Accuracy: 1135/10000 (11%)\n",
      "\n",
      "Training - Epoch: 302 loss: 2.3012517588297525 accuracy: 11.236666666666666\n",
      "\n",
      "Test set: Average loss: 2.3012, Accuracy: 1135/10000 (11%)\n",
      "\n",
      "Training - Epoch: 303 loss: 2.3012518010457357 accuracy: 11.236666666666666\n",
      "\n",
      "Test set: Average loss: 2.3012, Accuracy: 1135/10000 (11%)\n",
      "\n",
      "Training - Epoch: 304 loss: 2.3012517372131347 accuracy: 11.236666666666666\n",
      "\n",
      "Test set: Average loss: 2.3012, Accuracy: 1135/10000 (11%)\n",
      "\n",
      "Training - Epoch: 305 loss: 2.3012517573038735 accuracy: 11.236666666666666\n",
      "\n",
      "Test set: Average loss: 2.3012, Accuracy: 1135/10000 (11%)\n",
      "\n",
      "Training - Epoch: 306 loss: 2.3012517405192057 accuracy: 11.236666666666666\n",
      "\n",
      "Test set: Average loss: 2.3012, Accuracy: 1135/10000 (11%)\n",
      "\n",
      "Training - Epoch: 307 loss: 2.3012517669677734 accuracy: 11.236666666666666\n",
      "\n",
      "Test set: Average loss: 2.3012, Accuracy: 1135/10000 (11%)\n",
      "\n",
      "Training - Epoch: 308 loss: 2.301251767730713 accuracy: 11.236666666666666\n",
      "\n",
      "Test set: Average loss: 2.3012, Accuracy: 1135/10000 (11%)\n",
      "\n",
      "Training - Epoch: 309 loss: 2.3012517893473308 accuracy: 11.236666666666666\n",
      "\n",
      "Test set: Average loss: 2.3012, Accuracy: 1135/10000 (11%)\n",
      "\n",
      "Training - Epoch: 310 loss: 2.3012517634073895 accuracy: 11.236666666666666\n",
      "\n",
      "Test set: Average loss: 2.3012, Accuracy: 1135/10000 (11%)\n",
      "\n",
      "Training - Epoch: 311 loss: 2.3012517588297525 accuracy: 11.236666666666666\n",
      "\n",
      "Test set: Average loss: 2.3012, Accuracy: 1135/10000 (11%)\n",
      "\n",
      "Training - Epoch: 312 loss: 2.301251777394613 accuracy: 11.236666666666666\n",
      "\n",
      "Test set: Average loss: 2.3012, Accuracy: 1135/10000 (11%)\n",
      "\n",
      "Training - Epoch: 313 loss: 2.301251758066813 accuracy: 11.236666666666666\n",
      "\n",
      "Test set: Average loss: 2.3012, Accuracy: 1135/10000 (11%)\n",
      "\n",
      "Training - Epoch: 314 loss: 2.3012517684936524 accuracy: 11.236666666666666\n",
      "\n",
      "Test set: Average loss: 2.3012, Accuracy: 1135/10000 (11%)\n",
      "\n",
      "Training - Epoch: 315 loss: 2.3012517636617025 accuracy: 11.236666666666666\n",
      "\n",
      "Test set: Average loss: 2.3012, Accuracy: 1135/10000 (11%)\n",
      "\n",
      "Training - Epoch: 316 loss: 2.3012517417907716 accuracy: 11.236666666666666\n",
      "\n",
      "Test set: Average loss: 2.3012, Accuracy: 1135/10000 (11%)\n",
      "\n",
      "Training - Epoch: 317 loss: 2.301251752726237 accuracy: 11.236666666666666\n",
      "\n",
      "Test set: Average loss: 2.3012, Accuracy: 1135/10000 (11%)\n",
      "\n",
      "Training - Epoch: 318 loss: 2.3012517687479654 accuracy: 11.236666666666666\n",
      "\n",
      "Test set: Average loss: 2.3012, Accuracy: 1135/10000 (11%)\n",
      "\n",
      "Training - Epoch: 319 loss: 2.301251764170329 accuracy: 11.236666666666666\n",
      "\n",
      "Test set: Average loss: 2.3012, Accuracy: 1135/10000 (11%)\n",
      "\n",
      "Training - Epoch: 320 loss: 2.3012517690022785 accuracy: 11.236666666666666\n",
      "\n",
      "Test set: Average loss: 2.3012, Accuracy: 1135/10000 (11%)\n",
      "\n",
      "Training - Epoch: 321 loss: 2.301251755777995 accuracy: 11.236666666666666\n",
      "\n",
      "Test set: Average loss: 2.3012, Accuracy: 1135/10000 (11%)\n",
      "\n",
      "Training - Epoch: 322 loss: 2.3012517616271975 accuracy: 11.236666666666666\n",
      "\n",
      "Test set: Average loss: 2.3012, Accuracy: 1135/10000 (11%)\n",
      "\n",
      "Training - Epoch: 323 loss: 2.3012517682393394 accuracy: 11.236666666666666\n",
      "\n",
      "Test set: Average loss: 2.3012, Accuracy: 1135/10000 (11%)\n",
      "\n",
      "Training - Epoch: 324 loss: 2.3012517672220865 accuracy: 11.236666666666666\n",
      "\n",
      "Test set: Average loss: 2.3012, Accuracy: 1135/10000 (11%)\n",
      "\n",
      "Training - Epoch: 325 loss: 2.30125173441569 accuracy: 11.236666666666666\n",
      "\n",
      "Test set: Average loss: 2.3012, Accuracy: 1135/10000 (11%)\n",
      "\n",
      "Training - Epoch: 326 loss: 2.3012517555236816 accuracy: 11.236666666666666\n",
      "\n",
      "Test set: Average loss: 2.3012, Accuracy: 1135/10000 (11%)\n",
      "\n",
      "Training - Epoch: 327 loss: 2.3012518094380696 accuracy: 11.236666666666666\n",
      "\n",
      "Test set: Average loss: 2.3012, Accuracy: 1135/10000 (11%)\n",
      "\n",
      "Training - Epoch: 328 loss: 2.3012517333984377 accuracy: 11.236666666666666\n",
      "\n",
      "Test set: Average loss: 2.3012, Accuracy: 1135/10000 (11%)\n",
      "\n",
      "Training - Epoch: 329 loss: 2.3012517349243162 accuracy: 11.236666666666666\n",
      "\n",
      "Test set: Average loss: 2.3012, Accuracy: 1135/10000 (11%)\n",
      "\n",
      "Training - Epoch: 330 loss: 2.3012517700195314 accuracy: 11.236666666666666\n",
      "\n",
      "Test set: Average loss: 2.3012, Accuracy: 1135/10000 (11%)\n",
      "\n",
      "Training - Epoch: 331 loss: 2.3012517486572266 accuracy: 11.236666666666666\n",
      "\n",
      "Test set: Average loss: 2.3012, Accuracy: 1135/10000 (11%)\n",
      "\n",
      "Training - Epoch: 332 loss: 2.301251774851481 accuracy: 11.236666666666666\n",
      "\n",
      "Test set: Average loss: 2.3012, Accuracy: 1135/10000 (11%)\n",
      "\n",
      "Training - Epoch: 333 loss: 2.3012517077128094 accuracy: 11.236666666666666\n",
      "\n",
      "Test set: Average loss: 2.3012, Accuracy: 1135/10000 (11%)\n",
      "\n",
      "Training - Epoch: 334 loss: 2.3012517349243162 accuracy: 11.236666666666666\n",
      "\n",
      "Test set: Average loss: 2.3012, Accuracy: 1135/10000 (11%)\n",
      "\n",
      "Training - Epoch: 335 loss: 2.301251723734538 accuracy: 11.236666666666666\n",
      "\n",
      "Test set: Average loss: 2.3012, Accuracy: 1135/10000 (11%)\n",
      "\n",
      "Training - Epoch: 336 loss: 2.301251764170329 accuracy: 11.236666666666666\n",
      "\n",
      "Test set: Average loss: 2.3012, Accuracy: 1135/10000 (11%)\n",
      "\n",
      "Training - Epoch: 337 loss: 2.30125177154541 accuracy: 11.236666666666666\n",
      "\n",
      "Test set: Average loss: 2.3012, Accuracy: 1135/10000 (11%)\n",
      "\n",
      "Training - Epoch: 338 loss: 2.30125177154541 accuracy: 11.236666666666666\n",
      "\n",
      "Test set: Average loss: 2.3012, Accuracy: 1135/10000 (11%)\n",
      "\n",
      "Training - Epoch: 339 loss: 2.301251766459147 accuracy: 11.236666666666666\n",
      "\n",
      "Test set: Average loss: 2.3012, Accuracy: 1135/10000 (11%)\n",
      "\n",
      "Training - Epoch: 340 loss: 2.3012517918904623 accuracy: 11.236666666666666\n",
      "\n",
      "Test set: Average loss: 2.3012, Accuracy: 1135/10000 (11%)\n",
      "\n",
      "Training - Epoch: 341 loss: 2.3012517356872557 accuracy: 11.236666666666666\n",
      "\n",
      "Test set: Average loss: 2.3012, Accuracy: 1135/10000 (11%)\n",
      "\n",
      "Training - Epoch: 342 loss: 2.3012517433166506 accuracy: 11.236666666666666\n",
      "\n",
      "Test set: Average loss: 2.3012, Accuracy: 1135/10000 (11%)\n",
      "\n",
      "Training - Epoch: 343 loss: 2.301251758066813 accuracy: 11.236666666666666\n",
      "\n",
      "Test set: Average loss: 2.3012, Accuracy: 1135/10000 (11%)\n",
      "\n",
      "Training - Epoch: 344 loss: 2.3012517369588217 accuracy: 11.236666666666666\n",
      "\n",
      "Test set: Average loss: 2.3012, Accuracy: 1135/10000 (11%)\n",
      "\n",
      "Training - Epoch: 345 loss: 2.301251764678955 accuracy: 11.236666666666666\n",
      "\n",
      "Test set: Average loss: 2.3012, Accuracy: 1135/10000 (11%)\n",
      "\n",
      "Training - Epoch: 346 loss: 2.301251777648926 accuracy: 11.236666666666666\n",
      "\n",
      "Test set: Average loss: 2.3012, Accuracy: 1135/10000 (11%)\n",
      "\n",
      "Training - Epoch: 347 loss: 2.3012517723083494 accuracy: 11.236666666666666\n",
      "\n",
      "Test set: Average loss: 2.3012, Accuracy: 1135/10000 (11%)\n",
      "\n",
      "Training - Epoch: 348 loss: 2.3012517539978026 accuracy: 11.236666666666666\n",
      "\n",
      "Test set: Average loss: 2.3012, Accuracy: 1135/10000 (11%)\n",
      "\n",
      "Training - Epoch: 349 loss: 2.301251784769694 accuracy: 11.236666666666666\n",
      "\n",
      "Test set: Average loss: 2.3012, Accuracy: 1135/10000 (11%)\n",
      "\n",
      "Training - Epoch: 350 loss: 2.301251757558187 accuracy: 11.236666666666666\n",
      "\n",
      "Test set: Average loss: 2.3012, Accuracy: 1135/10000 (11%)\n",
      "\n",
      "Training - Epoch: 351 loss: 2.3012517438252766 accuracy: 11.236666666666666\n",
      "\n",
      "Test set: Average loss: 2.3012, Accuracy: 1135/10000 (11%)\n",
      "\n",
      "Training - Epoch: 352 loss: 2.301251760864258 accuracy: 11.236666666666666\n",
      "\n",
      "Test set: Average loss: 2.3012, Accuracy: 1135/10000 (11%)\n",
      "\n",
      "Training - Epoch: 353 loss: 2.301251756540934 accuracy: 11.236666666666666\n",
      "\n",
      "Test set: Average loss: 2.3012, Accuracy: 1135/10000 (11%)\n",
      "\n",
      "Training - Epoch: 354 loss: 2.30125181350708 accuracy: 11.236666666666666\n",
      "\n",
      "Test set: Average loss: 2.3012, Accuracy: 1135/10000 (11%)\n",
      "\n",
      "Training - Epoch: 355 loss: 2.3012517682393394 accuracy: 11.236666666666666\n",
      "\n",
      "Test set: Average loss: 2.3012, Accuracy: 1135/10000 (11%)\n",
      "\n",
      "Training - Epoch: 356 loss: 2.301251759592692 accuracy: 11.236666666666666\n",
      "\n",
      "Test set: Average loss: 2.3012, Accuracy: 1135/10000 (11%)\n",
      "\n",
      "Training - Epoch: 357 loss: 2.3012517898559572 accuracy: 11.236666666666666\n",
      "\n",
      "Test set: Average loss: 2.3012, Accuracy: 1135/10000 (11%)\n",
      "\n",
      "Training - Epoch: 358 loss: 2.3012517717997234 accuracy: 11.236666666666666\n",
      "\n",
      "Test set: Average loss: 2.3012, Accuracy: 1135/10000 (11%)\n",
      "\n",
      "Training - Epoch: 359 loss: 2.301251749674479 accuracy: 11.236666666666666\n",
      "\n",
      "Test set: Average loss: 2.3012, Accuracy: 1135/10000 (11%)\n",
      "\n",
      "Training - Epoch: 360 loss: 2.301251769256592 accuracy: 11.236666666666666\n",
      "\n",
      "Test set: Average loss: 2.3012, Accuracy: 1135/10000 (11%)\n",
      "\n",
      "Training - Epoch: 361 loss: 2.301251754252116 accuracy: 11.236666666666666\n",
      "\n",
      "Test set: Average loss: 2.3012, Accuracy: 1135/10000 (11%)\n",
      "\n",
      "Training - Epoch: 362 loss: 2.3012517720540364 accuracy: 11.236666666666666\n",
      "\n",
      "Test set: Average loss: 2.3012, Accuracy: 1135/10000 (11%)\n",
      "\n",
      "Training - Epoch: 363 loss: 2.3012517484029136 accuracy: 11.236666666666666\n",
      "\n",
      "Test set: Average loss: 2.3012, Accuracy: 1135/10000 (11%)\n",
      "\n",
      "Training - Epoch: 364 loss: 2.30125173441569 accuracy: 11.236666666666666\n",
      "\n",
      "Test set: Average loss: 2.3012, Accuracy: 1135/10000 (11%)\n",
      "\n",
      "Training - Epoch: 365 loss: 2.301251772816976 accuracy: 11.236666666666666\n",
      "\n",
      "Test set: Average loss: 2.3012, Accuracy: 1135/10000 (11%)\n",
      "\n",
      "Training - Epoch: 366 loss: 2.3012517489115396 accuracy: 11.236666666666666\n",
      "\n",
      "Test set: Average loss: 2.3012, Accuracy: 1135/10000 (11%)\n",
      "\n",
      "Training - Epoch: 367 loss: 2.3012517768859864 accuracy: 11.236666666666666\n",
      "\n",
      "Test set: Average loss: 2.3012, Accuracy: 1135/10000 (11%)\n",
      "\n",
      "Training - Epoch: 368 loss: 2.3012517306009928 accuracy: 11.236666666666666\n",
      "\n",
      "Test set: Average loss: 2.3012, Accuracy: 1135/10000 (11%)\n",
      "\n",
      "Training - Epoch: 369 loss: 2.301251753234863 accuracy: 11.236666666666666\n",
      "\n",
      "Test set: Average loss: 2.3012, Accuracy: 1135/10000 (11%)\n",
      "\n",
      "Training - Epoch: 370 loss: 2.301251779429118 accuracy: 11.236666666666666\n",
      "\n",
      "Test set: Average loss: 2.3012, Accuracy: 1135/10000 (11%)\n",
      "\n",
      "Training - Epoch: 371 loss: 2.30125179494222 accuracy: 11.236666666666666\n",
      "\n",
      "Test set: Average loss: 2.3012, Accuracy: 1135/10000 (11%)\n",
      "\n",
      "Training - Epoch: 372 loss: 2.301251767730713 accuracy: 11.236666666666666\n",
      "\n",
      "Test set: Average loss: 2.3012, Accuracy: 1135/10000 (11%)\n",
      "\n",
      "Training - Epoch: 373 loss: 2.3012517456054686 accuracy: 11.236666666666666\n",
      "\n",
      "Test set: Average loss: 2.3012, Accuracy: 1135/10000 (11%)\n",
      "\n",
      "Training - Epoch: 374 loss: 2.301251767730713 accuracy: 11.236666666666666\n",
      "\n",
      "Test set: Average loss: 2.3012, Accuracy: 1135/10000 (11%)\n",
      "\n",
      "Training - Epoch: 375 loss: 2.3012517936706542 accuracy: 11.236666666666666\n",
      "\n",
      "Test set: Average loss: 2.3012, Accuracy: 1135/10000 (11%)\n",
      "\n",
      "Training - Epoch: 376 loss: 2.3012517506917316 accuracy: 11.236666666666666\n",
      "\n",
      "Test set: Average loss: 2.3012, Accuracy: 1135/10000 (11%)\n",
      "\n",
      "Training - Epoch: 377 loss: 2.301251751200358 accuracy: 11.236666666666666\n",
      "\n",
      "Test set: Average loss: 2.3012, Accuracy: 1135/10000 (11%)\n",
      "\n",
      "Training - Epoch: 378 loss: 2.3012517723083494 accuracy: 11.236666666666666\n",
      "\n",
      "Test set: Average loss: 2.3012, Accuracy: 1135/10000 (11%)\n",
      "\n",
      "Training - Epoch: 379 loss: 2.301251777394613 accuracy: 11.236666666666666\n",
      "\n",
      "Test set: Average loss: 2.3012, Accuracy: 1135/10000 (11%)\n",
      "\n",
      "Training - Epoch: 380 loss: 2.3012517911275228 accuracy: 11.236666666666666\n",
      "\n",
      "Test set: Average loss: 2.3012, Accuracy: 1135/10000 (11%)\n",
      "\n",
      "Training - Epoch: 381 loss: 2.3012517946879067 accuracy: 11.236666666666666\n",
      "\n",
      "Test set: Average loss: 2.3012, Accuracy: 1135/10000 (11%)\n",
      "\n",
      "Training - Epoch: 382 loss: 2.301251781463623 accuracy: 11.236666666666666\n",
      "\n",
      "Test set: Average loss: 2.3012, Accuracy: 1135/10000 (11%)\n",
      "\n",
      "Training - Epoch: 383 loss: 2.301251771291097 accuracy: 11.236666666666666\n",
      "\n",
      "Test set: Average loss: 2.3012, Accuracy: 1135/10000 (11%)\n",
      "\n",
      "Training - Epoch: 384 loss: 2.3012517473856606 accuracy: 11.236666666666666\n",
      "\n",
      "Test set: Average loss: 2.3012, Accuracy: 1135/10000 (11%)\n",
      "\n",
      "Training - Epoch: 385 loss: 2.301251764170329 accuracy: 11.236666666666666\n",
      "\n",
      "Test set: Average loss: 2.3012, Accuracy: 1135/10000 (11%)\n",
      "\n",
      "Training - Epoch: 386 loss: 2.301251774088542 accuracy: 11.236666666666666\n",
      "\n",
      "Test set: Average loss: 2.3012, Accuracy: 1135/10000 (11%)\n",
      "\n",
      "Training - Epoch: 387 loss: 2.301251770782471 accuracy: 11.236666666666666\n",
      "\n",
      "Test set: Average loss: 2.3012, Accuracy: 1135/10000 (11%)\n",
      "\n",
      "Training - Epoch: 388 loss: 2.3012517181396484 accuracy: 11.236666666666666\n",
      "\n",
      "Test set: Average loss: 2.3012, Accuracy: 1135/10000 (11%)\n",
      "\n",
      "Training - Epoch: 389 loss: 2.301251766459147 accuracy: 11.236666666666666\n",
      "\n",
      "Test set: Average loss: 2.3012, Accuracy: 1135/10000 (11%)\n",
      "\n",
      "Training - Epoch: 390 loss: 2.3012517964680987 accuracy: 11.236666666666666\n",
      "\n",
      "Test set: Average loss: 2.3012, Accuracy: 1135/10000 (11%)\n",
      "\n",
      "Training - Epoch: 391 loss: 2.301251774597168 accuracy: 11.236666666666666\n",
      "\n",
      "Test set: Average loss: 2.3012, Accuracy: 1135/10000 (11%)\n",
      "\n",
      "Training - Epoch: 392 loss: 2.301251769256592 accuracy: 11.236666666666666\n",
      "\n",
      "Test set: Average loss: 2.3012, Accuracy: 1135/10000 (11%)\n",
      "\n",
      "Training - Epoch: 393 loss: 2.3012517654418945 accuracy: 11.236666666666666\n",
      "\n",
      "Test set: Average loss: 2.3012, Accuracy: 1135/10000 (11%)\n",
      "\n",
      "Training - Epoch: 394 loss: 2.3012517702738444 accuracy: 11.236666666666666\n",
      "\n",
      "Test set: Average loss: 2.3012, Accuracy: 1135/10000 (11%)\n",
      "\n",
      "Training - Epoch: 395 loss: 2.3012517873128253 accuracy: 11.236666666666666\n",
      "\n",
      "Test set: Average loss: 2.3012, Accuracy: 1135/10000 (11%)\n",
      "\n",
      "Training - Epoch: 396 loss: 2.3012517804463704 accuracy: 11.236666666666666\n",
      "\n",
      "Test set: Average loss: 2.3012, Accuracy: 1135/10000 (11%)\n",
      "\n",
      "Training - Epoch: 397 loss: 2.301251738230387 accuracy: 11.236666666666666\n",
      "\n",
      "Test set: Average loss: 2.3012, Accuracy: 1135/10000 (11%)\n",
      "\n",
      "Training - Epoch: 398 loss: 2.3012517407735187 accuracy: 11.236666666666666\n",
      "\n",
      "Test set: Average loss: 2.3012, Accuracy: 1135/10000 (11%)\n",
      "\n",
      "Training - Epoch: 399 loss: 2.3012517636617025 accuracy: 11.236666666666666\n",
      "\n",
      "Test set: Average loss: 2.3012, Accuracy: 1135/10000 (11%)\n",
      "\n",
      "Training - Epoch: 400 loss: 2.3012517433166506 accuracy: 11.236666666666666\n",
      "\n",
      "Test set: Average loss: 2.3012, Accuracy: 1135/10000 (11%)\n",
      "\n",
      "Training - Epoch: 401 loss: 2.301251755777995 accuracy: 11.236666666666666\n",
      "\n",
      "Test set: Average loss: 2.3012, Accuracy: 1135/10000 (11%)\n",
      "\n",
      "Training - Epoch: 402 loss: 2.301251761372884 accuracy: 11.236666666666666\n",
      "\n",
      "Test set: Average loss: 2.3012, Accuracy: 1135/10000 (11%)\n",
      "\n",
      "Training - Epoch: 403 loss: 2.3012517567952475 accuracy: 11.236666666666666\n",
      "\n",
      "Test set: Average loss: 2.3012, Accuracy: 1135/10000 (11%)\n",
      "\n",
      "Training - Epoch: 404 loss: 2.3012517690022785 accuracy: 11.236666666666666\n",
      "\n",
      "Test set: Average loss: 2.3012, Accuracy: 1135/10000 (11%)\n",
      "\n",
      "Training - Epoch: 405 loss: 2.3012517537434896 accuracy: 11.236666666666666\n",
      "\n",
      "Test set: Average loss: 2.3012, Accuracy: 1135/10000 (11%)\n",
      "\n",
      "Training - Epoch: 406 loss: 2.301251762898763 accuracy: 11.236666666666666\n",
      "\n",
      "Test set: Average loss: 2.3012, Accuracy: 1135/10000 (11%)\n",
      "\n",
      "Training - Epoch: 407 loss: 2.301251779683431 accuracy: 11.236666666666666\n",
      "\n",
      "Test set: Average loss: 2.3012, Accuracy: 1135/10000 (11%)\n",
      "\n",
      "Training - Epoch: 408 loss: 2.301251720428467 accuracy: 11.236666666666666\n",
      "\n",
      "Test set: Average loss: 2.3012, Accuracy: 1135/10000 (11%)\n",
      "\n",
      "Training - Epoch: 409 loss: 2.3012517868041993 accuracy: 11.236666666666666\n",
      "\n",
      "Test set: Average loss: 2.3012, Accuracy: 1135/10000 (11%)\n",
      "\n",
      "Training - Epoch: 410 loss: 2.3012517786661784 accuracy: 11.236666666666666\n",
      "\n",
      "Test set: Average loss: 2.3012, Accuracy: 1135/10000 (11%)\n",
      "\n",
      "Training - Epoch: 411 loss: 2.3012517534891765 accuracy: 11.236666666666666\n",
      "\n",
      "Test set: Average loss: 2.3012, Accuracy: 1135/10000 (11%)\n",
      "\n",
      "Training - Epoch: 412 loss: 2.3012517682393394 accuracy: 11.236666666666666\n",
      "\n",
      "Test set: Average loss: 2.3012, Accuracy: 1135/10000 (11%)\n",
      "\n",
      "Training - Epoch: 413 loss: 2.30125175298055 accuracy: 11.236666666666666\n",
      "\n",
      "Test set: Average loss: 2.3012, Accuracy: 1135/10000 (11%)\n",
      "\n",
      "Training - Epoch: 414 loss: 2.301251785786947 accuracy: 11.236666666666666\n",
      "\n",
      "Test set: Average loss: 2.3012, Accuracy: 1135/10000 (11%)\n",
      "\n",
      "Training - Epoch: 415 loss: 2.3012517618815105 accuracy: 11.236666666666666\n",
      "\n",
      "Test set: Average loss: 2.3012, Accuracy: 1135/10000 (11%)\n",
      "\n",
      "Training - Epoch: 416 loss: 2.3012517573038735 accuracy: 11.236666666666666\n",
      "\n",
      "Test set: Average loss: 2.3012, Accuracy: 1135/10000 (11%)\n",
      "\n",
      "Training - Epoch: 417 loss: 2.301251759338379 accuracy: 11.236666666666666\n",
      "\n",
      "Test set: Average loss: 2.3012, Accuracy: 1135/10000 (11%)\n",
      "\n",
      "Training - Epoch: 418 loss: 2.3012517422993977 accuracy: 11.236666666666666\n",
      "\n",
      "Test set: Average loss: 2.3012, Accuracy: 1135/10000 (11%)\n",
      "\n",
      "Training - Epoch: 419 loss: 2.301251767730713 accuracy: 11.236666666666666\n",
      "\n",
      "Test set: Average loss: 2.3012, Accuracy: 1135/10000 (11%)\n",
      "\n",
      "Training - Epoch: 420 loss: 2.301251744588216 accuracy: 11.236666666666666\n",
      "\n",
      "Test set: Average loss: 2.3012, Accuracy: 1135/10000 (11%)\n",
      "\n",
      "Training - Epoch: 421 loss: 2.301251767730713 accuracy: 11.236666666666666\n",
      "\n",
      "Test set: Average loss: 2.3012, Accuracy: 1135/10000 (11%)\n",
      "\n",
      "Training - Epoch: 422 loss: 2.3012517621358235 accuracy: 11.236666666666666\n",
      "\n",
      "Test set: Average loss: 2.3012, Accuracy: 1135/10000 (11%)\n",
      "\n",
      "Training - Epoch: 423 loss: 2.3012517603556315 accuracy: 11.236666666666666\n",
      "\n",
      "Test set: Average loss: 2.3012, Accuracy: 1135/10000 (11%)\n",
      "\n",
      "Training - Epoch: 424 loss: 2.301251741282145 accuracy: 11.236666666666666\n",
      "\n",
      "Test set: Average loss: 2.3012, Accuracy: 1135/10000 (11%)\n",
      "\n",
      "Training - Epoch: 425 loss: 2.301251764424642 accuracy: 11.236666666666666\n",
      "\n",
      "Test set: Average loss: 2.3012, Accuracy: 1135/10000 (11%)\n",
      "\n",
      "Training - Epoch: 426 loss: 2.301251817067464 accuracy: 11.236666666666666\n",
      "\n",
      "Test set: Average loss: 2.3012, Accuracy: 1135/10000 (11%)\n",
      "\n",
      "Training - Epoch: 427 loss: 2.301251776123047 accuracy: 11.236666666666666\n",
      "\n",
      "Test set: Average loss: 2.3012, Accuracy: 1135/10000 (11%)\n",
      "\n",
      "Training - Epoch: 428 loss: 2.3012517855326333 accuracy: 11.236666666666666\n",
      "\n",
      "Test set: Average loss: 2.3012, Accuracy: 1135/10000 (11%)\n",
      "\n",
      "Training - Epoch: 429 loss: 2.301251764170329 accuracy: 11.236666666666666\n",
      "\n",
      "Test set: Average loss: 2.3012, Accuracy: 1135/10000 (11%)\n",
      "\n",
      "Training - Epoch: 430 loss: 2.3012517669677734 accuracy: 11.236666666666666\n",
      "\n",
      "Test set: Average loss: 2.3012, Accuracy: 1135/10000 (11%)\n",
      "\n",
      "Training - Epoch: 431 loss: 2.3012517700195314 accuracy: 11.236666666666666\n",
      "\n",
      "Test set: Average loss: 2.3012, Accuracy: 1135/10000 (11%)\n",
      "\n",
      "Training - Epoch: 432 loss: 2.3012517672220865 accuracy: 11.236666666666666\n",
      "\n",
      "Test set: Average loss: 2.3012, Accuracy: 1135/10000 (11%)\n",
      "\n",
      "Training - Epoch: 433 loss: 2.301251755777995 accuracy: 11.236666666666666\n",
      "\n",
      "Test set: Average loss: 2.3012, Accuracy: 1135/10000 (11%)\n",
      "\n",
      "Training - Epoch: 434 loss: 2.301251779683431 accuracy: 11.236666666666666\n",
      "\n",
      "Test set: Average loss: 2.3012, Accuracy: 1135/10000 (11%)\n",
      "\n",
      "Training - Epoch: 435 loss: 2.3012517588297525 accuracy: 11.236666666666666\n",
      "\n",
      "Test set: Average loss: 2.3012, Accuracy: 1135/10000 (11%)\n",
      "\n",
      "Training - Epoch: 436 loss: 2.301251754506429 accuracy: 11.236666666666666\n",
      "\n",
      "Test set: Average loss: 2.3012, Accuracy: 1135/10000 (11%)\n",
      "\n",
      "Training - Epoch: 437 loss: 2.3012517486572266 accuracy: 11.236666666666666\n",
      "\n",
      "Test set: Average loss: 2.3012, Accuracy: 1135/10000 (11%)\n",
      "\n",
      "Training - Epoch: 438 loss: 2.301251769510905 accuracy: 11.236666666666666\n",
      "\n",
      "Test set: Average loss: 2.3012, Accuracy: 1135/10000 (11%)\n",
      "\n",
      "Training - Epoch: 439 loss: 2.301251754760742 accuracy: 11.236666666666666\n",
      "\n",
      "Test set: Average loss: 2.3012, Accuracy: 1135/10000 (11%)\n",
      "\n",
      "Training - Epoch: 440 loss: 2.301251756286621 accuracy: 11.236666666666666\n",
      "\n",
      "Test set: Average loss: 2.3012, Accuracy: 1135/10000 (11%)\n",
      "\n",
      "Training - Epoch: 441 loss: 2.3012517654418945 accuracy: 11.236666666666666\n",
      "\n",
      "Test set: Average loss: 2.3012, Accuracy: 1135/10000 (11%)\n",
      "\n",
      "Training - Epoch: 442 loss: 2.3012517850240073 accuracy: 11.236666666666666\n",
      "\n",
      "Test set: Average loss: 2.3012, Accuracy: 1135/10000 (11%)\n",
      "\n",
      "Training - Epoch: 443 loss: 2.301251722208659 accuracy: 11.236666666666666\n",
      "\n",
      "Test set: Average loss: 2.3012, Accuracy: 1135/10000 (11%)\n",
      "\n",
      "Training - Epoch: 444 loss: 2.30125177637736 accuracy: 11.236666666666666\n",
      "\n",
      "Test set: Average loss: 2.3012, Accuracy: 1135/10000 (11%)\n",
      "\n",
      "Training - Epoch: 445 loss: 2.3012517926534017 accuracy: 11.236666666666666\n",
      "\n",
      "Test set: Average loss: 2.3012, Accuracy: 1135/10000 (11%)\n",
      "\n",
      "Training - Epoch: 446 loss: 2.3012517649332684 accuracy: 11.236666666666666\n",
      "\n",
      "Test set: Average loss: 2.3012, Accuracy: 1135/10000 (11%)\n",
      "\n",
      "Training - Epoch: 447 loss: 2.301251788075765 accuracy: 11.236666666666666\n",
      "\n",
      "Test set: Average loss: 2.3012, Accuracy: 1135/10000 (11%)\n",
      "\n",
      "Training - Epoch: 448 loss: 2.3012517550150555 accuracy: 11.236666666666666\n",
      "\n",
      "Test set: Average loss: 2.3012, Accuracy: 1135/10000 (11%)\n",
      "\n",
      "Training - Epoch: 449 loss: 2.3012517539978026 accuracy: 11.236666666666666\n",
      "\n",
      "Test set: Average loss: 2.3012, Accuracy: 1135/10000 (11%)\n",
      "\n",
      "Training - Epoch: 450 loss: 2.301251774342855 accuracy: 11.236666666666666\n",
      "\n",
      "Test set: Average loss: 2.3012, Accuracy: 1135/10000 (11%)\n",
      "\n",
      "Training - Epoch: 451 loss: 2.3012517506917316 accuracy: 11.236666666666666\n",
      "\n",
      "Test set: Average loss: 2.3012, Accuracy: 1135/10000 (11%)\n",
      "\n",
      "Training - Epoch: 452 loss: 2.301251749928792 accuracy: 11.236666666666666\n",
      "\n",
      "Test set: Average loss: 2.3012, Accuracy: 1135/10000 (11%)\n",
      "\n",
      "Training - Epoch: 453 loss: 2.3012517519632976 accuracy: 11.236666666666666\n",
      "\n",
      "Test set: Average loss: 2.3012, Accuracy: 1135/10000 (11%)\n",
      "\n",
      "Training - Epoch: 454 loss: 2.301251785786947 accuracy: 11.236666666666666\n",
      "\n",
      "Test set: Average loss: 2.3012, Accuracy: 1135/10000 (11%)\n",
      "\n",
      "Training - Epoch: 455 loss: 2.301251777648926 accuracy: 11.236666666666666\n",
      "\n",
      "Test set: Average loss: 2.3012, Accuracy: 1135/10000 (11%)\n",
      "\n",
      "Training - Epoch: 456 loss: 2.3012517583211265 accuracy: 11.236666666666666\n",
      "\n",
      "Test set: Average loss: 2.3012, Accuracy: 1135/10000 (11%)\n",
      "\n",
      "Training - Epoch: 457 loss: 2.3012517667134604 accuracy: 11.236666666666666\n",
      "\n",
      "Test set: Average loss: 2.3012, Accuracy: 1135/10000 (11%)\n",
      "\n",
      "Training - Epoch: 458 loss: 2.3012517484029136 accuracy: 11.236666666666666\n",
      "\n",
      "Test set: Average loss: 2.3012, Accuracy: 1135/10000 (11%)\n",
      "\n",
      "Training - Epoch: 459 loss: 2.301251782989502 accuracy: 11.236666666666666\n",
      "\n",
      "Test set: Average loss: 2.3012, Accuracy: 1135/10000 (11%)\n",
      "\n",
      "Training - Epoch: 460 loss: 2.301251774597168 accuracy: 11.236666666666666\n",
      "\n",
      "Test set: Average loss: 2.3012, Accuracy: 1135/10000 (11%)\n",
      "\n",
      "Training - Epoch: 461 loss: 2.3012517405192057 accuracy: 11.236666666666666\n",
      "\n",
      "Test set: Average loss: 2.3012, Accuracy: 1135/10000 (11%)\n",
      "\n",
      "Training - Epoch: 462 loss: 2.3012517651875815 accuracy: 11.236666666666666\n",
      "\n",
      "Test set: Average loss: 2.3012, Accuracy: 1135/10000 (11%)\n",
      "\n",
      "Training - Epoch: 463 loss: 2.301251770782471 accuracy: 11.236666666666666\n",
      "\n",
      "Test set: Average loss: 2.3012, Accuracy: 1135/10000 (11%)\n",
      "\n",
      "Training - Epoch: 464 loss: 2.3012517252604168 accuracy: 11.236666666666666\n",
      "\n",
      "Test set: Average loss: 2.3012, Accuracy: 1135/10000 (11%)\n",
      "\n",
      "Training - Epoch: 465 loss: 2.3012517672220865 accuracy: 11.236666666666666\n",
      "\n",
      "Test set: Average loss: 2.3012, Accuracy: 1135/10000 (11%)\n",
      "\n",
      "Training - Epoch: 466 loss: 2.301251777394613 accuracy: 11.236666666666666\n",
      "\n",
      "Test set: Average loss: 2.3012, Accuracy: 1135/10000 (11%)\n",
      "\n",
      "Training - Epoch: 467 loss: 2.3012517336527507 accuracy: 11.236666666666666\n",
      "\n",
      "Test set: Average loss: 2.3012, Accuracy: 1135/10000 (11%)\n",
      "\n",
      "Training - Epoch: 468 loss: 2.301251785786947 accuracy: 11.236666666666666\n",
      "\n",
      "Test set: Average loss: 2.3012, Accuracy: 1135/10000 (11%)\n",
      "\n",
      "Training - Epoch: 469 loss: 2.3012517669677734 accuracy: 11.236666666666666\n",
      "\n",
      "Test set: Average loss: 2.3012, Accuracy: 1135/10000 (11%)\n",
      "\n",
      "Training - Epoch: 470 loss: 2.3012517603556315 accuracy: 11.236666666666666\n",
      "\n",
      "Test set: Average loss: 2.3012, Accuracy: 1135/10000 (11%)\n",
      "\n",
      "Training - Epoch: 471 loss: 2.3012517786661784 accuracy: 11.236666666666666\n",
      "\n",
      "Test set: Average loss: 2.3012, Accuracy: 1135/10000 (11%)\n",
      "\n",
      "Training - Epoch: 472 loss: 2.3012517906188963 accuracy: 11.236666666666666\n",
      "\n",
      "Test set: Average loss: 2.3012, Accuracy: 1135/10000 (11%)\n",
      "\n",
      "Training - Epoch: 473 loss: 2.3012518107096356 accuracy: 11.236666666666666\n",
      "\n",
      "Test set: Average loss: 2.3012, Accuracy: 1135/10000 (11%)\n",
      "\n",
      "Training - Epoch: 474 loss: 2.3012517756144204 accuracy: 11.236666666666666\n",
      "\n",
      "Test set: Average loss: 2.3012, Accuracy: 1135/10000 (11%)\n",
      "\n",
      "Training - Epoch: 475 loss: 2.301251754252116 accuracy: 11.236666666666666\n",
      "\n",
      "Test set: Average loss: 2.3012, Accuracy: 1135/10000 (11%)\n",
      "\n",
      "Training - Epoch: 476 loss: 2.301251778157552 accuracy: 11.236666666666666\n",
      "\n",
      "Test set: Average loss: 2.3012, Accuracy: 1135/10000 (11%)\n",
      "\n",
      "Training - Epoch: 477 loss: 2.3012517784118653 accuracy: 11.236666666666666\n",
      "\n",
      "Test set: Average loss: 2.3012, Accuracy: 1135/10000 (11%)\n",
      "\n",
      "Training - Epoch: 478 loss: 2.301251803588867 accuracy: 11.236666666666666\n",
      "\n",
      "Test set: Average loss: 2.3012, Accuracy: 1135/10000 (11%)\n",
      "\n",
      "Training - Epoch: 479 loss: 2.301251749165853 accuracy: 11.236666666666666\n",
      "\n",
      "Test set: Average loss: 2.3012, Accuracy: 1135/10000 (11%)\n",
      "\n",
      "Training - Epoch: 480 loss: 2.3012517901102703 accuracy: 11.236666666666666\n",
      "\n",
      "Test set: Average loss: 2.3012, Accuracy: 1135/10000 (11%)\n",
      "\n",
      "Training - Epoch: 481 loss: 2.3012517911275228 accuracy: 11.236666666666666\n",
      "\n",
      "Test set: Average loss: 2.3012, Accuracy: 1135/10000 (11%)\n",
      "\n",
      "Training - Epoch: 482 loss: 2.3012517387390137 accuracy: 11.236666666666666\n",
      "\n",
      "Test set: Average loss: 2.3012, Accuracy: 1135/10000 (11%)\n",
      "\n",
      "Training - Epoch: 483 loss: 2.301251766204834 accuracy: 11.236666666666666\n",
      "\n",
      "Test set: Average loss: 2.3012, Accuracy: 1135/10000 (11%)\n",
      "\n",
      "Training - Epoch: 484 loss: 2.3012517753601074 accuracy: 11.236666666666666\n",
      "\n",
      "Test set: Average loss: 2.3012, Accuracy: 1135/10000 (11%)\n",
      "\n",
      "Training - Epoch: 485 loss: 2.3012517438252766 accuracy: 11.236666666666666\n",
      "\n",
      "Test set: Average loss: 2.3012, Accuracy: 1135/10000 (11%)\n",
      "\n",
      "Training - Epoch: 486 loss: 2.3012517354329427 accuracy: 11.236666666666666\n",
      "\n",
      "Test set: Average loss: 2.3012, Accuracy: 1135/10000 (11%)\n",
      "\n",
      "Training - Epoch: 487 loss: 2.301251771036784 accuracy: 11.236666666666666\n",
      "\n",
      "Test set: Average loss: 2.3012, Accuracy: 1135/10000 (11%)\n",
      "\n",
      "Training - Epoch: 488 loss: 2.301251773071289 accuracy: 11.236666666666666\n",
      "\n",
      "Test set: Average loss: 2.3012, Accuracy: 1135/10000 (11%)\n",
      "\n",
      "Training - Epoch: 489 loss: 2.3012517603556315 accuracy: 11.236666666666666\n",
      "\n",
      "Test set: Average loss: 2.3012, Accuracy: 1135/10000 (11%)\n",
      "\n",
      "Training - Epoch: 490 loss: 2.3012517300923667 accuracy: 11.236666666666666\n",
      "\n",
      "Test set: Average loss: 2.3012, Accuracy: 1135/10000 (11%)\n",
      "\n",
      "Training - Epoch: 491 loss: 2.3012517517089845 accuracy: 11.236666666666666\n",
      "\n",
      "Test set: Average loss: 2.3012, Accuracy: 1135/10000 (11%)\n",
      "\n",
      "Training - Epoch: 492 loss: 2.3012517717997234 accuracy: 11.236666666666666\n",
      "\n",
      "Test set: Average loss: 2.3012, Accuracy: 1135/10000 (11%)\n",
      "\n",
      "Training - Epoch: 493 loss: 2.301251757558187 accuracy: 11.236666666666666\n",
      "\n",
      "Test set: Average loss: 2.3012, Accuracy: 1135/10000 (11%)\n",
      "\n",
      "Training - Epoch: 494 loss: 2.3012517682393394 accuracy: 11.236666666666666\n",
      "\n",
      "Test set: Average loss: 2.3012, Accuracy: 1135/10000 (11%)\n",
      "\n",
      "Training - Epoch: 495 loss: 2.3012517939249673 accuracy: 11.236666666666666\n",
      "\n",
      "Test set: Average loss: 2.3012, Accuracy: 1135/10000 (11%)\n",
      "\n",
      "Training - Epoch: 496 loss: 2.3012517570495605 accuracy: 11.236666666666666\n",
      "\n",
      "Test set: Average loss: 2.3012, Accuracy: 1135/10000 (11%)\n",
      "\n",
      "Training - Epoch: 497 loss: 2.301251753234863 accuracy: 11.236666666666666\n",
      "\n",
      "Test set: Average loss: 2.3012, Accuracy: 1135/10000 (11%)\n",
      "\n",
      "Training - Epoch: 498 loss: 2.301251766204834 accuracy: 11.236666666666666\n",
      "\n",
      "Test set: Average loss: 2.3012, Accuracy: 1135/10000 (11%)\n",
      "\n",
      "Training - Epoch: 499 loss: 2.301251772816976 accuracy: 11.236666666666666\n",
      "\n",
      "Test set: Average loss: 2.3012, Accuracy: 1135/10000 (11%)\n",
      "\n",
      "Training - Epoch: 500 loss: 2.301251773071289 accuracy: 11.236666666666666\n",
      "\n",
      "Test set: Average loss: 2.3012, Accuracy: 1135/10000 (11%)\n",
      "\n",
      "Training - Epoch: 501 loss: 2.301251764170329 accuracy: 11.236666666666666\n",
      "\n",
      "Test set: Average loss: 2.3012, Accuracy: 1135/10000 (11%)\n",
      "\n",
      "Training - Epoch: 502 loss: 2.301251766204834 accuracy: 11.236666666666666\n",
      "\n",
      "Test set: Average loss: 2.3012, Accuracy: 1135/10000 (11%)\n",
      "\n",
      "Training - Epoch: 503 loss: 2.3012517717997234 accuracy: 11.236666666666666\n",
      "\n",
      "Test set: Average loss: 2.3012, Accuracy: 1135/10000 (11%)\n",
      "\n",
      "Training - Epoch: 504 loss: 2.30125178120931 accuracy: 11.236666666666666\n",
      "\n",
      "Test set: Average loss: 2.3012, Accuracy: 1135/10000 (11%)\n",
      "\n",
      "Training - Epoch: 505 loss: 2.301251703643799 accuracy: 11.236666666666666\n",
      "\n",
      "Test set: Average loss: 2.3012, Accuracy: 1135/10000 (11%)\n",
      "\n",
      "Training - Epoch: 506 loss: 2.3012517674764 accuracy: 11.236666666666666\n",
      "\n",
      "Test set: Average loss: 2.3012, Accuracy: 1135/10000 (11%)\n",
      "\n",
      "Training - Epoch: 507 loss: 2.30125176264445 accuracy: 11.236666666666666\n",
      "\n",
      "Test set: Average loss: 2.3012, Accuracy: 1135/10000 (11%)\n",
      "\n",
      "Training - Epoch: 508 loss: 2.301251737721761 accuracy: 11.236666666666666\n",
      "\n",
      "Test set: Average loss: 2.3012, Accuracy: 1135/10000 (11%)\n",
      "\n",
      "Training - Epoch: 509 loss: 2.3012517918904623 accuracy: 11.236666666666666\n",
      "\n",
      "Test set: Average loss: 2.3012, Accuracy: 1135/10000 (11%)\n",
      "\n",
      "Training - Epoch: 510 loss: 2.3012517567952475 accuracy: 11.236666666666666\n",
      "\n",
      "Test set: Average loss: 2.3012, Accuracy: 1135/10000 (11%)\n",
      "\n",
      "Training - Epoch: 511 loss: 2.3012517283121743 accuracy: 11.236666666666666\n",
      "\n",
      "Test set: Average loss: 2.3012, Accuracy: 1135/10000 (11%)\n",
      "\n",
      "Training - Epoch: 512 loss: 2.3012517690022785 accuracy: 11.236666666666666\n",
      "\n",
      "Test set: Average loss: 2.3012, Accuracy: 1135/10000 (11%)\n",
      "\n",
      "Training - Epoch: 513 loss: 2.3012517585754395 accuracy: 11.236666666666666\n",
      "\n",
      "Test set: Average loss: 2.3012, Accuracy: 1135/10000 (11%)\n",
      "\n",
      "Training - Epoch: 514 loss: 2.3012517766316734 accuracy: 11.236666666666666\n",
      "\n",
      "Test set: Average loss: 2.3012, Accuracy: 1135/10000 (11%)\n",
      "\n",
      "Training - Epoch: 515 loss: 2.3012517435709636 accuracy: 11.236666666666666\n",
      "\n",
      "Test set: Average loss: 2.3012, Accuracy: 1135/10000 (11%)\n",
      "\n",
      "Training - Epoch: 516 loss: 2.30125173441569 accuracy: 11.236666666666666\n",
      "\n",
      "Test set: Average loss: 2.3012, Accuracy: 1135/10000 (11%)\n",
      "\n",
      "Training - Epoch: 517 loss: 2.3012517435709636 accuracy: 11.236666666666666\n",
      "\n",
      "Test set: Average loss: 2.3012, Accuracy: 1135/10000 (11%)\n",
      "\n",
      "Training - Epoch: 518 loss: 2.301251746368408 accuracy: 11.236666666666666\n",
      "\n",
      "Test set: Average loss: 2.3012, Accuracy: 1135/10000 (11%)\n",
      "\n",
      "Training - Epoch: 519 loss: 2.3012517634073895 accuracy: 11.236666666666666\n",
      "\n",
      "Test set: Average loss: 2.3012, Accuracy: 1135/10000 (11%)\n",
      "\n",
      "Training - Epoch: 520 loss: 2.301251775868734 accuracy: 11.236666666666666\n",
      "\n",
      "Test set: Average loss: 2.3012, Accuracy: 1135/10000 (11%)\n",
      "\n",
      "Training - Epoch: 521 loss: 2.3012517720540364 accuracy: 11.236666666666666\n",
      "\n",
      "Test set: Average loss: 2.3012, Accuracy: 1135/10000 (11%)\n",
      "\n",
      "Training - Epoch: 522 loss: 2.3012517389933267 accuracy: 11.236666666666666\n",
      "\n",
      "Test set: Average loss: 2.3012, Accuracy: 1135/10000 (11%)\n",
      "\n",
      "Training - Epoch: 523 loss: 2.301251776123047 accuracy: 11.236666666666666\n",
      "\n",
      "Test set: Average loss: 2.3012, Accuracy: 1135/10000 (11%)\n",
      "\n",
      "Training - Epoch: 524 loss: 2.301251746114095 accuracy: 11.236666666666666\n",
      "\n",
      "Test set: Average loss: 2.3012, Accuracy: 1135/10000 (11%)\n",
      "\n",
      "Training - Epoch: 525 loss: 2.30125177154541 accuracy: 11.236666666666666\n",
      "\n",
      "Test set: Average loss: 2.3012, Accuracy: 1135/10000 (11%)\n",
      "\n",
      "Training - Epoch: 526 loss: 2.301251752726237 accuracy: 11.236666666666666\n",
      "\n",
      "Test set: Average loss: 2.3012, Accuracy: 1135/10000 (11%)\n",
      "\n",
      "Training - Epoch: 527 loss: 2.3012517267862957 accuracy: 11.236666666666666\n",
      "\n",
      "Test set: Average loss: 2.3012, Accuracy: 1135/10000 (11%)\n",
      "\n",
      "Training - Epoch: 528 loss: 2.301251777903239 accuracy: 11.236666666666666\n",
      "\n",
      "Test set: Average loss: 2.3012, Accuracy: 1135/10000 (11%)\n",
      "\n",
      "Training - Epoch: 529 loss: 2.3012517333984377 accuracy: 11.236666666666666\n",
      "\n",
      "Test set: Average loss: 2.3012, Accuracy: 1135/10000 (11%)\n",
      "\n",
      "Training - Epoch: 530 loss: 2.30125175298055 accuracy: 11.236666666666666\n",
      "\n",
      "Test set: Average loss: 2.3012, Accuracy: 1135/10000 (11%)\n",
      "\n",
      "Training - Epoch: 531 loss: 2.301251767730713 accuracy: 11.236666666666666\n",
      "\n",
      "Test set: Average loss: 2.3012, Accuracy: 1135/10000 (11%)\n",
      "\n",
      "Training - Epoch: 532 loss: 2.301251759847005 accuracy: 11.236666666666666\n",
      "\n",
      "Test set: Average loss: 2.3012, Accuracy: 1135/10000 (11%)\n",
      "\n",
      "Training - Epoch: 533 loss: 2.301251771291097 accuracy: 11.236666666666666\n",
      "\n",
      "Test set: Average loss: 2.3012, Accuracy: 1135/10000 (11%)\n",
      "\n",
      "Training - Epoch: 534 loss: 2.301251741536458 accuracy: 11.236666666666666\n",
      "\n",
      "Test set: Average loss: 2.3012, Accuracy: 1135/10000 (11%)\n",
      "\n",
      "Training - Epoch: 535 loss: 2.30125178120931 accuracy: 11.236666666666666\n",
      "\n",
      "Test set: Average loss: 2.3012, Accuracy: 1135/10000 (11%)\n",
      "\n",
      "Training - Epoch: 536 loss: 2.301251764424642 accuracy: 11.236666666666666\n",
      "\n",
      "Test set: Average loss: 2.3012, Accuracy: 1135/10000 (11%)\n",
      "\n",
      "Training - Epoch: 537 loss: 2.3012517733256024 accuracy: 11.236666666666666\n",
      "\n",
      "Test set: Average loss: 2.3012, Accuracy: 1135/10000 (11%)\n",
      "\n",
      "Training - Epoch: 538 loss: 2.3012517639160155 accuracy: 11.236666666666666\n",
      "\n",
      "Test set: Average loss: 2.3012, Accuracy: 1135/10000 (11%)\n",
      "\n",
      "Training - Epoch: 539 loss: 2.30125178120931 accuracy: 11.236666666666666\n",
      "\n",
      "Test set: Average loss: 2.3012, Accuracy: 1135/10000 (11%)\n",
      "\n",
      "Training - Epoch: 540 loss: 2.3012517300923667 accuracy: 11.236666666666666\n",
      "\n",
      "Test set: Average loss: 2.3012, Accuracy: 1135/10000 (11%)\n",
      "\n",
      "Training - Epoch: 541 loss: 2.3012517669677734 accuracy: 11.236666666666666\n",
      "\n",
      "Test set: Average loss: 2.3012, Accuracy: 1135/10000 (11%)\n",
      "\n",
      "Training - Epoch: 542 loss: 2.3012517977396647 accuracy: 11.236666666666666\n",
      "\n",
      "Test set: Average loss: 2.3012, Accuracy: 1135/10000 (11%)\n",
      "\n",
      "Training - Epoch: 543 loss: 2.301251775868734 accuracy: 11.236666666666666\n",
      "\n",
      "Test set: Average loss: 2.3012, Accuracy: 1135/10000 (11%)\n",
      "\n",
      "Training - Epoch: 544 loss: 2.301251749420166 accuracy: 11.236666666666666\n",
      "\n",
      "Test set: Average loss: 2.3012, Accuracy: 1135/10000 (11%)\n",
      "\n",
      "Training - Epoch: 545 loss: 2.301251756032308 accuracy: 11.236666666666666\n",
      "\n",
      "Test set: Average loss: 2.3012, Accuracy: 1135/10000 (11%)\n",
      "\n",
      "Training - Epoch: 546 loss: 2.3012517473856606 accuracy: 11.236666666666666\n",
      "\n",
      "Test set: Average loss: 2.3012, Accuracy: 1135/10000 (11%)\n",
      "\n",
      "Training - Epoch: 547 loss: 2.301251746368408 accuracy: 11.236666666666666\n",
      "\n",
      "Test set: Average loss: 2.3012, Accuracy: 1135/10000 (11%)\n",
      "\n",
      "Training - Epoch: 548 loss: 2.301251741536458 accuracy: 11.236666666666666\n",
      "\n",
      "Test set: Average loss: 2.3012, Accuracy: 1135/10000 (11%)\n",
      "\n",
      "Training - Epoch: 549 loss: 2.3012517489115396 accuracy: 11.236666666666666\n",
      "\n",
      "Test set: Average loss: 2.3012, Accuracy: 1135/10000 (11%)\n",
      "\n",
      "Training - Epoch: 550 loss: 2.301251771036784 accuracy: 11.236666666666666\n",
      "\n",
      "Test set: Average loss: 2.3012, Accuracy: 1135/10000 (11%)\n",
      "\n",
      "Training - Epoch: 551 loss: 2.3012518076578776 accuracy: 11.236666666666666\n",
      "\n",
      "Test set: Average loss: 2.3012, Accuracy: 1135/10000 (11%)\n",
      "\n",
      "Training - Epoch: 552 loss: 2.301251746114095 accuracy: 11.236666666666666\n",
      "\n",
      "Test set: Average loss: 2.3012, Accuracy: 1135/10000 (11%)\n",
      "\n",
      "Training - Epoch: 553 loss: 2.301251771291097 accuracy: 11.236666666666666\n",
      "\n",
      "Test set: Average loss: 2.3012, Accuracy: 1135/10000 (11%)\n",
      "\n",
      "Training - Epoch: 554 loss: 2.3012517944335937 accuracy: 11.236666666666666\n",
      "\n",
      "Test set: Average loss: 2.3012, Accuracy: 1135/10000 (11%)\n",
      "\n",
      "Training - Epoch: 555 loss: 2.3012517555236816 accuracy: 11.236666666666666\n",
      "\n",
      "Test set: Average loss: 2.3012, Accuracy: 1135/10000 (11%)\n",
      "\n",
      "Training - Epoch: 556 loss: 2.3012517534891765 accuracy: 11.236666666666666\n",
      "\n",
      "Test set: Average loss: 2.3012, Accuracy: 1135/10000 (11%)\n",
      "\n",
      "Training - Epoch: 557 loss: 2.301251756540934 accuracy: 11.236666666666666\n",
      "\n",
      "Test set: Average loss: 2.3012, Accuracy: 1135/10000 (11%)\n",
      "\n",
      "Training - Epoch: 558 loss: 2.301251753234863 accuracy: 11.236666666666666\n",
      "\n",
      "Test set: Average loss: 2.3012, Accuracy: 1135/10000 (11%)\n",
      "\n",
      "Training - Epoch: 559 loss: 2.3012517636617025 accuracy: 11.236666666666666\n",
      "\n",
      "Test set: Average loss: 2.3012, Accuracy: 1135/10000 (11%)\n",
      "\n",
      "Training - Epoch: 560 loss: 2.3012517618815105 accuracy: 11.236666666666666\n",
      "\n",
      "Test set: Average loss: 2.3012, Accuracy: 1135/10000 (11%)\n",
      "\n",
      "Training - Epoch: 561 loss: 2.3012517537434896 accuracy: 11.236666666666666\n",
      "\n",
      "Test set: Average loss: 2.3012, Accuracy: 1135/10000 (11%)\n",
      "\n",
      "Training - Epoch: 562 loss: 2.301251746368408 accuracy: 11.236666666666666\n",
      "\n",
      "Test set: Average loss: 2.3012, Accuracy: 1135/10000 (11%)\n",
      "\n",
      "Training - Epoch: 563 loss: 2.3012518046061197 accuracy: 11.236666666666666\n",
      "\n",
      "Test set: Average loss: 2.3012, Accuracy: 1135/10000 (11%)\n",
      "\n",
      "Training - Epoch: 564 loss: 2.3012517588297525 accuracy: 11.236666666666666\n",
      "\n",
      "Test set: Average loss: 2.3012, Accuracy: 1135/10000 (11%)\n",
      "\n",
      "Training - Epoch: 565 loss: 2.3012517735799154 accuracy: 11.236666666666666\n",
      "\n",
      "Test set: Average loss: 2.3012, Accuracy: 1135/10000 (11%)\n",
      "\n",
      "Training - Epoch: 566 loss: 2.301251765950521 accuracy: 11.236666666666666\n",
      "\n",
      "Test set: Average loss: 2.3012, Accuracy: 1135/10000 (11%)\n",
      "\n",
      "Training - Epoch: 567 loss: 2.30125177637736 accuracy: 11.236666666666666\n",
      "\n",
      "Test set: Average loss: 2.3012, Accuracy: 1135/10000 (11%)\n",
      "\n",
      "Training - Epoch: 568 loss: 2.301251798756917 accuracy: 11.236666666666666\n",
      "\n",
      "Test set: Average loss: 2.3012, Accuracy: 1135/10000 (11%)\n",
      "\n",
      "Training - Epoch: 569 loss: 2.3012517834981283 accuracy: 11.236666666666666\n",
      "\n",
      "Test set: Average loss: 2.3012, Accuracy: 1135/10000 (11%)\n",
      "\n",
      "Training - Epoch: 570 loss: 2.3012517537434896 accuracy: 11.236666666666666\n",
      "\n",
      "Test set: Average loss: 2.3012, Accuracy: 1135/10000 (11%)\n",
      "\n",
      "Training - Epoch: 571 loss: 2.3012517799377443 accuracy: 11.236666666666666\n",
      "\n",
      "Test set: Average loss: 2.3012, Accuracy: 1135/10000 (11%)\n",
      "\n",
      "Training - Epoch: 572 loss: 2.301251781463623 accuracy: 11.236666666666666\n",
      "\n",
      "Test set: Average loss: 2.3012, Accuracy: 1135/10000 (11%)\n",
      "\n",
      "Training - Epoch: 573 loss: 2.301251782735189 accuracy: 11.236666666666666\n",
      "\n",
      "Test set: Average loss: 2.3012, Accuracy: 1135/10000 (11%)\n",
      "\n",
      "Training - Epoch: 574 loss: 2.3012517176310223 accuracy: 11.236666666666666\n",
      "\n",
      "Test set: Average loss: 2.3012, Accuracy: 1135/10000 (11%)\n",
      "\n",
      "Training - Epoch: 575 loss: 2.3012517623901365 accuracy: 11.236666666666666\n",
      "\n",
      "Test set: Average loss: 2.3012, Accuracy: 1135/10000 (11%)\n",
      "\n",
      "Training - Epoch: 576 loss: 2.3012517873128253 accuracy: 11.236666666666666\n",
      "\n",
      "Test set: Average loss: 2.3012, Accuracy: 1135/10000 (11%)\n",
      "\n",
      "Training - Epoch: 577 loss: 2.3012517049153645 accuracy: 11.236666666666666\n",
      "\n",
      "Test set: Average loss: 2.3012, Accuracy: 1135/10000 (11%)\n",
      "\n",
      "Training - Epoch: 578 loss: 2.3012517865498863 accuracy: 11.236666666666666\n",
      "\n",
      "Test set: Average loss: 2.3012, Accuracy: 1135/10000 (11%)\n",
      "\n",
      "Training - Epoch: 579 loss: 2.3012517903645833 accuracy: 11.236666666666666\n",
      "\n",
      "Test set: Average loss: 2.3012, Accuracy: 1135/10000 (11%)\n",
      "\n",
      "Training - Epoch: 580 loss: 2.301251749674479 accuracy: 11.236666666666666\n",
      "\n",
      "Test set: Average loss: 2.3012, Accuracy: 1135/10000 (11%)\n",
      "\n",
      "Training - Epoch: 581 loss: 2.3012517211914063 accuracy: 11.236666666666666\n",
      "\n",
      "Test set: Average loss: 2.3012, Accuracy: 1135/10000 (11%)\n",
      "\n",
      "Training - Epoch: 582 loss: 2.301251744588216 accuracy: 11.236666666666666\n",
      "\n",
      "Test set: Average loss: 2.3012, Accuracy: 1135/10000 (11%)\n",
      "\n",
      "Training - Epoch: 583 loss: 2.301251772816976 accuracy: 11.236666666666666\n",
      "\n",
      "Test set: Average loss: 2.3012, Accuracy: 1135/10000 (11%)\n",
      "\n",
      "Training - Epoch: 584 loss: 2.301251766204834 accuracy: 11.236666666666666\n",
      "\n",
      "Test set: Average loss: 2.3012, Accuracy: 1135/10000 (11%)\n",
      "\n",
      "Training - Epoch: 585 loss: 2.301251777394613 accuracy: 11.236666666666666\n",
      "\n",
      "Test set: Average loss: 2.3012, Accuracy: 1135/10000 (11%)\n",
      "\n",
      "Training - Epoch: 586 loss: 2.301251766459147 accuracy: 11.236666666666666\n",
      "\n",
      "Test set: Average loss: 2.3012, Accuracy: 1135/10000 (11%)\n",
      "\n",
      "Training - Epoch: 587 loss: 2.30125173924764 accuracy: 11.236666666666666\n",
      "\n",
      "Test set: Average loss: 2.3012, Accuracy: 1135/10000 (11%)\n",
      "\n",
      "Training - Epoch: 588 loss: 2.3012517700195314 accuracy: 11.236666666666666\n",
      "\n",
      "Test set: Average loss: 2.3012, Accuracy: 1135/10000 (11%)\n",
      "\n",
      "Training - Epoch: 589 loss: 2.301251774088542 accuracy: 11.236666666666666\n",
      "\n",
      "Test set: Average loss: 2.3012, Accuracy: 1135/10000 (11%)\n",
      "\n",
      "Training - Epoch: 590 loss: 2.3012517918904623 accuracy: 11.236666666666666\n",
      "\n",
      "Test set: Average loss: 2.3012, Accuracy: 1135/10000 (11%)\n",
      "\n",
      "Training - Epoch: 591 loss: 2.3012517570495605 accuracy: 11.236666666666666\n",
      "\n",
      "Test set: Average loss: 2.3012, Accuracy: 1135/10000 (11%)\n",
      "\n",
      "Training - Epoch: 592 loss: 2.3012517705281574 accuracy: 11.236666666666666\n",
      "\n",
      "Test set: Average loss: 2.3012, Accuracy: 1135/10000 (11%)\n",
      "\n",
      "Training - Epoch: 593 loss: 2.3012517606099445 accuracy: 11.236666666666666\n",
      "\n",
      "Test set: Average loss: 2.3012, Accuracy: 1135/10000 (11%)\n",
      "\n",
      "Training - Epoch: 594 loss: 2.301251764424642 accuracy: 11.236666666666666\n",
      "\n",
      "Test set: Average loss: 2.3012, Accuracy: 1135/10000 (11%)\n",
      "\n",
      "Training - Epoch: 595 loss: 2.301251778157552 accuracy: 11.236666666666666\n",
      "\n",
      "Test set: Average loss: 2.3012, Accuracy: 1135/10000 (11%)\n",
      "\n",
      "Training - Epoch: 596 loss: 2.3012517573038735 accuracy: 11.236666666666666\n",
      "\n",
      "Test set: Average loss: 2.3012, Accuracy: 1135/10000 (11%)\n",
      "\n",
      "Training - Epoch: 597 loss: 2.3012517567952475 accuracy: 11.236666666666666\n",
      "\n",
      "Test set: Average loss: 2.3012, Accuracy: 1135/10000 (11%)\n",
      "\n",
      "Training - Epoch: 598 loss: 2.3012517339070637 accuracy: 11.236666666666666\n",
      "\n",
      "Test set: Average loss: 2.3012, Accuracy: 1135/10000 (11%)\n",
      "\n",
      "Training - Epoch: 599 loss: 2.301251772816976 accuracy: 11.236666666666666\n",
      "\n",
      "Test set: Average loss: 2.3012, Accuracy: 1135/10000 (11%)\n",
      "\n",
      "Training - Epoch: 600 loss: 2.301251764170329 accuracy: 11.236666666666666\n",
      "\n",
      "Test set: Average loss: 2.3012, Accuracy: 1135/10000 (11%)\n",
      "\n",
      "Training - Epoch: 601 loss: 2.301251754252116 accuracy: 11.236666666666666\n",
      "\n",
      "Test set: Average loss: 2.3012, Accuracy: 1135/10000 (11%)\n",
      "\n",
      "Training - Epoch: 602 loss: 2.301251784515381 accuracy: 11.236666666666666\n",
      "\n",
      "Test set: Average loss: 2.3012, Accuracy: 1135/10000 (11%)\n",
      "\n",
      "Training - Epoch: 603 loss: 2.3012517618815105 accuracy: 11.236666666666666\n",
      "\n",
      "Test set: Average loss: 2.3012, Accuracy: 1135/10000 (11%)\n",
      "\n",
      "Training - Epoch: 604 loss: 2.301251773071289 accuracy: 11.236666666666666\n",
      "\n",
      "Test set: Average loss: 2.3012, Accuracy: 1135/10000 (11%)\n",
      "\n",
      "Training - Epoch: 605 loss: 2.3012517372131347 accuracy: 11.236666666666666\n",
      "\n",
      "Test set: Average loss: 2.3012, Accuracy: 1135/10000 (11%)\n",
      "\n",
      "Training - Epoch: 606 loss: 2.301251727294922 accuracy: 11.236666666666666\n",
      "\n",
      "Test set: Average loss: 2.3012, Accuracy: 1135/10000 (11%)\n",
      "\n",
      "Training - Epoch: 607 loss: 2.3012517552693685 accuracy: 11.236666666666666\n",
      "\n",
      "Test set: Average loss: 2.3012, Accuracy: 1135/10000 (11%)\n",
      "\n",
      "Training - Epoch: 608 loss: 2.301251749420166 accuracy: 11.236666666666666\n",
      "\n",
      "Test set: Average loss: 2.3012, Accuracy: 1135/10000 (11%)\n",
      "\n",
      "Training - Epoch: 609 loss: 2.3012517468770346 accuracy: 11.236666666666666\n",
      "\n",
      "Test set: Average loss: 2.3012, Accuracy: 1135/10000 (11%)\n",
      "\n",
      "Training - Epoch: 610 loss: 2.301251749420166 accuracy: 11.236666666666666\n",
      "\n",
      "Test set: Average loss: 2.3012, Accuracy: 1135/10000 (11%)\n",
      "\n",
      "Training - Epoch: 611 loss: 2.3012517700195314 accuracy: 11.236666666666666\n",
      "\n",
      "Test set: Average loss: 2.3012, Accuracy: 1135/10000 (11%)\n",
      "\n",
      "Training - Epoch: 612 loss: 2.3012517011006675 accuracy: 11.236666666666666\n",
      "\n",
      "Test set: Average loss: 2.3012, Accuracy: 1135/10000 (11%)\n",
      "\n",
      "Training - Epoch: 613 loss: 2.301251770782471 accuracy: 11.236666666666666\n",
      "\n",
      "Test set: Average loss: 2.3012, Accuracy: 1135/10000 (11%)\n",
      "\n",
      "Training - Epoch: 614 loss: 2.3012517318725587 accuracy: 11.236666666666666\n",
      "\n",
      "Test set: Average loss: 2.3012, Accuracy: 1135/10000 (11%)\n",
      "\n",
      "Training - Epoch: 615 loss: 2.301251744333903 accuracy: 11.236666666666666\n",
      "\n",
      "Test set: Average loss: 2.3012, Accuracy: 1135/10000 (11%)\n",
      "\n",
      "Training - Epoch: 616 loss: 2.3012517306009928 accuracy: 11.236666666666666\n",
      "\n",
      "Test set: Average loss: 2.3012, Accuracy: 1135/10000 (11%)\n",
      "\n",
      "Training - Epoch: 617 loss: 2.3012517402648927 accuracy: 11.236666666666666\n",
      "\n",
      "Test set: Average loss: 2.3012, Accuracy: 1135/10000 (11%)\n",
      "\n",
      "Training - Epoch: 618 loss: 2.301251735941569 accuracy: 11.236666666666666\n",
      "\n",
      "Test set: Average loss: 2.3012, Accuracy: 1135/10000 (11%)\n",
      "\n",
      "Training - Epoch: 619 loss: 2.3012517717997234 accuracy: 11.236666666666666\n",
      "\n",
      "Test set: Average loss: 2.3012, Accuracy: 1135/10000 (11%)\n",
      "\n",
      "Training - Epoch: 620 loss: 2.301251744588216 accuracy: 11.236666666666666\n",
      "\n",
      "Test set: Average loss: 2.3012, Accuracy: 1135/10000 (11%)\n",
      "\n",
      "Training - Epoch: 621 loss: 2.301251796722412 accuracy: 11.236666666666666\n",
      "\n",
      "Test set: Average loss: 2.3012, Accuracy: 1135/10000 (11%)\n",
      "\n",
      "Training - Epoch: 622 loss: 2.3012517753601074 accuracy: 11.236666666666666\n",
      "\n",
      "Test set: Average loss: 2.3012, Accuracy: 1135/10000 (11%)\n",
      "\n",
      "Training - Epoch: 623 loss: 2.3012517570495605 accuracy: 11.236666666666666\n",
      "\n",
      "Test set: Average loss: 2.3012, Accuracy: 1135/10000 (11%)\n",
      "\n",
      "Training - Epoch: 624 loss: 2.301251770782471 accuracy: 11.236666666666666\n",
      "\n",
      "Test set: Average loss: 2.3012, Accuracy: 1135/10000 (11%)\n",
      "\n",
      "Training - Epoch: 625 loss: 2.30125177154541 accuracy: 11.236666666666666\n",
      "\n",
      "Test set: Average loss: 2.3012, Accuracy: 1135/10000 (11%)\n",
      "\n",
      "Training - Epoch: 626 loss: 2.301251766204834 accuracy: 11.236666666666666\n",
      "\n",
      "Test set: Average loss: 2.3012, Accuracy: 1135/10000 (11%)\n",
      "\n",
      "Training - Epoch: 627 loss: 2.3012517550150555 accuracy: 11.236666666666666\n",
      "\n",
      "Test set: Average loss: 2.3012, Accuracy: 1135/10000 (11%)\n",
      "\n",
      "Training - Epoch: 628 loss: 2.301251749165853 accuracy: 11.236666666666666\n",
      "\n",
      "Test set: Average loss: 2.3012, Accuracy: 1135/10000 (11%)\n",
      "\n",
      "Training - Epoch: 629 loss: 2.301251769510905 accuracy: 11.236666666666666\n",
      "\n",
      "Test set: Average loss: 2.3012, Accuracy: 1135/10000 (11%)\n",
      "\n",
      "Training - Epoch: 630 loss: 2.3012517837524413 accuracy: 11.236666666666666\n",
      "\n",
      "Test set: Average loss: 2.3012, Accuracy: 1135/10000 (11%)\n",
      "\n",
      "Training - Epoch: 631 loss: 2.301251751454671 accuracy: 11.236666666666666\n",
      "\n",
      "Test set: Average loss: 2.3012, Accuracy: 1135/10000 (11%)\n",
      "\n",
      "Training - Epoch: 632 loss: 2.3012517578125 accuracy: 11.236666666666666\n",
      "\n",
      "Test set: Average loss: 2.3012, Accuracy: 1135/10000 (11%)\n",
      "\n",
      "Training - Epoch: 633 loss: 2.3012517341613767 accuracy: 11.236666666666666\n",
      "\n",
      "Test set: Average loss: 2.3012, Accuracy: 1135/10000 (11%)\n",
      "\n",
      "Training - Epoch: 634 loss: 2.3012517962137857 accuracy: 11.236666666666666\n",
      "\n",
      "Test set: Average loss: 2.3012, Accuracy: 1135/10000 (11%)\n",
      "\n",
      "Training - Epoch: 635 loss: 2.3012517768859864 accuracy: 11.236666666666666\n",
      "\n",
      "Test set: Average loss: 2.3012, Accuracy: 1135/10000 (11%)\n",
      "\n",
      "Training - Epoch: 636 loss: 2.3012517504374186 accuracy: 11.236666666666666\n",
      "\n",
      "Test set: Average loss: 2.3012, Accuracy: 1135/10000 (11%)\n",
      "\n",
      "Training - Epoch: 637 loss: 2.3012517669677734 accuracy: 11.236666666666666\n",
      "\n",
      "Test set: Average loss: 2.3012, Accuracy: 1135/10000 (11%)\n",
      "\n",
      "Training - Epoch: 638 loss: 2.3012517453511556 accuracy: 11.236666666666666\n",
      "\n",
      "Test set: Average loss: 2.3012, Accuracy: 1135/10000 (11%)\n",
      "\n",
      "Training - Epoch: 639 loss: 2.3012517603556315 accuracy: 11.236666666666666\n",
      "\n",
      "Test set: Average loss: 2.3012, Accuracy: 1135/10000 (11%)\n",
      "\n",
      "Training - Epoch: 640 loss: 2.3012517585754395 accuracy: 11.236666666666666\n",
      "\n",
      "Test set: Average loss: 2.3012, Accuracy: 1135/10000 (11%)\n",
      "\n",
      "Training - Epoch: 641 loss: 2.301251759847005 accuracy: 11.236666666666666\n",
      "\n",
      "Test set: Average loss: 2.3012, Accuracy: 1135/10000 (11%)\n",
      "\n",
      "Training - Epoch: 642 loss: 2.301251769256592 accuracy: 11.236666666666666\n",
      "\n",
      "Test set: Average loss: 2.3012, Accuracy: 1135/10000 (11%)\n",
      "\n",
      "Training - Epoch: 643 loss: 2.301251756540934 accuracy: 11.236666666666666\n",
      "\n",
      "Test set: Average loss: 2.3012, Accuracy: 1135/10000 (11%)\n",
      "\n",
      "Training - Epoch: 644 loss: 2.301251762898763 accuracy: 11.236666666666666\n",
      "\n",
      "Test set: Average loss: 2.3012, Accuracy: 1135/10000 (11%)\n",
      "\n",
      "Training - Epoch: 645 loss: 2.301251749420166 accuracy: 11.236666666666666\n",
      "\n",
      "Test set: Average loss: 2.3012, Accuracy: 1135/10000 (11%)\n",
      "\n",
      "Training - Epoch: 646 loss: 2.3012517784118653 accuracy: 11.236666666666666\n",
      "\n",
      "Test set: Average loss: 2.3012, Accuracy: 1135/10000 (11%)\n",
      "\n",
      "Training - Epoch: 647 loss: 2.301251754506429 accuracy: 11.236666666666666\n",
      "\n",
      "Test set: Average loss: 2.3012, Accuracy: 1135/10000 (11%)\n",
      "\n",
      "Training - Epoch: 648 loss: 2.3012517738342284 accuracy: 11.236666666666666\n",
      "\n",
      "Test set: Average loss: 2.3012, Accuracy: 1135/10000 (11%)\n",
      "\n",
      "Training - Epoch: 649 loss: 2.3012517606099445 accuracy: 11.236666666666666\n",
      "\n",
      "Test set: Average loss: 2.3012, Accuracy: 1135/10000 (11%)\n",
      "\n",
      "Training - Epoch: 650 loss: 2.3012517178853353 accuracy: 11.236666666666666\n",
      "\n",
      "Test set: Average loss: 2.3012, Accuracy: 1135/10000 (11%)\n",
      "\n",
      "Training - Epoch: 651 loss: 2.3012517674764 accuracy: 11.236666666666666\n",
      "\n",
      "Test set: Average loss: 2.3012, Accuracy: 1135/10000 (11%)\n",
      "\n",
      "Training - Epoch: 652 loss: 2.301251723734538 accuracy: 11.236666666666666\n",
      "\n",
      "Test set: Average loss: 2.3012, Accuracy: 1135/10000 (11%)\n",
      "\n",
      "Training - Epoch: 653 loss: 2.3012517929077148 accuracy: 11.236666666666666\n",
      "\n",
      "Test set: Average loss: 2.3012, Accuracy: 1135/10000 (11%)\n",
      "\n",
      "Training - Epoch: 654 loss: 2.3012517285664877 accuracy: 11.236666666666666\n",
      "\n",
      "Test set: Average loss: 2.3012, Accuracy: 1135/10000 (11%)\n",
      "\n",
      "Training - Epoch: 655 loss: 2.3012517522176106 accuracy: 11.236666666666666\n",
      "\n",
      "Test set: Average loss: 2.3012, Accuracy: 1135/10000 (11%)\n",
      "\n",
      "Training - Epoch: 656 loss: 2.301251777903239 accuracy: 11.236666666666666\n",
      "\n",
      "Test set: Average loss: 2.3012, Accuracy: 1135/10000 (11%)\n",
      "\n",
      "Training - Epoch: 657 loss: 2.301251781463623 accuracy: 11.236666666666666\n",
      "\n",
      "Test set: Average loss: 2.3012, Accuracy: 1135/10000 (11%)\n",
      "\n",
      "Training - Epoch: 658 loss: 2.301251759592692 accuracy: 11.236666666666666\n",
      "\n",
      "Test set: Average loss: 2.3012, Accuracy: 1135/10000 (11%)\n",
      "\n",
      "Training - Epoch: 659 loss: 2.301251767985026 accuracy: 11.236666666666666\n",
      "\n",
      "Test set: Average loss: 2.3012, Accuracy: 1135/10000 (11%)\n",
      "\n",
      "Training - Epoch: 660 loss: 2.301251739756266 accuracy: 11.236666666666666\n",
      "\n",
      "Test set: Average loss: 2.3012, Accuracy: 1135/10000 (11%)\n",
      "\n",
      "Training - Epoch: 661 loss: 2.301251749420166 accuracy: 11.236666666666666\n",
      "\n",
      "Test set: Average loss: 2.3012, Accuracy: 1135/10000 (11%)\n",
      "\n",
      "Training - Epoch: 662 loss: 2.3012517603556315 accuracy: 11.236666666666666\n",
      "\n",
      "Test set: Average loss: 2.3012, Accuracy: 1135/10000 (11%)\n",
      "\n",
      "Training - Epoch: 663 loss: 2.3012517384847007 accuracy: 11.236666666666666\n",
      "\n",
      "Test set: Average loss: 2.3012, Accuracy: 1135/10000 (11%)\n",
      "\n",
      "Training - Epoch: 664 loss: 2.301251820119222 accuracy: 11.236666666666666\n",
      "\n",
      "Test set: Average loss: 2.3012, Accuracy: 1135/10000 (11%)\n",
      "\n",
      "Training - Epoch: 665 loss: 2.3012517616271975 accuracy: 11.236666666666666\n",
      "\n",
      "Test set: Average loss: 2.3012, Accuracy: 1135/10000 (11%)\n",
      "\n",
      "Training - Epoch: 666 loss: 2.3012517506917316 accuracy: 11.236666666666666\n",
      "\n",
      "Test set: Average loss: 2.3012, Accuracy: 1135/10000 (11%)\n",
      "\n",
      "Training - Epoch: 667 loss: 2.3012517702738444 accuracy: 11.236666666666666\n",
      "\n",
      "Test set: Average loss: 2.3012, Accuracy: 1135/10000 (11%)\n",
      "\n",
      "Training - Epoch: 668 loss: 2.301251746114095 accuracy: 11.236666666666666\n",
      "\n",
      "Test set: Average loss: 2.3012, Accuracy: 1135/10000 (11%)\n",
      "\n",
      "Training - Epoch: 669 loss: 2.3012517331441242 accuracy: 11.236666666666666\n",
      "\n",
      "Test set: Average loss: 2.3012, Accuracy: 1135/10000 (11%)\n",
      "\n",
      "Training - Epoch: 670 loss: 2.3012517687479654 accuracy: 11.236666666666666\n",
      "\n",
      "Test set: Average loss: 2.3012, Accuracy: 1135/10000 (11%)\n",
      "\n",
      "Training - Epoch: 671 loss: 2.3012517651875815 accuracy: 11.236666666666666\n",
      "\n",
      "Test set: Average loss: 2.3012, Accuracy: 1135/10000 (11%)\n",
      "\n",
      "Training - Epoch: 672 loss: 2.3012517552693685 accuracy: 11.236666666666666\n",
      "\n",
      "Test set: Average loss: 2.3012, Accuracy: 1135/10000 (11%)\n",
      "\n",
      "Training - Epoch: 673 loss: 2.3012517667134604 accuracy: 11.236666666666666\n",
      "\n",
      "Test set: Average loss: 2.3012, Accuracy: 1135/10000 (11%)\n",
      "\n",
      "Training - Epoch: 674 loss: 2.3012517336527507 accuracy: 11.236666666666666\n",
      "\n",
      "Test set: Average loss: 2.3012, Accuracy: 1135/10000 (11%)\n",
      "\n",
      "Training - Epoch: 675 loss: 2.3012517489115396 accuracy: 11.236666666666666\n",
      "\n",
      "Test set: Average loss: 2.3012, Accuracy: 1135/10000 (11%)\n",
      "\n",
      "Training - Epoch: 676 loss: 2.301251746622721 accuracy: 11.236666666666666\n",
      "\n",
      "Test set: Average loss: 2.3012, Accuracy: 1135/10000 (11%)\n",
      "\n",
      "Training - Epoch: 677 loss: 2.3012517893473308 accuracy: 11.236666666666666\n",
      "\n",
      "Test set: Average loss: 2.3012, Accuracy: 1135/10000 (11%)\n",
      "\n",
      "Training - Epoch: 678 loss: 2.3012517232259113 accuracy: 11.236666666666666\n",
      "\n",
      "Test set: Average loss: 2.3012, Accuracy: 1135/10000 (11%)\n",
      "\n",
      "Training - Epoch: 679 loss: 2.3012517850240073 accuracy: 11.236666666666666\n",
      "\n",
      "Test set: Average loss: 2.3012, Accuracy: 1135/10000 (11%)\n",
      "\n",
      "Training - Epoch: 680 loss: 2.301251780954997 accuracy: 11.236666666666666\n",
      "\n",
      "Test set: Average loss: 2.3012, Accuracy: 1135/10000 (11%)\n",
      "\n",
      "Training - Epoch: 681 loss: 2.3012517290751138 accuracy: 11.236666666666666\n",
      "\n",
      "Test set: Average loss: 2.3012, Accuracy: 1135/10000 (11%)\n",
      "\n",
      "Training - Epoch: 682 loss: 2.301251749674479 accuracy: 11.236666666666666\n",
      "\n",
      "Test set: Average loss: 2.3012, Accuracy: 1135/10000 (11%)\n",
      "\n",
      "Training - Epoch: 683 loss: 2.301251759847005 accuracy: 11.236666666666666\n",
      "\n",
      "Test set: Average loss: 2.3012, Accuracy: 1135/10000 (11%)\n",
      "\n",
      "Training - Epoch: 684 loss: 2.301251762898763 accuracy: 11.236666666666666\n",
      "\n",
      "Test set: Average loss: 2.3012, Accuracy: 1135/10000 (11%)\n",
      "\n",
      "Training - Epoch: 685 loss: 2.301251759592692 accuracy: 11.236666666666666\n",
      "\n",
      "Test set: Average loss: 2.3012, Accuracy: 1135/10000 (11%)\n",
      "\n",
      "Training - Epoch: 686 loss: 2.3012517888387043 accuracy: 11.236666666666666\n",
      "\n",
      "Test set: Average loss: 2.3012, Accuracy: 1135/10000 (11%)\n",
      "\n",
      "Training - Epoch: 687 loss: 2.3012517435709636 accuracy: 11.236666666666666\n",
      "\n",
      "Test set: Average loss: 2.3012, Accuracy: 1135/10000 (11%)\n",
      "\n",
      "Training - Epoch: 688 loss: 2.3012517939249673 accuracy: 11.236666666666666\n",
      "\n",
      "Test set: Average loss: 2.3012, Accuracy: 1135/10000 (11%)\n",
      "\n",
      "Training - Epoch: 689 loss: 2.301251720428467 accuracy: 11.236666666666666\n",
      "\n",
      "Test set: Average loss: 2.3012, Accuracy: 1135/10000 (11%)\n",
      "\n",
      "Training - Epoch: 690 loss: 2.30125178120931 accuracy: 11.236666666666666\n",
      "\n",
      "Test set: Average loss: 2.3012, Accuracy: 1135/10000 (11%)\n",
      "\n",
      "Training - Epoch: 691 loss: 2.301251782735189 accuracy: 11.236666666666666\n",
      "\n",
      "Test set: Average loss: 2.3012, Accuracy: 1135/10000 (11%)\n",
      "\n",
      "Training - Epoch: 692 loss: 2.3012517506917316 accuracy: 11.236666666666666\n",
      "\n",
      "Test set: Average loss: 2.3012, Accuracy: 1135/10000 (11%)\n",
      "\n",
      "Training - Epoch: 693 loss: 2.301251756540934 accuracy: 11.236666666666666\n",
      "\n",
      "Test set: Average loss: 2.3012, Accuracy: 1135/10000 (11%)\n",
      "\n",
      "Training - Epoch: 694 loss: 2.3012517537434896 accuracy: 11.236666666666666\n",
      "\n",
      "Test set: Average loss: 2.3012, Accuracy: 1135/10000 (11%)\n",
      "\n",
      "Training - Epoch: 695 loss: 2.301251780700684 accuracy: 11.236666666666666\n",
      "\n",
      "Test set: Average loss: 2.3012, Accuracy: 1135/10000 (11%)\n",
      "\n",
      "Training - Epoch: 696 loss: 2.301251756032308 accuracy: 11.236666666666666\n",
      "\n",
      "Test set: Average loss: 2.3012, Accuracy: 1135/10000 (11%)\n",
      "\n",
      "Training - Epoch: 697 loss: 2.3012517295837402 accuracy: 11.236666666666666\n",
      "\n",
      "Test set: Average loss: 2.3012, Accuracy: 1135/10000 (11%)\n",
      "\n",
      "Training - Epoch: 698 loss: 2.301251766204834 accuracy: 11.236666666666666\n",
      "\n",
      "Test set: Average loss: 2.3012, Accuracy: 1135/10000 (11%)\n",
      "\n",
      "Training - Epoch: 699 loss: 2.30125178120931 accuracy: 11.236666666666666\n",
      "\n",
      "Test set: Average loss: 2.3012, Accuracy: 1135/10000 (11%)\n",
      "\n",
      "Training - Epoch: 700 loss: 2.3012517786661784 accuracy: 11.236666666666666\n",
      "\n",
      "Test set: Average loss: 2.3012, Accuracy: 1135/10000 (11%)\n",
      "\n",
      "Training - Epoch: 701 loss: 2.301251754760742 accuracy: 11.236666666666666\n",
      "\n",
      "Test set: Average loss: 2.3012, Accuracy: 1135/10000 (11%)\n",
      "\n",
      "Training - Epoch: 702 loss: 2.3012517534891765 accuracy: 11.236666666666666\n",
      "\n",
      "Test set: Average loss: 2.3012, Accuracy: 1135/10000 (11%)\n",
      "\n",
      "Training - Epoch: 703 loss: 2.301251756540934 accuracy: 11.236666666666666\n",
      "\n",
      "Test set: Average loss: 2.3012, Accuracy: 1135/10000 (11%)\n",
      "\n",
      "Training - Epoch: 704 loss: 2.301251771291097 accuracy: 11.236666666666666\n",
      "\n",
      "Test set: Average loss: 2.3012, Accuracy: 1135/10000 (11%)\n",
      "\n",
      "Training - Epoch: 705 loss: 2.301251746622721 accuracy: 11.236666666666666\n",
      "\n",
      "Test set: Average loss: 2.3012, Accuracy: 1135/10000 (11%)\n",
      "\n",
      "Training - Epoch: 706 loss: 2.3012517639160155 accuracy: 11.236666666666666\n",
      "\n",
      "Test set: Average loss: 2.3012, Accuracy: 1135/10000 (11%)\n",
      "\n",
      "Training - Epoch: 707 loss: 2.3012517262776693 accuracy: 11.236666666666666\n",
      "\n",
      "Test set: Average loss: 2.3012, Accuracy: 1135/10000 (11%)\n",
      "\n",
      "Training - Epoch: 708 loss: 2.301251784515381 accuracy: 11.236666666666666\n",
      "\n",
      "Test set: Average loss: 2.3012, Accuracy: 1135/10000 (11%)\n",
      "\n",
      "Training - Epoch: 709 loss: 2.301251771291097 accuracy: 11.236666666666666\n",
      "\n",
      "Test set: Average loss: 2.3012, Accuracy: 1135/10000 (11%)\n",
      "\n",
      "Training - Epoch: 710 loss: 2.3012517583211265 accuracy: 11.236666666666666\n",
      "\n",
      "Test set: Average loss: 2.3012, Accuracy: 1135/10000 (11%)\n",
      "\n",
      "Training - Epoch: 711 loss: 2.3012517654418945 accuracy: 11.236666666666666\n",
      "\n",
      "Test set: Average loss: 2.3012, Accuracy: 1135/10000 (11%)\n",
      "\n",
      "Training - Epoch: 712 loss: 2.30125176264445 accuracy: 11.236666666666666\n",
      "\n",
      "Test set: Average loss: 2.3012, Accuracy: 1135/10000 (11%)\n",
      "\n",
      "Training - Epoch: 713 loss: 2.301251759847005 accuracy: 11.236666666666666\n",
      "\n",
      "Test set: Average loss: 2.3012, Accuracy: 1135/10000 (11%)\n",
      "\n",
      "Training - Epoch: 714 loss: 2.301251769765218 accuracy: 11.236666666666666\n",
      "\n",
      "Test set: Average loss: 2.3012, Accuracy: 1135/10000 (11%)\n",
      "\n",
      "Training - Epoch: 715 loss: 2.301251747894287 accuracy: 11.236666666666666\n",
      "\n",
      "Test set: Average loss: 2.3012, Accuracy: 1135/10000 (11%)\n",
      "\n",
      "Training - Epoch: 716 loss: 2.301251759338379 accuracy: 11.236666666666666\n",
      "\n",
      "Test set: Average loss: 2.3012, Accuracy: 1135/10000 (11%)\n",
      "\n",
      "Training - Epoch: 717 loss: 2.3012517700195314 accuracy: 11.236666666666666\n",
      "\n",
      "Test set: Average loss: 2.3012, Accuracy: 1135/10000 (11%)\n",
      "\n",
      "Training - Epoch: 718 loss: 2.3012517649332684 accuracy: 11.236666666666666\n",
      "\n",
      "Test set: Average loss: 2.3012, Accuracy: 1135/10000 (11%)\n",
      "\n",
      "Training - Epoch: 719 loss: 2.301251725769043 accuracy: 11.236666666666666\n",
      "\n",
      "Test set: Average loss: 2.3012, Accuracy: 1135/10000 (11%)\n",
      "\n",
      "Training - Epoch: 720 loss: 2.301251773071289 accuracy: 11.236666666666666\n",
      "\n",
      "Test set: Average loss: 2.3012, Accuracy: 1135/10000 (11%)\n",
      "\n",
      "Training - Epoch: 721 loss: 2.3012517400105796 accuracy: 11.236666666666666\n",
      "\n",
      "Test set: Average loss: 2.3012, Accuracy: 1135/10000 (11%)\n",
      "\n",
      "Training - Epoch: 722 loss: 2.3012517367045087 accuracy: 11.236666666666666\n",
      "\n",
      "Test set: Average loss: 2.3012, Accuracy: 1135/10000 (11%)\n",
      "\n",
      "Training - Epoch: 723 loss: 2.301251774597168 accuracy: 11.236666666666666\n",
      "\n",
      "Test set: Average loss: 2.3012, Accuracy: 1135/10000 (11%)\n",
      "\n",
      "Training - Epoch: 724 loss: 2.3012517601013185 accuracy: 11.236666666666666\n",
      "\n",
      "Test set: Average loss: 2.3012, Accuracy: 1135/10000 (11%)\n",
      "\n",
      "Training - Epoch: 725 loss: 2.301251770782471 accuracy: 11.236666666666666\n",
      "\n",
      "Test set: Average loss: 2.3012, Accuracy: 1135/10000 (11%)\n",
      "\n",
      "Training - Epoch: 726 loss: 2.3012517573038735 accuracy: 11.236666666666666\n",
      "\n",
      "Test set: Average loss: 2.3012, Accuracy: 1135/10000 (11%)\n",
      "\n",
      "Training - Epoch: 727 loss: 2.3012517957051597 accuracy: 11.236666666666666\n",
      "\n",
      "Test set: Average loss: 2.3012, Accuracy: 1135/10000 (11%)\n",
      "\n",
      "Training - Epoch: 728 loss: 2.3012517784118653 accuracy: 11.236666666666666\n",
      "\n",
      "Test set: Average loss: 2.3012, Accuracy: 1135/10000 (11%)\n",
      "\n",
      "Training - Epoch: 729 loss: 2.3012517621358235 accuracy: 11.236666666666666\n",
      "\n",
      "Test set: Average loss: 2.3012, Accuracy: 1135/10000 (11%)\n",
      "\n",
      "Training - Epoch: 730 loss: 2.3012517651875815 accuracy: 11.236666666666666\n",
      "\n",
      "Test set: Average loss: 2.3012, Accuracy: 1135/10000 (11%)\n",
      "\n",
      "Training - Epoch: 731 loss: 2.30125177154541 accuracy: 11.236666666666666\n",
      "\n",
      "Test set: Average loss: 2.3012, Accuracy: 1135/10000 (11%)\n",
      "\n",
      "Training - Epoch: 732 loss: 2.301251764678955 accuracy: 11.236666666666666\n",
      "\n",
      "Test set: Average loss: 2.3012, Accuracy: 1135/10000 (11%)\n",
      "\n",
      "Training - Epoch: 733 loss: 2.301251774851481 accuracy: 11.236666666666666\n",
      "\n",
      "Test set: Average loss: 2.3012, Accuracy: 1135/10000 (11%)\n",
      "\n",
      "Training - Epoch: 734 loss: 2.301251754506429 accuracy: 11.236666666666666\n",
      "\n",
      "Test set: Average loss: 2.3012, Accuracy: 1135/10000 (11%)\n",
      "\n",
      "Training - Epoch: 735 loss: 2.3012517883300783 accuracy: 11.236666666666666\n",
      "\n",
      "Test set: Average loss: 2.3012, Accuracy: 1135/10000 (11%)\n",
      "\n",
      "Training - Epoch: 736 loss: 2.3012517654418945 accuracy: 11.236666666666666\n",
      "\n",
      "Test set: Average loss: 2.3012, Accuracy: 1135/10000 (11%)\n",
      "\n",
      "Training - Epoch: 737 loss: 2.3012517890930178 accuracy: 11.236666666666666\n",
      "\n",
      "Test set: Average loss: 2.3012, Accuracy: 1135/10000 (11%)\n",
      "\n",
      "Training - Epoch: 738 loss: 2.3012517888387043 accuracy: 11.236666666666666\n",
      "\n",
      "Test set: Average loss: 2.3012, Accuracy: 1135/10000 (11%)\n",
      "\n",
      "Training - Epoch: 739 loss: 2.3012517400105796 accuracy: 11.236666666666666\n",
      "\n",
      "Test set: Average loss: 2.3012, Accuracy: 1135/10000 (11%)\n",
      "\n",
      "Training - Epoch: 740 loss: 2.3012518173217775 accuracy: 11.236666666666666\n",
      "\n",
      "Test set: Average loss: 2.3012, Accuracy: 1135/10000 (11%)\n",
      "\n",
      "Training - Epoch: 741 loss: 2.3012517832438153 accuracy: 11.236666666666666\n",
      "\n",
      "Test set: Average loss: 2.3012, Accuracy: 1135/10000 (11%)\n",
      "\n",
      "Training - Epoch: 742 loss: 2.3012517384847007 accuracy: 11.236666666666666\n",
      "\n",
      "Test set: Average loss: 2.3012, Accuracy: 1135/10000 (11%)\n",
      "\n",
      "Training - Epoch: 743 loss: 2.3012517801920573 accuracy: 11.236666666666666\n",
      "\n",
      "Test set: Average loss: 2.3012, Accuracy: 1135/10000 (11%)\n",
      "\n",
      "Training - Epoch: 744 loss: 2.301251761118571 accuracy: 11.236666666666666\n",
      "\n",
      "Test set: Average loss: 2.3012, Accuracy: 1135/10000 (11%)\n",
      "\n",
      "Training - Epoch: 745 loss: 2.3012517603556315 accuracy: 11.236666666666666\n",
      "\n",
      "Test set: Average loss: 2.3012, Accuracy: 1135/10000 (11%)\n",
      "\n",
      "Training - Epoch: 746 loss: 2.3012517784118653 accuracy: 11.236666666666666\n",
      "\n",
      "Test set: Average loss: 2.3012, Accuracy: 1135/10000 (11%)\n",
      "\n",
      "Training - Epoch: 747 loss: 2.301251732889811 accuracy: 11.236666666666666\n",
      "\n",
      "Test set: Average loss: 2.3012, Accuracy: 1135/10000 (11%)\n",
      "\n",
      "Training - Epoch: 748 loss: 2.3012517840067543 accuracy: 11.236666666666666\n",
      "\n",
      "Test set: Average loss: 2.3012, Accuracy: 1135/10000 (11%)\n",
      "\n",
      "Training - Epoch: 749 loss: 2.3012517705281574 accuracy: 11.236666666666666\n",
      "\n",
      "Test set: Average loss: 2.3012, Accuracy: 1135/10000 (11%)\n",
      "\n",
      "Training - Epoch: 750 loss: 2.3012517672220865 accuracy: 11.236666666666666\n",
      "\n",
      "Test set: Average loss: 2.3012, Accuracy: 1135/10000 (11%)\n",
      "\n",
      "Training - Epoch: 751 loss: 2.3012517962137857 accuracy: 11.236666666666666\n",
      "\n",
      "Test set: Average loss: 2.3012, Accuracy: 1135/10000 (11%)\n",
      "\n",
      "Training - Epoch: 752 loss: 2.301251730855306 accuracy: 11.236666666666666\n",
      "\n",
      "Test set: Average loss: 2.3012, Accuracy: 1135/10000 (11%)\n",
      "\n",
      "Training - Epoch: 753 loss: 2.3012517865498863 accuracy: 11.236666666666666\n",
      "\n",
      "Test set: Average loss: 2.3012, Accuracy: 1135/10000 (11%)\n",
      "\n",
      "Training - Epoch: 754 loss: 2.3012517603556315 accuracy: 11.236666666666666\n",
      "\n",
      "Test set: Average loss: 2.3012, Accuracy: 1135/10000 (11%)\n",
      "\n",
      "Training - Epoch: 755 loss: 2.301251780954997 accuracy: 11.236666666666666\n",
      "\n",
      "Test set: Average loss: 2.3012, Accuracy: 1135/10000 (11%)\n",
      "\n",
      "Training - Epoch: 756 loss: 2.3012517639160155 accuracy: 11.236666666666666\n",
      "\n",
      "Test set: Average loss: 2.3012, Accuracy: 1135/10000 (11%)\n",
      "\n",
      "Training - Epoch: 757 loss: 2.301251769256592 accuracy: 11.236666666666666\n",
      "\n",
      "Test set: Average loss: 2.3012, Accuracy: 1135/10000 (11%)\n",
      "\n",
      "Training - Epoch: 758 loss: 2.3012517669677734 accuracy: 11.236666666666666\n",
      "\n",
      "Test set: Average loss: 2.3012, Accuracy: 1135/10000 (11%)\n",
      "\n",
      "Training - Epoch: 759 loss: 2.3012517435709636 accuracy: 11.236666666666666\n",
      "\n",
      "Test set: Average loss: 2.3012, Accuracy: 1135/10000 (11%)\n",
      "\n",
      "Training - Epoch: 760 loss: 2.301251760864258 accuracy: 11.236666666666666\n",
      "\n",
      "Test set: Average loss: 2.3012, Accuracy: 1135/10000 (11%)\n",
      "\n",
      "Training - Epoch: 761 loss: 2.3012517682393394 accuracy: 11.236666666666666\n",
      "\n",
      "Test set: Average loss: 2.3012, Accuracy: 1135/10000 (11%)\n",
      "\n",
      "Training - Epoch: 762 loss: 2.301251774342855 accuracy: 11.236666666666666\n",
      "\n",
      "Test set: Average loss: 2.3012, Accuracy: 1135/10000 (11%)\n",
      "\n",
      "Training - Epoch: 763 loss: 2.301251806894938 accuracy: 11.236666666666666\n",
      "\n",
      "Test set: Average loss: 2.3012, Accuracy: 1135/10000 (11%)\n",
      "\n",
      "Training - Epoch: 764 loss: 2.301251756032308 accuracy: 11.236666666666666\n",
      "\n",
      "Test set: Average loss: 2.3012, Accuracy: 1135/10000 (11%)\n",
      "\n",
      "Training - Epoch: 765 loss: 2.301251741282145 accuracy: 11.236666666666666\n",
      "\n",
      "Test set: Average loss: 2.3012, Accuracy: 1135/10000 (11%)\n",
      "\n",
      "Training - Epoch: 766 loss: 2.301251761118571 accuracy: 11.236666666666666\n",
      "\n",
      "Test set: Average loss: 2.3012, Accuracy: 1135/10000 (11%)\n",
      "\n",
      "Training - Epoch: 767 loss: 2.301251766459147 accuracy: 11.236666666666666\n",
      "\n",
      "Test set: Average loss: 2.3012, Accuracy: 1135/10000 (11%)\n",
      "\n",
      "Training - Epoch: 768 loss: 2.301251759592692 accuracy: 11.236666666666666\n",
      "\n",
      "Test set: Average loss: 2.3012, Accuracy: 1135/10000 (11%)\n",
      "\n",
      "Training - Epoch: 769 loss: 2.3012517501831056 accuracy: 11.236666666666666\n",
      "\n",
      "Test set: Average loss: 2.3012, Accuracy: 1135/10000 (11%)\n",
      "\n",
      "Training - Epoch: 770 loss: 2.301251767985026 accuracy: 11.236666666666666\n",
      "\n",
      "Test set: Average loss: 2.3012, Accuracy: 1135/10000 (11%)\n",
      "\n",
      "Training - Epoch: 771 loss: 2.30125177637736 accuracy: 11.236666666666666\n",
      "\n",
      "Test set: Average loss: 2.3012, Accuracy: 1135/10000 (11%)\n",
      "\n",
      "Training - Epoch: 772 loss: 2.301251769510905 accuracy: 11.236666666666666\n",
      "\n",
      "Test set: Average loss: 2.3012, Accuracy: 1135/10000 (11%)\n",
      "\n",
      "Training - Epoch: 773 loss: 2.3012517356872557 accuracy: 11.236666666666666\n",
      "\n",
      "Test set: Average loss: 2.3012, Accuracy: 1135/10000 (11%)\n",
      "\n",
      "Training - Epoch: 774 loss: 2.301251771036784 accuracy: 11.236666666666666\n",
      "\n",
      "Test set: Average loss: 2.3012, Accuracy: 1135/10000 (11%)\n",
      "\n",
      "Training - Epoch: 775 loss: 2.301251770782471 accuracy: 11.236666666666666\n",
      "\n",
      "Test set: Average loss: 2.3012, Accuracy: 1135/10000 (11%)\n",
      "\n",
      "Training - Epoch: 776 loss: 2.3012517669677734 accuracy: 11.236666666666666\n",
      "\n",
      "Test set: Average loss: 2.3012, Accuracy: 1135/10000 (11%)\n",
      "\n",
      "Training - Epoch: 777 loss: 2.3012517486572266 accuracy: 11.236666666666666\n",
      "\n",
      "Test set: Average loss: 2.3012, Accuracy: 1135/10000 (11%)\n",
      "\n",
      "Training - Epoch: 778 loss: 2.3012517705281574 accuracy: 11.236666666666666\n",
      "\n",
      "Test set: Average loss: 2.3012, Accuracy: 1135/10000 (11%)\n",
      "\n",
      "Training - Epoch: 779 loss: 2.3012517636617025 accuracy: 11.236666666666666\n",
      "\n",
      "Test set: Average loss: 2.3012, Accuracy: 1135/10000 (11%)\n",
      "\n",
      "Training - Epoch: 780 loss: 2.3012517145792644 accuracy: 11.236666666666666\n",
      "\n",
      "Test set: Average loss: 2.3012, Accuracy: 1135/10000 (11%)\n",
      "\n",
      "Training - Epoch: 781 loss: 2.3012517702738444 accuracy: 11.236666666666666\n",
      "\n",
      "Test set: Average loss: 2.3012, Accuracy: 1135/10000 (11%)\n",
      "\n",
      "Training - Epoch: 782 loss: 2.3012517674764 accuracy: 11.236666666666666\n",
      "\n",
      "Test set: Average loss: 2.3012, Accuracy: 1135/10000 (11%)\n",
      "\n",
      "Training - Epoch: 783 loss: 2.301251793162028 accuracy: 11.236666666666666\n",
      "\n",
      "Test set: Average loss: 2.3012, Accuracy: 1135/10000 (11%)\n",
      "\n",
      "Training - Epoch: 784 loss: 2.301251764424642 accuracy: 11.236666666666666\n",
      "\n",
      "Test set: Average loss: 2.3012, Accuracy: 1135/10000 (11%)\n",
      "\n",
      "Training - Epoch: 785 loss: 2.3012517438252766 accuracy: 11.236666666666666\n",
      "\n",
      "Test set: Average loss: 2.3012, Accuracy: 1135/10000 (11%)\n",
      "\n",
      "Training - Epoch: 786 loss: 2.30125180384318 accuracy: 11.236666666666666\n",
      "\n",
      "Test set: Average loss: 2.3012, Accuracy: 1135/10000 (11%)\n",
      "\n",
      "Training - Epoch: 787 loss: 2.3012517649332684 accuracy: 11.236666666666666\n",
      "\n",
      "Test set: Average loss: 2.3012, Accuracy: 1135/10000 (11%)\n",
      "\n",
      "Training - Epoch: 788 loss: 2.3012517649332684 accuracy: 11.236666666666666\n",
      "\n",
      "Test set: Average loss: 2.3012, Accuracy: 1135/10000 (11%)\n",
      "\n",
      "Training - Epoch: 789 loss: 2.301251751454671 accuracy: 11.236666666666666\n",
      "\n",
      "Test set: Average loss: 2.3012, Accuracy: 1135/10000 (11%)\n",
      "\n",
      "Training - Epoch: 790 loss: 2.3012517705281574 accuracy: 11.236666666666666\n",
      "\n",
      "Test set: Average loss: 2.3012, Accuracy: 1135/10000 (11%)\n",
      "\n",
      "Training - Epoch: 791 loss: 2.3012517300923667 accuracy: 11.236666666666666\n",
      "\n",
      "Test set: Average loss: 2.3012, Accuracy: 1135/10000 (11%)\n",
      "\n",
      "Training - Epoch: 792 loss: 2.3012517422993977 accuracy: 11.236666666666666\n",
      "\n",
      "Test set: Average loss: 2.3012, Accuracy: 1135/10000 (11%)\n",
      "\n",
      "Training - Epoch: 793 loss: 2.3012517468770346 accuracy: 11.236666666666666\n",
      "\n",
      "Test set: Average loss: 2.3012, Accuracy: 1135/10000 (11%)\n",
      "\n",
      "Training - Epoch: 794 loss: 2.301251743062337 accuracy: 11.236666666666666\n",
      "\n",
      "Test set: Average loss: 2.3012, Accuracy: 1135/10000 (11%)\n",
      "\n",
      "Training - Epoch: 795 loss: 2.301251744333903 accuracy: 11.236666666666666\n",
      "\n",
      "Test set: Average loss: 2.3012, Accuracy: 1135/10000 (11%)\n",
      "\n",
      "Training - Epoch: 796 loss: 2.301251744588216 accuracy: 11.236666666666666\n",
      "\n",
      "Test set: Average loss: 2.3012, Accuracy: 1135/10000 (11%)\n",
      "\n",
      "Training - Epoch: 797 loss: 2.3012517908732097 accuracy: 11.236666666666666\n",
      "\n",
      "Test set: Average loss: 2.3012, Accuracy: 1135/10000 (11%)\n",
      "\n",
      "Training - Epoch: 798 loss: 2.3012517519632976 accuracy: 11.236666666666666\n",
      "\n",
      "Test set: Average loss: 2.3012, Accuracy: 1135/10000 (11%)\n",
      "\n",
      "Training - Epoch: 799 loss: 2.3012517433166506 accuracy: 11.236666666666666\n",
      "\n",
      "Test set: Average loss: 2.3012, Accuracy: 1135/10000 (11%)\n",
      "\n",
      "Training - Epoch: 800 loss: 2.301251765950521 accuracy: 11.236666666666666\n",
      "\n",
      "Test set: Average loss: 2.3012, Accuracy: 1135/10000 (11%)\n",
      "\n",
      "Training - Epoch: 801 loss: 2.3012517163594564 accuracy: 11.236666666666666\n",
      "\n",
      "Test set: Average loss: 2.3012, Accuracy: 1135/10000 (11%)\n",
      "\n",
      "Training - Epoch: 802 loss: 2.3012517112731934 accuracy: 11.236666666666666\n",
      "\n",
      "Test set: Average loss: 2.3012, Accuracy: 1135/10000 (11%)\n",
      "\n",
      "Training - Epoch: 803 loss: 2.3012517717997234 accuracy: 11.236666666666666\n",
      "\n",
      "Test set: Average loss: 2.3012, Accuracy: 1135/10000 (11%)\n",
      "\n",
      "Training - Epoch: 804 loss: 2.3012517211914063 accuracy: 11.236666666666666\n",
      "\n",
      "Test set: Average loss: 2.3012, Accuracy: 1135/10000 (11%)\n",
      "\n",
      "Training - Epoch: 805 loss: 2.3012517738342284 accuracy: 11.236666666666666\n",
      "\n",
      "Test set: Average loss: 2.3012, Accuracy: 1135/10000 (11%)\n",
      "\n",
      "Training - Epoch: 806 loss: 2.301251784769694 accuracy: 11.236666666666666\n",
      "\n",
      "Test set: Average loss: 2.3012, Accuracy: 1135/10000 (11%)\n",
      "\n",
      "Training - Epoch: 807 loss: 2.3012517468770346 accuracy: 11.236666666666666\n",
      "\n",
      "Test set: Average loss: 2.3012, Accuracy: 1135/10000 (11%)\n",
      "\n",
      "Training - Epoch: 808 loss: 2.3012517717997234 accuracy: 11.236666666666666\n",
      "\n",
      "Test set: Average loss: 2.3012, Accuracy: 1135/10000 (11%)\n",
      "\n",
      "Training - Epoch: 809 loss: 2.3012517283121743 accuracy: 11.236666666666666\n",
      "\n",
      "Test set: Average loss: 2.3012, Accuracy: 1135/10000 (11%)\n",
      "\n",
      "Training - Epoch: 810 loss: 2.3012517519632976 accuracy: 11.236666666666666\n",
      "\n",
      "Test set: Average loss: 2.3012, Accuracy: 1135/10000 (11%)\n",
      "\n",
      "Training - Epoch: 811 loss: 2.301251782989502 accuracy: 11.236666666666666\n",
      "\n",
      "Test set: Average loss: 2.3012, Accuracy: 1135/10000 (11%)\n",
      "\n",
      "Training - Epoch: 812 loss: 2.3012518043518067 accuracy: 11.236666666666666\n",
      "\n",
      "Test set: Average loss: 2.3012, Accuracy: 1135/10000 (11%)\n",
      "\n",
      "Training - Epoch: 813 loss: 2.3012517578125 accuracy: 11.236666666666666\n",
      "\n",
      "Test set: Average loss: 2.3012, Accuracy: 1135/10000 (11%)\n",
      "\n",
      "Training - Epoch: 814 loss: 2.301251751454671 accuracy: 11.236666666666666\n",
      "\n",
      "Test set: Average loss: 2.3012, Accuracy: 1135/10000 (11%)\n",
      "\n",
      "Training - Epoch: 815 loss: 2.3012517702738444 accuracy: 11.236666666666666\n",
      "\n",
      "Test set: Average loss: 2.3012, Accuracy: 1135/10000 (11%)\n",
      "\n",
      "Training - Epoch: 816 loss: 2.3012518040974936 accuracy: 11.236666666666666\n",
      "\n",
      "Test set: Average loss: 2.3012, Accuracy: 1135/10000 (11%)\n",
      "\n",
      "Training - Epoch: 817 loss: 2.3012517400105796 accuracy: 11.236666666666666\n",
      "\n",
      "Test set: Average loss: 2.3012, Accuracy: 1135/10000 (11%)\n",
      "\n",
      "Training - Epoch: 818 loss: 2.3012517517089845 accuracy: 11.236666666666666\n",
      "\n",
      "Test set: Average loss: 2.3012, Accuracy: 1135/10000 (11%)\n",
      "\n",
      "Training - Epoch: 819 loss: 2.3012517649332684 accuracy: 11.236666666666666\n",
      "\n",
      "Test set: Average loss: 2.3012, Accuracy: 1135/10000 (11%)\n",
      "\n",
      "Training - Epoch: 820 loss: 2.3012517402648927 accuracy: 11.236666666666666\n",
      "\n",
      "Test set: Average loss: 2.3012, Accuracy: 1135/10000 (11%)\n",
      "\n",
      "Training - Epoch: 821 loss: 2.301251779429118 accuracy: 11.236666666666666\n",
      "\n",
      "Test set: Average loss: 2.3012, Accuracy: 1135/10000 (11%)\n",
      "\n",
      "Training - Epoch: 822 loss: 2.301251762898763 accuracy: 11.236666666666666\n",
      "\n",
      "Test set: Average loss: 2.3012, Accuracy: 1135/10000 (11%)\n",
      "\n",
      "Training - Epoch: 823 loss: 2.301251746114095 accuracy: 11.236666666666666\n",
      "\n",
      "Test set: Average loss: 2.3012, Accuracy: 1135/10000 (11%)\n",
      "\n",
      "Training - Epoch: 824 loss: 2.301251765950521 accuracy: 11.236666666666666\n",
      "\n",
      "Test set: Average loss: 2.3012, Accuracy: 1135/10000 (11%)\n",
      "\n",
      "Training - Epoch: 825 loss: 2.3012517590840655 accuracy: 11.236666666666666\n",
      "\n",
      "Test set: Average loss: 2.3012, Accuracy: 1135/10000 (11%)\n",
      "\n",
      "Training - Epoch: 826 loss: 2.3012517588297525 accuracy: 11.236666666666666\n",
      "\n",
      "Test set: Average loss: 2.3012, Accuracy: 1135/10000 (11%)\n",
      "\n",
      "Training - Epoch: 827 loss: 2.301251760864258 accuracy: 11.236666666666666\n",
      "\n",
      "Test set: Average loss: 2.3012, Accuracy: 1135/10000 (11%)\n",
      "\n",
      "Training - Epoch: 828 loss: 2.3012517850240073 accuracy: 11.236666666666666\n",
      "\n",
      "Test set: Average loss: 2.3012, Accuracy: 1135/10000 (11%)\n",
      "\n",
      "Training - Epoch: 829 loss: 2.3012517822265623 accuracy: 11.236666666666666\n",
      "\n",
      "Test set: Average loss: 2.3012, Accuracy: 1135/10000 (11%)\n",
      "\n",
      "Training - Epoch: 830 loss: 2.301251756286621 accuracy: 11.236666666666666\n",
      "\n",
      "Test set: Average loss: 2.3012, Accuracy: 1135/10000 (11%)\n",
      "\n",
      "Training - Epoch: 831 loss: 2.3012517539978026 accuracy: 11.236666666666666\n",
      "\n",
      "Test set: Average loss: 2.3012, Accuracy: 1135/10000 (11%)\n",
      "\n",
      "Training - Epoch: 832 loss: 2.3012517550150555 accuracy: 11.236666666666666\n",
      "\n",
      "Test set: Average loss: 2.3012, Accuracy: 1135/10000 (11%)\n",
      "\n",
      "Training - Epoch: 833 loss: 2.301251746114095 accuracy: 11.236666666666666\n",
      "\n",
      "Test set: Average loss: 2.3012, Accuracy: 1135/10000 (11%)\n",
      "\n",
      "Training - Epoch: 834 loss: 2.301251725769043 accuracy: 11.236666666666666\n",
      "\n",
      "Test set: Average loss: 2.3012, Accuracy: 1135/10000 (11%)\n",
      "\n",
      "Training - Epoch: 835 loss: 2.3012517977396647 accuracy: 11.236666666666666\n",
      "\n",
      "Test set: Average loss: 2.3012, Accuracy: 1135/10000 (11%)\n",
      "\n",
      "Training - Epoch: 836 loss: 2.301251754252116 accuracy: 11.236666666666666\n",
      "\n",
      "Test set: Average loss: 2.3012, Accuracy: 1135/10000 (11%)\n",
      "\n",
      "Training - Epoch: 837 loss: 2.3012517555236816 accuracy: 11.236666666666666\n",
      "\n",
      "Test set: Average loss: 2.3012, Accuracy: 1135/10000 (11%)\n",
      "\n",
      "Training - Epoch: 838 loss: 2.3012517163594564 accuracy: 11.236666666666666\n",
      "\n",
      "Test set: Average loss: 2.3012, Accuracy: 1135/10000 (11%)\n",
      "\n",
      "Training - Epoch: 839 loss: 2.301251749420166 accuracy: 11.236666666666666\n",
      "\n",
      "Test set: Average loss: 2.3012, Accuracy: 1135/10000 (11%)\n",
      "\n",
      "Training - Epoch: 840 loss: 2.301251745859782 accuracy: 11.236666666666666\n",
      "\n",
      "Test set: Average loss: 2.3012, Accuracy: 1135/10000 (11%)\n",
      "\n",
      "Training - Epoch: 841 loss: 2.3012517690022785 accuracy: 11.236666666666666\n",
      "\n",
      "Test set: Average loss: 2.3012, Accuracy: 1135/10000 (11%)\n",
      "\n",
      "Training - Epoch: 842 loss: 2.3012517450968426 accuracy: 11.236666666666666\n",
      "\n",
      "Test set: Average loss: 2.3012, Accuracy: 1135/10000 (11%)\n",
      "\n",
      "Training - Epoch: 843 loss: 2.3012517834981283 accuracy: 11.236666666666666\n",
      "\n",
      "Test set: Average loss: 2.3012, Accuracy: 1135/10000 (11%)\n",
      "\n",
      "Training - Epoch: 844 loss: 2.3012517519632976 accuracy: 11.236666666666666\n",
      "\n",
      "Test set: Average loss: 2.3012, Accuracy: 1135/10000 (11%)\n",
      "\n",
      "Training - Epoch: 845 loss: 2.3012517768859864 accuracy: 11.236666666666666\n",
      "\n",
      "Test set: Average loss: 2.3012, Accuracy: 1135/10000 (11%)\n",
      "\n",
      "Training - Epoch: 846 loss: 2.301251776123047 accuracy: 11.236666666666666\n",
      "\n",
      "Test set: Average loss: 2.3012, Accuracy: 1135/10000 (11%)\n",
      "\n",
      "Training - Epoch: 847 loss: 2.301251759847005 accuracy: 11.236666666666666\n",
      "\n",
      "Test set: Average loss: 2.3012, Accuracy: 1135/10000 (11%)\n",
      "\n",
      "Training - Epoch: 848 loss: 2.3012517784118653 accuracy: 11.236666666666666\n",
      "\n",
      "Test set: Average loss: 2.3012, Accuracy: 1135/10000 (11%)\n",
      "\n",
      "Training - Epoch: 849 loss: 2.301251762898763 accuracy: 11.236666666666666\n",
      "\n",
      "Test set: Average loss: 2.3012, Accuracy: 1135/10000 (11%)\n",
      "\n",
      "Training - Epoch: 850 loss: 2.3012517623901365 accuracy: 11.236666666666666\n",
      "\n",
      "Test set: Average loss: 2.3012, Accuracy: 1135/10000 (11%)\n",
      "\n",
      "Training - Epoch: 851 loss: 2.3012517555236816 accuracy: 11.236666666666666\n",
      "\n",
      "Test set: Average loss: 2.3012, Accuracy: 1135/10000 (11%)\n",
      "\n",
      "Training - Epoch: 852 loss: 2.3012517784118653 accuracy: 11.236666666666666\n",
      "\n",
      "Test set: Average loss: 2.3012, Accuracy: 1135/10000 (11%)\n",
      "\n",
      "Training - Epoch: 853 loss: 2.3012517979939777 accuracy: 11.236666666666666\n",
      "\n",
      "Test set: Average loss: 2.3012, Accuracy: 1135/10000 (11%)\n",
      "\n",
      "Training - Epoch: 854 loss: 2.3012517656962075 accuracy: 11.236666666666666\n",
      "\n",
      "Test set: Average loss: 2.3012, Accuracy: 1135/10000 (11%)\n",
      "\n",
      "Training - Epoch: 855 loss: 2.301251749420166 accuracy: 11.236666666666666\n",
      "\n",
      "Test set: Average loss: 2.3012, Accuracy: 1135/10000 (11%)\n",
      "\n",
      "Training - Epoch: 856 loss: 2.301251752726237 accuracy: 11.236666666666666\n",
      "\n",
      "Test set: Average loss: 2.3012, Accuracy: 1135/10000 (11%)\n",
      "\n",
      "Training - Epoch: 857 loss: 2.3012517453511556 accuracy: 11.236666666666666\n",
      "\n",
      "Test set: Average loss: 2.3012, Accuracy: 1135/10000 (11%)\n",
      "\n",
      "Training - Epoch: 858 loss: 2.3012517768859864 accuracy: 11.236666666666666\n",
      "\n",
      "Test set: Average loss: 2.3012, Accuracy: 1135/10000 (11%)\n",
      "\n",
      "Training - Epoch: 859 loss: 2.3012517979939777 accuracy: 11.236666666666666\n",
      "\n",
      "Test set: Average loss: 2.3012, Accuracy: 1135/10000 (11%)\n",
      "\n",
      "Training - Epoch: 860 loss: 2.301251732889811 accuracy: 11.236666666666666\n",
      "\n",
      "Test set: Average loss: 2.3012, Accuracy: 1135/10000 (11%)\n",
      "\n",
      "Training - Epoch: 861 loss: 2.3012517923990887 accuracy: 11.236666666666666\n",
      "\n",
      "Test set: Average loss: 2.3012, Accuracy: 1135/10000 (11%)\n",
      "\n",
      "Training - Epoch: 862 loss: 2.3012517784118653 accuracy: 11.236666666666666\n",
      "\n",
      "Test set: Average loss: 2.3012, Accuracy: 1135/10000 (11%)\n",
      "\n",
      "Training - Epoch: 863 loss: 2.3012517534891765 accuracy: 11.236666666666666\n",
      "\n",
      "Test set: Average loss: 2.3012, Accuracy: 1135/10000 (11%)\n",
      "\n",
      "Training - Epoch: 864 loss: 2.3012517738342284 accuracy: 11.236666666666666\n",
      "\n",
      "Test set: Average loss: 2.3012, Accuracy: 1135/10000 (11%)\n",
      "\n",
      "Training - Epoch: 865 loss: 2.3012517957051597 accuracy: 11.236666666666666\n",
      "\n",
      "Test set: Average loss: 2.3012, Accuracy: 1135/10000 (11%)\n",
      "\n",
      "Training - Epoch: 866 loss: 2.301251732889811 accuracy: 11.236666666666666\n",
      "\n",
      "Test set: Average loss: 2.3012, Accuracy: 1135/10000 (11%)\n",
      "\n",
      "Training - Epoch: 867 loss: 2.3012517840067543 accuracy: 11.236666666666666\n",
      "\n",
      "Test set: Average loss: 2.3012, Accuracy: 1135/10000 (11%)\n",
      "\n",
      "Training - Epoch: 868 loss: 2.3012517341613767 accuracy: 11.236666666666666\n",
      "\n",
      "Test set: Average loss: 2.3012, Accuracy: 1135/10000 (11%)\n",
      "\n",
      "Training - Epoch: 869 loss: 2.3012517687479654 accuracy: 11.236666666666666\n",
      "\n",
      "Test set: Average loss: 2.3012, Accuracy: 1135/10000 (11%)\n",
      "\n",
      "Training - Epoch: 870 loss: 2.3012517313639322 accuracy: 11.236666666666666\n",
      "\n",
      "Test set: Average loss: 2.3012, Accuracy: 1135/10000 (11%)\n",
      "\n",
      "Training - Epoch: 871 loss: 2.3012517621358235 accuracy: 11.236666666666666\n",
      "\n",
      "Test set: Average loss: 2.3012, Accuracy: 1135/10000 (11%)\n",
      "\n",
      "Training - Epoch: 872 loss: 2.3012517832438153 accuracy: 11.236666666666666\n",
      "\n",
      "Test set: Average loss: 2.3012, Accuracy: 1135/10000 (11%)\n",
      "\n",
      "Training - Epoch: 873 loss: 2.3012517636617025 accuracy: 11.236666666666666\n",
      "\n",
      "Test set: Average loss: 2.3012, Accuracy: 1135/10000 (11%)\n",
      "\n",
      "Training - Epoch: 874 loss: 2.301251776123047 accuracy: 11.236666666666666\n",
      "\n",
      "Test set: Average loss: 2.3012, Accuracy: 1135/10000 (11%)\n",
      "\n",
      "Training - Epoch: 875 loss: 2.301251749165853 accuracy: 11.236666666666666\n",
      "\n",
      "Test set: Average loss: 2.3012, Accuracy: 1135/10000 (11%)\n",
      "\n",
      "Training - Epoch: 876 loss: 2.3012517583211265 accuracy: 11.236666666666666\n",
      "\n",
      "Test set: Average loss: 2.3012, Accuracy: 1135/10000 (11%)\n",
      "\n",
      "Training - Epoch: 877 loss: 2.301251779174805 accuracy: 11.236666666666666\n",
      "\n",
      "Test set: Average loss: 2.3012, Accuracy: 1135/10000 (11%)\n",
      "\n",
      "Training - Epoch: 878 loss: 2.3012517405192057 accuracy: 11.236666666666666\n",
      "\n",
      "Test set: Average loss: 2.3012, Accuracy: 1135/10000 (11%)\n",
      "\n",
      "Training - Epoch: 879 loss: 2.301251777394613 accuracy: 11.236666666666666\n",
      "\n",
      "Test set: Average loss: 2.3012, Accuracy: 1135/10000 (11%)\n",
      "\n",
      "Training - Epoch: 880 loss: 2.301251757558187 accuracy: 11.236666666666666\n",
      "\n",
      "Test set: Average loss: 2.3012, Accuracy: 1135/10000 (11%)\n",
      "\n",
      "Training - Epoch: 881 loss: 2.301251755777995 accuracy: 11.236666666666666\n",
      "\n",
      "Test set: Average loss: 2.3012, Accuracy: 1135/10000 (11%)\n",
      "\n",
      "Training - Epoch: 882 loss: 2.301251772562663 accuracy: 11.236666666666666\n",
      "\n",
      "Test set: Average loss: 2.3012, Accuracy: 1135/10000 (11%)\n",
      "\n",
      "Training - Epoch: 883 loss: 2.301251787821452 accuracy: 11.236666666666666\n",
      "\n",
      "Test set: Average loss: 2.3012, Accuracy: 1135/10000 (11%)\n",
      "\n",
      "Training - Epoch: 884 loss: 2.3012517552693685 accuracy: 11.236666666666666\n",
      "\n",
      "Test set: Average loss: 2.3012, Accuracy: 1135/10000 (11%)\n",
      "\n",
      "Training - Epoch: 885 loss: 2.301251781463623 accuracy: 11.236666666666666\n",
      "\n",
      "Test set: Average loss: 2.3012, Accuracy: 1135/10000 (11%)\n",
      "\n",
      "Training - Epoch: 886 loss: 2.3012517275492352 accuracy: 11.236666666666666\n",
      "\n",
      "Test set: Average loss: 2.3012, Accuracy: 1135/10000 (11%)\n",
      "\n",
      "Training - Epoch: 887 loss: 2.301251758066813 accuracy: 11.236666666666666\n",
      "\n",
      "Test set: Average loss: 2.3012, Accuracy: 1135/10000 (11%)\n",
      "\n",
      "Training - Epoch: 888 loss: 2.3012517389933267 accuracy: 11.236666666666666\n",
      "\n",
      "Test set: Average loss: 2.3012, Accuracy: 1135/10000 (11%)\n",
      "\n",
      "Training - Epoch: 889 loss: 2.3012517049153645 accuracy: 11.236666666666666\n",
      "\n",
      "Test set: Average loss: 2.3012, Accuracy: 1135/10000 (11%)\n",
      "\n",
      "Training - Epoch: 890 loss: 2.3012517046610514 accuracy: 11.236666666666666\n",
      "\n",
      "Test set: Average loss: 2.3012, Accuracy: 1135/10000 (11%)\n",
      "\n",
      "Training - Epoch: 891 loss: 2.301251761372884 accuracy: 11.236666666666666\n",
      "\n",
      "Test set: Average loss: 2.3012, Accuracy: 1135/10000 (11%)\n",
      "\n",
      "Training - Epoch: 892 loss: 2.301251774851481 accuracy: 11.236666666666666\n",
      "\n",
      "Test set: Average loss: 2.3012, Accuracy: 1135/10000 (11%)\n",
      "\n",
      "Training - Epoch: 893 loss: 2.301251732889811 accuracy: 11.236666666666666\n",
      "\n",
      "Test set: Average loss: 2.3012, Accuracy: 1135/10000 (11%)\n",
      "\n",
      "Training - Epoch: 894 loss: 2.3012517435709636 accuracy: 11.236666666666666\n",
      "\n",
      "Test set: Average loss: 2.3012, Accuracy: 1135/10000 (11%)\n",
      "\n",
      "Training - Epoch: 895 loss: 2.3012517583211265 accuracy: 11.236666666666666\n",
      "\n",
      "Test set: Average loss: 2.3012, Accuracy: 1135/10000 (11%)\n",
      "\n",
      "Training - Epoch: 896 loss: 2.301251753234863 accuracy: 11.236666666666666\n",
      "\n",
      "Test set: Average loss: 2.3012, Accuracy: 1135/10000 (11%)\n",
      "\n",
      "Training - Epoch: 897 loss: 2.3012517840067543 accuracy: 11.236666666666666\n",
      "\n",
      "Test set: Average loss: 2.3012, Accuracy: 1135/10000 (11%)\n",
      "\n",
      "Training - Epoch: 898 loss: 2.3012517313639322 accuracy: 11.236666666666666\n",
      "\n",
      "Test set: Average loss: 2.3012, Accuracy: 1135/10000 (11%)\n",
      "\n",
      "Training - Epoch: 899 loss: 2.3012517389933267 accuracy: 11.236666666666666\n",
      "\n",
      "Test set: Average loss: 2.3012, Accuracy: 1135/10000 (11%)\n",
      "\n",
      "Training - Epoch: 900 loss: 2.3012517519632976 accuracy: 11.236666666666666\n",
      "\n",
      "Test set: Average loss: 2.3012, Accuracy: 1135/10000 (11%)\n",
      "\n",
      "Training - Epoch: 901 loss: 2.3012517550150555 accuracy: 11.236666666666666\n",
      "\n",
      "Test set: Average loss: 2.3012, Accuracy: 1135/10000 (11%)\n",
      "\n",
      "Training - Epoch: 902 loss: 2.301251747894287 accuracy: 11.236666666666666\n",
      "\n",
      "Test set: Average loss: 2.3012, Accuracy: 1135/10000 (11%)\n",
      "\n",
      "Training - Epoch: 903 loss: 2.3012517768859864 accuracy: 11.236666666666666\n",
      "\n",
      "Test set: Average loss: 2.3012, Accuracy: 1135/10000 (11%)\n",
      "\n",
      "Training - Epoch: 904 loss: 2.3012517583211265 accuracy: 11.236666666666666\n",
      "\n",
      "Test set: Average loss: 2.3012, Accuracy: 1135/10000 (11%)\n",
      "\n",
      "Training - Epoch: 905 loss: 2.3012517603556315 accuracy: 11.236666666666666\n",
      "\n",
      "Test set: Average loss: 2.3012, Accuracy: 1135/10000 (11%)\n",
      "\n",
      "Training - Epoch: 906 loss: 2.301251759847005 accuracy: 11.236666666666666\n",
      "\n",
      "Test set: Average loss: 2.3012, Accuracy: 1135/10000 (11%)\n",
      "\n",
      "Training - Epoch: 907 loss: 2.3012517784118653 accuracy: 11.236666666666666\n",
      "\n",
      "Test set: Average loss: 2.3012, Accuracy: 1135/10000 (11%)\n",
      "\n",
      "Training - Epoch: 908 loss: 2.3012517979939777 accuracy: 11.236666666666666\n",
      "\n",
      "Test set: Average loss: 2.3012, Accuracy: 1135/10000 (11%)\n",
      "\n",
      "Training - Epoch: 909 loss: 2.3012517537434896 accuracy: 11.236666666666666\n",
      "\n",
      "Test set: Average loss: 2.3012, Accuracy: 1135/10000 (11%)\n",
      "\n",
      "Training - Epoch: 910 loss: 2.3012517690022785 accuracy: 11.236666666666666\n",
      "\n",
      "Test set: Average loss: 2.3012, Accuracy: 1135/10000 (11%)\n",
      "\n",
      "Training - Epoch: 911 loss: 2.3012517771402994 accuracy: 11.236666666666666\n",
      "\n",
      "Test set: Average loss: 2.3012, Accuracy: 1135/10000 (11%)\n",
      "\n",
      "Training - Epoch: 912 loss: 2.3012517601013185 accuracy: 11.236666666666666\n",
      "\n",
      "Test set: Average loss: 2.3012, Accuracy: 1135/10000 (11%)\n",
      "\n",
      "Training - Epoch: 913 loss: 2.301251749420166 accuracy: 11.236666666666666\n",
      "\n",
      "Test set: Average loss: 2.3012, Accuracy: 1135/10000 (11%)\n",
      "\n",
      "Training - Epoch: 914 loss: 2.3012517550150555 accuracy: 11.236666666666666\n",
      "\n",
      "Test set: Average loss: 2.3012, Accuracy: 1135/10000 (11%)\n",
      "\n",
      "Training - Epoch: 915 loss: 2.301251769765218 accuracy: 11.236666666666666\n",
      "\n",
      "Test set: Average loss: 2.3012, Accuracy: 1135/10000 (11%)\n",
      "\n",
      "Training - Epoch: 916 loss: 2.3012517433166506 accuracy: 11.236666666666666\n",
      "\n",
      "Test set: Average loss: 2.3012, Accuracy: 1135/10000 (11%)\n",
      "\n",
      "Training - Epoch: 917 loss: 2.3012517702738444 accuracy: 11.236666666666666\n",
      "\n",
      "Test set: Average loss: 2.3012, Accuracy: 1135/10000 (11%)\n",
      "\n",
      "Training - Epoch: 918 loss: 2.3012517387390137 accuracy: 11.236666666666666\n",
      "\n",
      "Test set: Average loss: 2.3012, Accuracy: 1135/10000 (11%)\n",
      "\n",
      "Training - Epoch: 919 loss: 2.3012517351786297 accuracy: 11.236666666666666\n",
      "\n",
      "Test set: Average loss: 2.3012, Accuracy: 1135/10000 (11%)\n",
      "\n",
      "Training - Epoch: 920 loss: 2.3012517651875815 accuracy: 11.236666666666666\n",
      "\n",
      "Test set: Average loss: 2.3012, Accuracy: 1135/10000 (11%)\n",
      "\n",
      "Training - Epoch: 921 loss: 2.301251754760742 accuracy: 11.236666666666666\n",
      "\n",
      "Test set: Average loss: 2.3012, Accuracy: 1135/10000 (11%)\n",
      "\n",
      "Training - Epoch: 922 loss: 2.3012517667134604 accuracy: 11.236666666666666\n",
      "\n",
      "Test set: Average loss: 2.3012, Accuracy: 1135/10000 (11%)\n",
      "\n",
      "Training - Epoch: 923 loss: 2.301251757558187 accuracy: 11.236666666666666\n",
      "\n",
      "Test set: Average loss: 2.3012, Accuracy: 1135/10000 (11%)\n",
      "\n",
      "Training - Epoch: 924 loss: 2.301251816813151 accuracy: 11.236666666666666\n",
      "\n",
      "Test set: Average loss: 2.3012, Accuracy: 1135/10000 (11%)\n",
      "\n",
      "Training - Epoch: 925 loss: 2.3012517573038735 accuracy: 11.236666666666666\n",
      "\n",
      "Test set: Average loss: 2.3012, Accuracy: 1135/10000 (11%)\n",
      "\n",
      "Training - Epoch: 926 loss: 2.3012517489115396 accuracy: 11.236666666666666\n",
      "\n",
      "Test set: Average loss: 2.3012, Accuracy: 1135/10000 (11%)\n",
      "\n",
      "Training - Epoch: 927 loss: 2.3012517486572266 accuracy: 11.236666666666666\n",
      "\n",
      "Test set: Average loss: 2.3012, Accuracy: 1135/10000 (11%)\n",
      "\n",
      "Training - Epoch: 928 loss: 2.3012517318725587 accuracy: 11.236666666666666\n",
      "\n",
      "Test set: Average loss: 2.3012, Accuracy: 1135/10000 (11%)\n",
      "\n",
      "Training - Epoch: 929 loss: 2.3012517957051597 accuracy: 11.236666666666666\n",
      "\n",
      "Test set: Average loss: 2.3012, Accuracy: 1135/10000 (11%)\n",
      "\n",
      "Training - Epoch: 930 loss: 2.3012517275492352 accuracy: 11.236666666666666\n",
      "\n",
      "Test set: Average loss: 2.3012, Accuracy: 1135/10000 (11%)\n",
      "\n",
      "Training - Epoch: 931 loss: 2.3012517705281574 accuracy: 11.236666666666666\n",
      "\n",
      "Test set: Average loss: 2.3012, Accuracy: 1135/10000 (11%)\n",
      "\n",
      "Training - Epoch: 932 loss: 2.3012517417907716 accuracy: 11.236666666666666\n",
      "\n",
      "Test set: Average loss: 2.3012, Accuracy: 1135/10000 (11%)\n",
      "\n",
      "Training - Epoch: 933 loss: 2.3012517603556315 accuracy: 11.236666666666666\n",
      "\n",
      "Test set: Average loss: 2.3012, Accuracy: 1135/10000 (11%)\n",
      "\n",
      "Training - Epoch: 934 loss: 2.301251746114095 accuracy: 11.236666666666666\n",
      "\n",
      "Test set: Average loss: 2.3012, Accuracy: 1135/10000 (11%)\n",
      "\n",
      "Training - Epoch: 935 loss: 2.301251774851481 accuracy: 11.236666666666666\n",
      "\n",
      "Test set: Average loss: 2.3012, Accuracy: 1135/10000 (11%)\n",
      "\n",
      "Training - Epoch: 936 loss: 2.3012517247517903 accuracy: 11.236666666666666\n",
      "\n",
      "Test set: Average loss: 2.3012, Accuracy: 1135/10000 (11%)\n",
      "\n",
      "Training - Epoch: 937 loss: 2.301251759847005 accuracy: 11.236666666666666\n",
      "\n",
      "Test set: Average loss: 2.3012, Accuracy: 1135/10000 (11%)\n",
      "\n",
      "Training - Epoch: 938 loss: 2.3012517918904623 accuracy: 11.236666666666666\n",
      "\n",
      "Test set: Average loss: 2.3012, Accuracy: 1135/10000 (11%)\n",
      "\n",
      "Training - Epoch: 939 loss: 2.3012517651875815 accuracy: 11.236666666666666\n",
      "\n",
      "Test set: Average loss: 2.3012, Accuracy: 1135/10000 (11%)\n",
      "\n",
      "Training - Epoch: 940 loss: 2.30125177154541 accuracy: 11.236666666666666\n",
      "\n",
      "Test set: Average loss: 2.3012, Accuracy: 1135/10000 (11%)\n",
      "\n",
      "Training - Epoch: 941 loss: 2.3012517331441242 accuracy: 11.236666666666666\n",
      "\n",
      "Test set: Average loss: 2.3012, Accuracy: 1135/10000 (11%)\n",
      "\n",
      "Training - Epoch: 942 loss: 2.3012517588297525 accuracy: 11.236666666666666\n",
      "\n",
      "Test set: Average loss: 2.3012, Accuracy: 1135/10000 (11%)\n",
      "\n",
      "Training - Epoch: 943 loss: 2.3012517504374186 accuracy: 11.236666666666666\n",
      "\n",
      "Test set: Average loss: 2.3012, Accuracy: 1135/10000 (11%)\n",
      "\n",
      "Training - Epoch: 944 loss: 2.301251767985026 accuracy: 11.236666666666666\n",
      "\n",
      "Test set: Average loss: 2.3012, Accuracy: 1135/10000 (11%)\n",
      "\n",
      "Training - Epoch: 945 loss: 2.3012517636617025 accuracy: 11.236666666666666\n",
      "\n",
      "Test set: Average loss: 2.3012, Accuracy: 1135/10000 (11%)\n",
      "\n",
      "Training - Epoch: 946 loss: 2.30125175298055 accuracy: 11.236666666666666\n",
      "\n",
      "Test set: Average loss: 2.3012, Accuracy: 1135/10000 (11%)\n",
      "\n",
      "Training - Epoch: 947 loss: 2.30125179494222 accuracy: 11.236666666666666\n",
      "\n",
      "Test set: Average loss: 2.3012, Accuracy: 1135/10000 (11%)\n",
      "\n",
      "Training - Epoch: 948 loss: 2.301251755777995 accuracy: 11.236666666666666\n",
      "\n",
      "Test set: Average loss: 2.3012, Accuracy: 1135/10000 (11%)\n",
      "\n",
      "Training - Epoch: 949 loss: 2.3012517789204914 accuracy: 11.236666666666666\n",
      "\n",
      "Test set: Average loss: 2.3012, Accuracy: 1135/10000 (11%)\n",
      "\n",
      "Training - Epoch: 950 loss: 2.301251758066813 accuracy: 11.236666666666666\n",
      "\n",
      "Test set: Average loss: 2.3012, Accuracy: 1135/10000 (11%)\n",
      "\n",
      "Training - Epoch: 951 loss: 2.30125177637736 accuracy: 11.236666666666666\n",
      "\n",
      "Test set: Average loss: 2.3012, Accuracy: 1135/10000 (11%)\n",
      "\n",
      "Training - Epoch: 952 loss: 2.301251745859782 accuracy: 11.236666666666666\n",
      "\n",
      "Test set: Average loss: 2.3012, Accuracy: 1135/10000 (11%)\n",
      "\n",
      "Training - Epoch: 953 loss: 2.3012517926534017 accuracy: 11.236666666666666\n",
      "\n",
      "Test set: Average loss: 2.3012, Accuracy: 1135/10000 (11%)\n",
      "\n",
      "Training - Epoch: 954 loss: 2.30125177154541 accuracy: 11.236666666666666\n",
      "\n",
      "Test set: Average loss: 2.3012, Accuracy: 1135/10000 (11%)\n",
      "\n",
      "Training - Epoch: 955 loss: 2.301251770782471 accuracy: 11.236666666666666\n",
      "\n",
      "Test set: Average loss: 2.3012, Accuracy: 1135/10000 (11%)\n",
      "\n",
      "Training - Epoch: 956 loss: 2.3012517552693685 accuracy: 11.236666666666666\n",
      "\n",
      "Test set: Average loss: 2.3012, Accuracy: 1135/10000 (11%)\n",
      "\n",
      "Training - Epoch: 957 loss: 2.3012517893473308 accuracy: 11.236666666666666\n",
      "\n",
      "Test set: Average loss: 2.3012, Accuracy: 1135/10000 (11%)\n",
      "\n",
      "Training - Epoch: 958 loss: 2.301251767985026 accuracy: 11.236666666666666\n",
      "\n",
      "Test set: Average loss: 2.3012, Accuracy: 1135/10000 (11%)\n",
      "\n",
      "Training - Epoch: 959 loss: 2.301251771036784 accuracy: 11.236666666666666\n",
      "\n",
      "Test set: Average loss: 2.3012, Accuracy: 1135/10000 (11%)\n",
      "\n",
      "Training - Epoch: 960 loss: 2.3012517486572266 accuracy: 11.236666666666666\n",
      "\n",
      "Test set: Average loss: 2.3012, Accuracy: 1135/10000 (11%)\n",
      "\n",
      "Training - Epoch: 961 loss: 2.3012517519632976 accuracy: 11.236666666666666\n",
      "\n",
      "Test set: Average loss: 2.3012, Accuracy: 1135/10000 (11%)\n",
      "\n",
      "Training - Epoch: 962 loss: 2.301251772816976 accuracy: 11.236666666666666\n",
      "\n",
      "Test set: Average loss: 2.3012, Accuracy: 1135/10000 (11%)\n",
      "\n",
      "Training - Epoch: 963 loss: 2.3012517654418945 accuracy: 11.236666666666666\n",
      "\n",
      "Test set: Average loss: 2.3012, Accuracy: 1135/10000 (11%)\n",
      "\n",
      "Training - Epoch: 964 loss: 2.301251744333903 accuracy: 11.236666666666666\n",
      "\n",
      "Test set: Average loss: 2.3012, Accuracy: 1135/10000 (11%)\n",
      "\n",
      "Training - Epoch: 965 loss: 2.3012518163045246 accuracy: 11.236666666666666\n",
      "\n",
      "Test set: Average loss: 2.3012, Accuracy: 1135/10000 (11%)\n",
      "\n",
      "Training - Epoch: 966 loss: 2.3012517522176106 accuracy: 11.236666666666666\n",
      "\n",
      "Test set: Average loss: 2.3012, Accuracy: 1135/10000 (11%)\n",
      "\n",
      "Training - Epoch: 967 loss: 2.3012517756144204 accuracy: 11.236666666666666\n",
      "\n",
      "Test set: Average loss: 2.3012, Accuracy: 1135/10000 (11%)\n",
      "\n",
      "Training - Epoch: 968 loss: 2.3012517267862957 accuracy: 11.236666666666666\n",
      "\n",
      "Test set: Average loss: 2.3012, Accuracy: 1135/10000 (11%)\n",
      "\n",
      "Training - Epoch: 969 loss: 2.3012517616271975 accuracy: 11.236666666666666\n",
      "\n",
      "Test set: Average loss: 2.3012, Accuracy: 1135/10000 (11%)\n",
      "\n",
      "Training - Epoch: 970 loss: 2.301251767730713 accuracy: 11.236666666666666\n",
      "\n",
      "Test set: Average loss: 2.3012, Accuracy: 1135/10000 (11%)\n",
      "\n",
      "Training - Epoch: 971 loss: 2.3012517283121743 accuracy: 11.236666666666666\n",
      "\n",
      "Test set: Average loss: 2.3012, Accuracy: 1135/10000 (11%)\n",
      "\n",
      "Training - Epoch: 972 loss: 2.301251752726237 accuracy: 11.236666666666666\n",
      "\n",
      "Test set: Average loss: 2.3012, Accuracy: 1135/10000 (11%)\n",
      "\n",
      "Training - Epoch: 973 loss: 2.3012517438252766 accuracy: 11.236666666666666\n",
      "\n",
      "Test set: Average loss: 2.3012, Accuracy: 1135/10000 (11%)\n",
      "\n",
      "Training - Epoch: 974 loss: 2.301251747639974 accuracy: 11.236666666666666\n",
      "\n",
      "Test set: Average loss: 2.3012, Accuracy: 1135/10000 (11%)\n",
      "\n",
      "Training - Epoch: 975 loss: 2.301251749420166 accuracy: 11.236666666666666\n",
      "\n",
      "Test set: Average loss: 2.3012, Accuracy: 1135/10000 (11%)\n",
      "\n",
      "Training - Epoch: 976 loss: 2.301251754252116 accuracy: 11.236666666666666\n",
      "\n",
      "Test set: Average loss: 2.3012, Accuracy: 1135/10000 (11%)\n",
      "\n",
      "Training - Epoch: 977 loss: 2.3012517639160155 accuracy: 11.236666666666666\n",
      "\n",
      "Test set: Average loss: 2.3012, Accuracy: 1135/10000 (11%)\n",
      "\n",
      "Training - Epoch: 978 loss: 2.3012517934163412 accuracy: 11.236666666666666\n",
      "\n",
      "Test set: Average loss: 2.3012, Accuracy: 1135/10000 (11%)\n",
      "\n",
      "Training - Epoch: 979 loss: 2.3012517618815105 accuracy: 11.236666666666666\n",
      "\n",
      "Test set: Average loss: 2.3012, Accuracy: 1135/10000 (11%)\n",
      "\n",
      "Training - Epoch: 980 loss: 2.301251761118571 accuracy: 11.236666666666666\n",
      "\n",
      "Test set: Average loss: 2.3012, Accuracy: 1135/10000 (11%)\n",
      "\n",
      "Training - Epoch: 981 loss: 2.3012517440795897 accuracy: 11.236666666666666\n",
      "\n",
      "Test set: Average loss: 2.3012, Accuracy: 1135/10000 (11%)\n",
      "\n",
      "Training - Epoch: 982 loss: 2.3012517583211265 accuracy: 11.236666666666666\n",
      "\n",
      "Test set: Average loss: 2.3012, Accuracy: 1135/10000 (11%)\n",
      "\n",
      "Training - Epoch: 983 loss: 2.3012517501831056 accuracy: 11.236666666666666\n",
      "\n",
      "Test set: Average loss: 2.3012, Accuracy: 1135/10000 (11%)\n",
      "\n",
      "Training - Epoch: 984 loss: 2.3012517440795897 accuracy: 11.236666666666666\n",
      "\n",
      "Test set: Average loss: 2.3012, Accuracy: 1135/10000 (11%)\n",
      "\n",
      "Training - Epoch: 985 loss: 2.301251764424642 accuracy: 11.236666666666666\n",
      "\n",
      "Test set: Average loss: 2.3012, Accuracy: 1135/10000 (11%)\n",
      "\n",
      "Training - Epoch: 986 loss: 2.3012517573038735 accuracy: 11.236666666666666\n",
      "\n",
      "Test set: Average loss: 2.3012, Accuracy: 1135/10000 (11%)\n",
      "\n",
      "Training - Epoch: 987 loss: 2.3012517908732097 accuracy: 11.236666666666666\n",
      "\n",
      "Test set: Average loss: 2.3012, Accuracy: 1135/10000 (11%)\n",
      "\n",
      "Training - Epoch: 988 loss: 2.301251787821452 accuracy: 11.236666666666666\n",
      "\n",
      "Test set: Average loss: 2.3012, Accuracy: 1135/10000 (11%)\n",
      "\n",
      "Training - Epoch: 989 loss: 2.3012517433166506 accuracy: 11.236666666666666\n",
      "\n",
      "Test set: Average loss: 2.3012, Accuracy: 1135/10000 (11%)\n",
      "\n",
      "Training - Epoch: 990 loss: 2.3012517570495605 accuracy: 11.236666666666666\n",
      "\n",
      "Test set: Average loss: 2.3012, Accuracy: 1135/10000 (11%)\n",
      "\n",
      "Training - Epoch: 991 loss: 2.3012517911275228 accuracy: 11.236666666666666\n",
      "\n",
      "Test set: Average loss: 2.3012, Accuracy: 1135/10000 (11%)\n",
      "\n",
      "Training - Epoch: 992 loss: 2.3012517870585123 accuracy: 11.236666666666666\n",
      "\n",
      "Test set: Average loss: 2.3012, Accuracy: 1135/10000 (11%)\n",
      "\n",
      "Training - Epoch: 993 loss: 2.3012517489115396 accuracy: 11.236666666666666\n",
      "\n",
      "Test set: Average loss: 2.3012, Accuracy: 1135/10000 (11%)\n",
      "\n",
      "Training - Epoch: 994 loss: 2.3012517481486 accuracy: 11.236666666666666\n",
      "\n",
      "Test set: Average loss: 2.3012, Accuracy: 1135/10000 (11%)\n",
      "\n",
      "Training - Epoch: 995 loss: 2.301251739501953 accuracy: 11.236666666666666\n",
      "\n",
      "Test set: Average loss: 2.3012, Accuracy: 1135/10000 (11%)\n",
      "\n",
      "Training - Epoch: 996 loss: 2.3012517717997234 accuracy: 11.236666666666666\n",
      "\n",
      "Test set: Average loss: 2.3012, Accuracy: 1135/10000 (11%)\n",
      "\n",
      "Training - Epoch: 997 loss: 2.3012517941792807 accuracy: 11.236666666666666\n",
      "\n",
      "Test set: Average loss: 2.3012, Accuracy: 1135/10000 (11%)\n",
      "\n",
      "Training - Epoch: 998 loss: 2.3012517840067543 accuracy: 11.236666666666666\n",
      "\n",
      "Test set: Average loss: 2.3012, Accuracy: 1135/10000 (11%)\n",
      "\n",
      "Training - Epoch: 999 loss: 2.301251746368408 accuracy: 11.236666666666666\n",
      "\n",
      "Test set: Average loss: 2.3012, Accuracy: 1135/10000 (11%)\n",
      "\n",
      "Training - Epoch: 1000 loss: 2.301251800537109 accuracy: 11.236666666666666\n",
      "\n",
      "Test set: Average loss: 2.3012, Accuracy: 1135/10000 (11%)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "epoch=1000\n",
    "model = CnnNet().to(device)\n",
    "optimizer = optim.Adadelta(model.parameters(), lr=1)\n",
    "scheduler = StepLR(optimizer, step_size=1, gamma=0.1)\n",
    "loss = nn.CrossEntropyLoss()\n",
    "train_loss_1, accuracy_train_1, test_loss_1, test_accuracy_1= returnModelAccAndLoss(model, device, train_loader_shuffle, test_loader, optimizer, loss, epoch, scheduler)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "7ddc1177",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x432 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt1=plt.figure(figsize=(10, 6))\n",
    "plt.plot(np.array(train_loss_1), 'r', label='training')\n",
    "plt.plot(np.array(test_loss_1), 'b', label='test')\n",
    "\n",
    "# naming the x axis\n",
    "plt.xlabel('Epochs')\n",
    "# naming the y axis\n",
    "plt.ylabel('Loss') \n",
    "\n",
    "plt.legend()\n",
    "plt.title(\"Loss function For MNIST\")\n",
    "plt.show()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24b64187",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d352c5e-eac9-4678-aa2e-6793157cbdf0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
