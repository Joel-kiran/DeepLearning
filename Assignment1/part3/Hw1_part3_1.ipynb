{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "54f45fad",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import argparse\n",
    "import torch\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "from torchsummary import summary\n",
    "device = torch.device(\"cuda\" if use_cuda else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "b4e15736-9bd4-4f59-8d05-88a773020c57",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CnnNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CnnNet, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 16, 3, 1)\n",
    "        self.fc1 = nn.Linear(2704, 128)\n",
    "        self.fc2 = nn.Linear(128, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = F.relu(x)\n",
    "        x = F.max_pool2d(x, 2)\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = self.fc1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        output = F.log_softmax(x, dim=1)\n",
    "        return output\n",
    "    \n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d069aa5c-3de6-44b5-8308-75ecada1a5ab",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "efacf574",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train( model, device, train_loader, optimizer,loss_fn, epoch):\n",
    "    train_loss=0\n",
    "    model.train()\n",
    "    correct = 0\n",
    "    loss_fn = nn.CrossEntropyLoss(reduction='sum')\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        loss = loss_fn(output, target)\n",
    "        train_loss+=loss.item()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        pred = output.argmax(dim=1, keepdim=True)  # get the index of the max log-probability\n",
    "        correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "    train_loss /= len(train_loader.dataset)\n",
    "    print(\"Training - Epoch:\", epoch, \"loss:\", train_loss, \"accuracy:\", 100. * correct / len(train_loader.dataset))  \n",
    "    return train_loss, 100. * correct / len(train_loader.dataset)\n",
    "\n",
    "def test(model, device, test_loader):\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    loss_fn = nn.CrossEntropyLoss(reduction='sum')\n",
    "    with torch.no_grad():\n",
    "        for data, target in test_loader:\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            output = model(data)\n",
    "            test_loss += loss_fn(output, target)  # sum up batch loss\n",
    "            pred = output.argmax(dim=1, keepdim=True)  # get the index of the max log-probability\n",
    "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "\n",
    "    test_loss /= len(test_loader.dataset)\n",
    "\n",
    "    print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
    "        test_loss, correct, len(test_loader.dataset),\n",
    "        100. * correct / len(test_loader.dataset)))\n",
    "    return test_loss, 100. * correct / len(test_loader.dataset)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "9a287dac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torchvision.datasets.mnist.MNIST\n"
     ]
    }
   ],
   "source": [
    "use_cuda = torch.cuda.is_available()\n",
    "torch.manual_seed(1)\n",
    "\n",
    "device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
    "\n",
    "train_kwargs = {'batch_size': 512}\n",
    "test_kwargs = {'batch_size': 512}\n",
    "if use_cuda:\n",
    "    cuda_kwargs = {'num_workers': 1,\n",
    "                   'pin_memory': True,\n",
    "                   'shuffle': True}\n",
    "    train_kwargs.update(cuda_kwargs)\n",
    "    test_kwargs.update(cuda_kwargs)\n",
    "\n",
    "transform=transforms.Compose([\n",
    "        transforms.ToTensor()\n",
    "        ])\n",
    "dataset1 = datasets.MNIST('../data', train=True, download=True, transform=transform)\n",
    "dataset2 = datasets.MNIST('../data', train=False, transform=transform)\n",
    "print(type(dataset1))\n",
    "train_loader = torch.utils.data.DataLoader(dataset1,**train_kwargs)\n",
    "test_loader = torch.utils.data.DataLoader(dataset2, **test_kwargs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "6333413f",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data=[]\n",
    "train_labels=[]\n",
    "for data, label in train_loader:\n",
    "    for dt in data: \n",
    "        train_data.append(dt)\n",
    "    for lb in label:\n",
    "        train_labels.append(lb)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "88a94d28",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "random.shuffle(train_labels)\n",
    "\n",
    "train_data_shuffle = []\n",
    "for i in range(len(train_data)):\n",
    "    train_data_shuffle.append([train_data[i], train_labels[i]])\n",
    "\n",
    "train_loader_shuffle = torch.utils.data.DataLoader(train_data_shuffle, shuffle=True, batch_size=512)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "59cce36b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def returnModelAccAndLoss(model, device, train_loader, test_loader, optimizer, loss, epoch, scheduler):\n",
    "    train_loss=[]\n",
    "    test_loss=[]\n",
    "    accuracy_train=[]\n",
    "    accuracy_test=[]\n",
    "    for epoch in range(1, epoch+1):\n",
    "        tr_loss, tr_accuracy=train(model, device, train_loader, optimizer, loss, epoch)\n",
    "        train_loss.append(tr_loss)\n",
    "        accuracy_train.append(tr_accuracy)\n",
    "        tst_loss, tst_accuracy = test(model, device, test_loader)\n",
    "        test_loss.append(tst_loss)\n",
    "        accuracy_test.append(tst_accuracy)\n",
    "        #scheduler.step()\n",
    "    return train_loss, accuracy_train, test_loss, accuracy_test\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf869b0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training - Epoch: 1 loss: 2.3023507970174153 accuracy: 10.911666666666667\n",
      "\n",
      "Test set: Average loss: 2.3056, Accuracy: 1130/10000 (11%)\n",
      "\n",
      "Training - Epoch: 2 loss: 2.3016385210673014 accuracy: 11.15\n",
      "\n",
      "Test set: Average loss: 2.3054, Accuracy: 1143/10000 (11%)\n",
      "\n",
      "Training - Epoch: 3 loss: 2.301399491882324 accuracy: 11.2\n",
      "\n",
      "Test set: Average loss: 2.3053, Accuracy: 1145/10000 (11%)\n",
      "\n",
      "Training - Epoch: 4 loss: 2.301260819753011 accuracy: 11.181666666666667\n",
      "\n",
      "Test set: Average loss: 2.3051, Accuracy: 1146/10000 (11%)\n",
      "\n",
      "Training - Epoch: 5 loss: 2.3011598383585614 accuracy: 11.181666666666667\n",
      "\n",
      "Test set: Average loss: 2.3050, Accuracy: 1145/10000 (11%)\n",
      "\n",
      "Training - Epoch: 6 loss: 2.3010778531392417 accuracy: 11.218333333333334\n",
      "\n",
      "Test set: Average loss: 2.3050, Accuracy: 1144/10000 (11%)\n",
      "\n",
      "Training - Epoch: 7 loss: 2.3009973823547365 accuracy: 11.221666666666666\n",
      "\n",
      "Test set: Average loss: 2.3048, Accuracy: 1147/10000 (11%)\n",
      "\n",
      "Training - Epoch: 8 loss: 2.3009054776509603 accuracy: 11.226666666666667\n",
      "\n",
      "Test set: Average loss: 2.3049, Accuracy: 1142/10000 (11%)\n",
      "\n",
      "Training - Epoch: 9 loss: 2.300834103902181 accuracy: 11.213333333333333\n",
      "\n",
      "Test set: Average loss: 2.3047, Accuracy: 1144/10000 (11%)\n",
      "\n",
      "Training - Epoch: 10 loss: 2.30075402247111 accuracy: 11.2\n",
      "\n",
      "Test set: Average loss: 2.3045, Accuracy: 1143/10000 (11%)\n",
      "\n",
      "Training - Epoch: 11 loss: 2.3006858070373535 accuracy: 11.19\n",
      "\n",
      "Test set: Average loss: 2.3046, Accuracy: 1144/10000 (11%)\n",
      "\n",
      "Training - Epoch: 12 loss: 2.3006230527242026 accuracy: 11.223333333333333\n",
      "\n",
      "Test set: Average loss: 2.3045, Accuracy: 1143/10000 (11%)\n",
      "\n",
      "Training - Epoch: 13 loss: 2.300553733317057 accuracy: 11.24\n",
      "\n",
      "Test set: Average loss: 2.3042, Accuracy: 1147/10000 (11%)\n",
      "\n",
      "Training - Epoch: 14 loss: 2.300491578420003 accuracy: 11.166666666666666\n",
      "\n",
      "Test set: Average loss: 2.3044, Accuracy: 1143/10000 (11%)\n",
      "\n",
      "Training - Epoch: 15 loss: 2.3004236389160155 accuracy: 11.24\n",
      "\n",
      "Test set: Average loss: 2.3041, Accuracy: 1149/10000 (11%)\n",
      "\n",
      "Training - Epoch: 16 loss: 2.3003756385803222 accuracy: 11.246666666666666\n",
      "\n",
      "Test set: Average loss: 2.3041, Accuracy: 1149/10000 (11%)\n",
      "\n",
      "Training - Epoch: 17 loss: 2.3003250208536783 accuracy: 11.203333333333333\n",
      "\n",
      "Test set: Average loss: 2.3041, Accuracy: 1146/10000 (11%)\n",
      "\n",
      "Training - Epoch: 18 loss: 2.3002721705118816 accuracy: 11.2\n",
      "\n",
      "Test set: Average loss: 2.3042, Accuracy: 1144/10000 (11%)\n",
      "\n",
      "Training - Epoch: 19 loss: 2.3002118273417156 accuracy: 11.236666666666666\n",
      "\n",
      "Test set: Average loss: 2.3044, Accuracy: 1144/10000 (11%)\n",
      "\n",
      "Training - Epoch: 20 loss: 2.3001636545817057 accuracy: 11.2\n",
      "\n",
      "Test set: Average loss: 2.3043, Accuracy: 1143/10000 (11%)\n",
      "\n",
      "Training - Epoch: 21 loss: 2.3001154851277668 accuracy: 11.25\n",
      "\n",
      "Test set: Average loss: 2.3043, Accuracy: 1145/10000 (11%)\n",
      "\n",
      "Training - Epoch: 22 loss: 2.300068547821045 accuracy: 11.233333333333333\n",
      "\n",
      "Test set: Average loss: 2.3043, Accuracy: 1145/10000 (11%)\n",
      "\n",
      "Training - Epoch: 23 loss: 2.3000273953755697 accuracy: 11.236666666666666\n",
      "\n",
      "Test set: Average loss: 2.3043, Accuracy: 1145/10000 (11%)\n",
      "\n",
      "Training - Epoch: 24 loss: 2.299979170735677 accuracy: 11.23\n",
      "\n",
      "Test set: Average loss: 2.3042, Accuracy: 1144/10000 (11%)\n",
      "\n",
      "Training - Epoch: 25 loss: 2.299924745941162 accuracy: 11.245\n",
      "\n",
      "Test set: Average loss: 2.3042, Accuracy: 1144/10000 (11%)\n",
      "\n",
      "Training - Epoch: 26 loss: 2.2998852121988933 accuracy: 11.241666666666667\n",
      "\n",
      "Test set: Average loss: 2.3042, Accuracy: 1147/10000 (11%)\n",
      "\n",
      "Training - Epoch: 27 loss: 2.299842198689779 accuracy: 11.231666666666667\n",
      "\n",
      "Test set: Average loss: 2.3041, Accuracy: 1147/10000 (11%)\n",
      "\n",
      "Training - Epoch: 28 loss: 2.299796341451009 accuracy: 11.25\n",
      "\n",
      "Test set: Average loss: 2.3041, Accuracy: 1147/10000 (11%)\n",
      "\n",
      "Training - Epoch: 29 loss: 2.2997417872111003 accuracy: 11.26\n",
      "\n",
      "Test set: Average loss: 2.3041, Accuracy: 1147/10000 (11%)\n",
      "\n",
      "Training - Epoch: 30 loss: 2.2997137499491376 accuracy: 11.253333333333334\n",
      "\n",
      "Test set: Average loss: 2.3041, Accuracy: 1148/10000 (11%)\n",
      "\n",
      "Training - Epoch: 31 loss: 2.299665187072754 accuracy: 11.251666666666667\n",
      "\n",
      "Test set: Average loss: 2.3041, Accuracy: 1145/10000 (11%)\n",
      "\n",
      "Training - Epoch: 32 loss: 2.2996215761820475 accuracy: 11.275\n",
      "\n",
      "Test set: Average loss: 2.3043, Accuracy: 1145/10000 (11%)\n",
      "\n",
      "Training - Epoch: 33 loss: 2.2995721488952636 accuracy: 11.296666666666667\n",
      "\n",
      "Test set: Average loss: 2.3042, Accuracy: 1149/10000 (11%)\n",
      "\n",
      "Training - Epoch: 34 loss: 2.2995360781351724 accuracy: 11.318333333333333\n",
      "\n",
      "Test set: Average loss: 2.3043, Accuracy: 1148/10000 (11%)\n",
      "\n",
      "Training - Epoch: 35 loss: 2.2994909200032554 accuracy: 11.32\n",
      "\n",
      "Test set: Average loss: 2.3043, Accuracy: 1148/10000 (11%)\n",
      "\n",
      "Training - Epoch: 36 loss: 2.2994493573506674 accuracy: 11.305\n",
      "\n",
      "Test set: Average loss: 2.3043, Accuracy: 1146/10000 (11%)\n",
      "\n",
      "Training - Epoch: 37 loss: 2.299408674621582 accuracy: 11.335\n",
      "\n",
      "Test set: Average loss: 2.3044, Accuracy: 1143/10000 (11%)\n",
      "\n",
      "Training - Epoch: 38 loss: 2.2993714444478353 accuracy: 11.308333333333334\n",
      "\n",
      "Test set: Average loss: 2.3043, Accuracy: 1145/10000 (11%)\n",
      "\n",
      "Training - Epoch: 39 loss: 2.2993235733032225 accuracy: 11.306666666666667\n",
      "\n",
      "Test set: Average loss: 2.3044, Accuracy: 1143/10000 (11%)\n",
      "\n",
      "Training - Epoch: 40 loss: 2.2992828221639 accuracy: 11.316666666666666\n",
      "\n",
      "Test set: Average loss: 2.3042, Accuracy: 1146/10000 (11%)\n",
      "\n",
      "Training - Epoch: 41 loss: 2.2992436818440756 accuracy: 11.341666666666667\n",
      "\n",
      "Test set: Average loss: 2.3042, Accuracy: 1151/10000 (12%)\n",
      "\n",
      "Training - Epoch: 42 loss: 2.2992025698343914 accuracy: 11.338333333333333\n",
      "\n",
      "Test set: Average loss: 2.3045, Accuracy: 1146/10000 (11%)\n",
      "\n",
      "Training - Epoch: 43 loss: 2.2991643107096356 accuracy: 11.338333333333333\n",
      "\n",
      "Test set: Average loss: 2.3043, Accuracy: 1148/10000 (11%)\n",
      "\n",
      "Training - Epoch: 44 loss: 2.2991146695454914 accuracy: 11.345\n",
      "\n",
      "Test set: Average loss: 2.3044, Accuracy: 1145/10000 (11%)\n",
      "\n",
      "Training - Epoch: 45 loss: 2.2990792269388836 accuracy: 11.368333333333334\n",
      "\n",
      "Test set: Average loss: 2.3046, Accuracy: 1145/10000 (11%)\n",
      "\n",
      "Training - Epoch: 46 loss: 2.299036459859212 accuracy: 11.343333333333334\n",
      "\n",
      "Test set: Average loss: 2.3043, Accuracy: 1157/10000 (12%)\n",
      "\n",
      "Training - Epoch: 47 loss: 2.2990059682210284 accuracy: 11.345\n",
      "\n",
      "Test set: Average loss: 2.3044, Accuracy: 1148/10000 (11%)\n",
      "\n",
      "Training - Epoch: 48 loss: 2.2989546775817873 accuracy: 11.356666666666667\n",
      "\n",
      "Test set: Average loss: 2.3047, Accuracy: 1155/10000 (12%)\n",
      "\n",
      "Training - Epoch: 49 loss: 2.2989304580688477 accuracy: 11.325\n",
      "\n",
      "Test set: Average loss: 2.3046, Accuracy: 1153/10000 (12%)\n",
      "\n",
      "Training - Epoch: 50 loss: 2.2988876220703127 accuracy: 11.355\n",
      "\n",
      "Test set: Average loss: 2.3045, Accuracy: 1157/10000 (12%)\n",
      "\n",
      "Training - Epoch: 51 loss: 2.298849851735433 accuracy: 11.39\n",
      "\n",
      "Test set: Average loss: 2.3045, Accuracy: 1152/10000 (12%)\n",
      "\n",
      "Training - Epoch: 52 loss: 2.2988125073750814 accuracy: 11.31\n",
      "\n",
      "Test set: Average loss: 2.3045, Accuracy: 1150/10000 (12%)\n",
      "\n",
      "Training - Epoch: 53 loss: 2.2987751452128093 accuracy: 11.365\n",
      "\n",
      "Test set: Average loss: 2.3046, Accuracy: 1147/10000 (11%)\n",
      "\n",
      "Training - Epoch: 54 loss: 2.2987351135253906 accuracy: 11.35\n",
      "\n",
      "Test set: Average loss: 2.3047, Accuracy: 1151/10000 (12%)\n",
      "\n",
      "Training - Epoch: 55 loss: 2.2986918924967448 accuracy: 11.366666666666667\n",
      "\n",
      "Test set: Average loss: 2.3047, Accuracy: 1151/10000 (12%)\n",
      "\n",
      "Training - Epoch: 56 loss: 2.298660107421875 accuracy: 11.426666666666666\n",
      "\n",
      "Test set: Average loss: 2.3046, Accuracy: 1152/10000 (12%)\n",
      "\n",
      "Training - Epoch: 57 loss: 2.2986270484924316 accuracy: 11.393333333333333\n",
      "\n",
      "Test set: Average loss: 2.3047, Accuracy: 1152/10000 (12%)\n",
      "\n",
      "Training - Epoch: 58 loss: 2.2985908093770346 accuracy: 11.375\n",
      "\n",
      "Test set: Average loss: 2.3048, Accuracy: 1153/10000 (12%)\n",
      "\n",
      "Training - Epoch: 59 loss: 2.2985456364949544 accuracy: 11.436666666666667\n",
      "\n",
      "Test set: Average loss: 2.3048, Accuracy: 1153/10000 (12%)\n",
      "\n",
      "Training - Epoch: 60 loss: 2.2985132095336915 accuracy: 11.408333333333333\n",
      "\n",
      "Test set: Average loss: 2.3049, Accuracy: 1150/10000 (12%)\n",
      "\n",
      "Training - Epoch: 61 loss: 2.2984671567281088 accuracy: 11.426666666666666\n",
      "\n",
      "Test set: Average loss: 2.3050, Accuracy: 1158/10000 (12%)\n",
      "\n",
      "Training - Epoch: 62 loss: 2.298434902445475 accuracy: 11.435\n",
      "\n",
      "Test set: Average loss: 2.3049, Accuracy: 1160/10000 (12%)\n",
      "\n",
      "Training - Epoch: 63 loss: 2.2984050018310547 accuracy: 11.43\n",
      "\n",
      "Test set: Average loss: 2.3049, Accuracy: 1158/10000 (12%)\n",
      "\n",
      "Training - Epoch: 64 loss: 2.2983593002319336 accuracy: 11.41\n",
      "\n",
      "Test set: Average loss: 2.3050, Accuracy: 1155/10000 (12%)\n",
      "\n",
      "Training - Epoch: 65 loss: 2.2983280082702637 accuracy: 11.458333333333334\n",
      "\n",
      "Test set: Average loss: 2.3051, Accuracy: 1156/10000 (12%)\n",
      "\n",
      "Training - Epoch: 66 loss: 2.29828889948527 accuracy: 11.426666666666666\n",
      "\n",
      "Test set: Average loss: 2.3050, Accuracy: 1158/10000 (12%)\n",
      "\n",
      "Training - Epoch: 67 loss: 2.298256593322754 accuracy: 11.48\n",
      "\n",
      "Test set: Average loss: 2.3051, Accuracy: 1155/10000 (12%)\n",
      "\n",
      "Training - Epoch: 68 loss: 2.298217844136556 accuracy: 11.476666666666667\n",
      "\n",
      "Test set: Average loss: 2.3052, Accuracy: 1156/10000 (12%)\n",
      "\n",
      "Training - Epoch: 69 loss: 2.298174248758952 accuracy: 11.493333333333334\n",
      "\n",
      "Test set: Average loss: 2.3053, Accuracy: 1154/10000 (12%)\n",
      "\n",
      "Training - Epoch: 70 loss: 2.298146939086914 accuracy: 11.493333333333334\n",
      "\n",
      "Test set: Average loss: 2.3053, Accuracy: 1154/10000 (12%)\n",
      "\n",
      "Training - Epoch: 71 loss: 2.298106963602702 accuracy: 11.46\n",
      "\n",
      "Test set: Average loss: 2.3051, Accuracy: 1159/10000 (12%)\n",
      "\n",
      "Training - Epoch: 72 loss: 2.298063168589274 accuracy: 11.498333333333333\n",
      "\n",
      "Test set: Average loss: 2.3054, Accuracy: 1163/10000 (12%)\n",
      "\n",
      "Training - Epoch: 73 loss: 2.2980276527404784 accuracy: 11.531666666666666\n",
      "\n",
      "Test set: Average loss: 2.3054, Accuracy: 1155/10000 (12%)\n",
      "\n",
      "Training - Epoch: 74 loss: 2.2979983627319336 accuracy: 11.528333333333334\n",
      "\n",
      "Test set: Average loss: 2.3054, Accuracy: 1162/10000 (12%)\n",
      "\n",
      "Training - Epoch: 75 loss: 2.297968541208903 accuracy: 11.603333333333333\n",
      "\n",
      "Test set: Average loss: 2.3053, Accuracy: 1156/10000 (12%)\n",
      "\n",
      "Training - Epoch: 76 loss: 2.297920516459147 accuracy: 11.556666666666667\n",
      "\n",
      "Test set: Average loss: 2.3052, Accuracy: 1154/10000 (12%)\n",
      "\n",
      "Training - Epoch: 77 loss: 2.2978915087382 accuracy: 11.538333333333334\n",
      "\n",
      "Test set: Average loss: 2.3053, Accuracy: 1159/10000 (12%)\n",
      "\n",
      "Training - Epoch: 78 loss: 2.297856248982747 accuracy: 11.488333333333333\n",
      "\n",
      "Test set: Average loss: 2.3053, Accuracy: 1161/10000 (12%)\n",
      "\n",
      "Training - Epoch: 79 loss: 2.297811775970459 accuracy: 11.511666666666667\n",
      "\n",
      "Test set: Average loss: 2.3052, Accuracy: 1161/10000 (12%)\n",
      "\n",
      "Training - Epoch: 80 loss: 2.297788023122152 accuracy: 11.53\n",
      "\n",
      "Test set: Average loss: 2.3055, Accuracy: 1156/10000 (12%)\n",
      "\n",
      "Training - Epoch: 81 loss: 2.297757253519694 accuracy: 11.551666666666666\n",
      "\n",
      "Test set: Average loss: 2.3056, Accuracy: 1161/10000 (12%)\n",
      "\n",
      "Training - Epoch: 82 loss: 2.297710611979167 accuracy: 11.531666666666666\n",
      "\n",
      "Test set: Average loss: 2.3058, Accuracy: 1158/10000 (12%)\n",
      "\n",
      "Training - Epoch: 83 loss: 2.297674096171061 accuracy: 11.58\n",
      "\n",
      "Test set: Average loss: 2.3056, Accuracy: 1161/10000 (12%)\n",
      "\n",
      "Training - Epoch: 84 loss: 2.297649361673991 accuracy: 11.568333333333333\n",
      "\n",
      "Test set: Average loss: 2.3056, Accuracy: 1161/10000 (12%)\n",
      "\n",
      "Training - Epoch: 85 loss: 2.297602370961507 accuracy: 11.54\n",
      "\n",
      "Test set: Average loss: 2.3056, Accuracy: 1159/10000 (12%)\n",
      "\n",
      "Training - Epoch: 86 loss: 2.297574286142985 accuracy: 11.53\n",
      "\n",
      "Test set: Average loss: 2.3057, Accuracy: 1158/10000 (12%)\n",
      "\n",
      "Training - Epoch: 87 loss: 2.297535449473063 accuracy: 11.563333333333333\n",
      "\n",
      "Test set: Average loss: 2.3057, Accuracy: 1165/10000 (12%)\n",
      "\n",
      "Training - Epoch: 88 loss: 2.297500440724691 accuracy: 11.578333333333333\n",
      "\n",
      "Test set: Average loss: 2.3060, Accuracy: 1161/10000 (12%)\n",
      "\n",
      "Training - Epoch: 89 loss: 2.2974691212972007 accuracy: 11.59\n",
      "\n",
      "Test set: Average loss: 2.3059, Accuracy: 1162/10000 (12%)\n",
      "\n",
      "Training - Epoch: 90 loss: 2.297432002003988 accuracy: 11.568333333333333\n",
      "\n",
      "Test set: Average loss: 2.3059, Accuracy: 1162/10000 (12%)\n",
      "\n",
      "Training - Epoch: 91 loss: 2.2973940175374348 accuracy: 11.575\n",
      "\n",
      "Test set: Average loss: 2.3058, Accuracy: 1168/10000 (12%)\n",
      "\n",
      "Training - Epoch: 92 loss: 2.297368394470215 accuracy: 11.531666666666666\n",
      "\n",
      "Test set: Average loss: 2.3057, Accuracy: 1175/10000 (12%)\n",
      "\n",
      "Training - Epoch: 93 loss: 2.297328601328532 accuracy: 11.59\n",
      "\n",
      "Test set: Average loss: 2.3058, Accuracy: 1169/10000 (12%)\n",
      "\n",
      "Training - Epoch: 94 loss: 2.2972937624613445 accuracy: 11.563333333333333\n",
      "\n",
      "Test set: Average loss: 2.3059, Accuracy: 1168/10000 (12%)\n",
      "\n",
      "Training - Epoch: 95 loss: 2.2972633176167805 accuracy: 11.548333333333334\n",
      "\n",
      "Test set: Average loss: 2.3058, Accuracy: 1168/10000 (12%)\n",
      "\n",
      "Training - Epoch: 96 loss: 2.2972268180847166 accuracy: 11.586666666666666\n",
      "\n",
      "Test set: Average loss: 2.3060, Accuracy: 1165/10000 (12%)\n",
      "\n",
      "Training - Epoch: 97 loss: 2.297184006500244 accuracy: 11.571666666666667\n",
      "\n",
      "Test set: Average loss: 2.3061, Accuracy: 1171/10000 (12%)\n",
      "\n",
      "Training - Epoch: 98 loss: 2.2971491505940755 accuracy: 11.62\n",
      "\n",
      "Test set: Average loss: 2.3059, Accuracy: 1177/10000 (12%)\n",
      "\n",
      "Training - Epoch: 99 loss: 2.2971261423746743 accuracy: 11.685\n",
      "\n",
      "Test set: Average loss: 2.3061, Accuracy: 1166/10000 (12%)\n",
      "\n",
      "Training - Epoch: 100 loss: 2.297078156534831 accuracy: 11.633333333333333\n",
      "\n",
      "Test set: Average loss: 2.3059, Accuracy: 1171/10000 (12%)\n",
      "\n",
      "Training - Epoch: 101 loss: 2.2970507298787437 accuracy: 11.646666666666667\n",
      "\n",
      "Test set: Average loss: 2.3062, Accuracy: 1164/10000 (12%)\n",
      "\n",
      "Training - Epoch: 102 loss: 2.2970145383199054 accuracy: 11.598333333333333\n",
      "\n",
      "Test set: Average loss: 2.3062, Accuracy: 1164/10000 (12%)\n",
      "\n",
      "Training - Epoch: 103 loss: 2.2969772654215497 accuracy: 11.658333333333333\n",
      "\n",
      "Test set: Average loss: 2.3062, Accuracy: 1160/10000 (12%)\n",
      "\n",
      "Training - Epoch: 104 loss: 2.2969465306599934 accuracy: 11.708333333333334\n",
      "\n",
      "Test set: Average loss: 2.3062, Accuracy: 1166/10000 (12%)\n",
      "\n",
      "Training - Epoch: 105 loss: 2.2969170850118 accuracy: 11.631666666666666\n",
      "\n",
      "Test set: Average loss: 2.3063, Accuracy: 1169/10000 (12%)\n",
      "\n",
      "Training - Epoch: 106 loss: 2.2968700932820636 accuracy: 11.683333333333334\n",
      "\n",
      "Test set: Average loss: 2.3062, Accuracy: 1169/10000 (12%)\n",
      "\n",
      "Training - Epoch: 107 loss: 2.29683642578125 accuracy: 11.691666666666666\n",
      "\n",
      "Test set: Average loss: 2.3063, Accuracy: 1167/10000 (12%)\n",
      "\n",
      "Training - Epoch: 108 loss: 2.2968060877482097 accuracy: 11.643333333333333\n",
      "\n",
      "Test set: Average loss: 2.3065, Accuracy: 1168/10000 (12%)\n",
      "\n",
      "Training - Epoch: 109 loss: 2.296772021230062 accuracy: 11.678333333333333\n",
      "\n",
      "Test set: Average loss: 2.3066, Accuracy: 1165/10000 (12%)\n",
      "\n",
      "Training - Epoch: 110 loss: 2.296727879079183 accuracy: 11.703333333333333\n",
      "\n",
      "Test set: Average loss: 2.3063, Accuracy: 1158/10000 (12%)\n",
      "\n",
      "Training - Epoch: 111 loss: 2.2966996185302735 accuracy: 11.695\n",
      "\n",
      "Test set: Average loss: 2.3062, Accuracy: 1165/10000 (12%)\n",
      "\n",
      "Training - Epoch: 112 loss: 2.296663326517741 accuracy: 11.785\n",
      "\n",
      "Test set: Average loss: 2.3067, Accuracy: 1163/10000 (12%)\n",
      "\n",
      "Training - Epoch: 113 loss: 2.29663814163208 accuracy: 11.703333333333333\n",
      "\n",
      "Test set: Average loss: 2.3065, Accuracy: 1172/10000 (12%)\n",
      "\n",
      "Training - Epoch: 114 loss: 2.2965890940348306 accuracy: 11.735\n",
      "\n",
      "Test set: Average loss: 2.3066, Accuracy: 1174/10000 (12%)\n",
      "\n",
      "Training - Epoch: 115 loss: 2.2965646303812663 accuracy: 11.738333333333333\n",
      "\n",
      "Test set: Average loss: 2.3065, Accuracy: 1179/10000 (12%)\n",
      "\n",
      "Training - Epoch: 116 loss: 2.2965271336873374 accuracy: 11.773333333333333\n",
      "\n",
      "Test set: Average loss: 2.3068, Accuracy: 1172/10000 (12%)\n",
      "\n",
      "Training - Epoch: 117 loss: 2.296496691385905 accuracy: 11.738333333333333\n",
      "\n",
      "Test set: Average loss: 2.3066, Accuracy: 1175/10000 (12%)\n",
      "\n",
      "Training - Epoch: 118 loss: 2.296458204650879 accuracy: 11.756666666666666\n",
      "\n",
      "Test set: Average loss: 2.3066, Accuracy: 1177/10000 (12%)\n",
      "\n",
      "Training - Epoch: 119 loss: 2.296428550465902 accuracy: 11.753333333333334\n",
      "\n",
      "Test set: Average loss: 2.3065, Accuracy: 1180/10000 (12%)\n",
      "\n",
      "Training - Epoch: 120 loss: 2.296399735514323 accuracy: 11.831666666666667\n",
      "\n",
      "Test set: Average loss: 2.3067, Accuracy: 1177/10000 (12%)\n",
      "\n",
      "Training - Epoch: 121 loss: 2.296350666809082 accuracy: 11.763333333333334\n",
      "\n",
      "Test set: Average loss: 2.3064, Accuracy: 1187/10000 (12%)\n",
      "\n",
      "Training - Epoch: 122 loss: 2.2963216354370117 accuracy: 11.833333333333334\n",
      "\n",
      "Test set: Average loss: 2.3067, Accuracy: 1180/10000 (12%)\n",
      "\n",
      "Training - Epoch: 123 loss: 2.296284014638265 accuracy: 11.846666666666666\n",
      "\n",
      "Test set: Average loss: 2.3065, Accuracy: 1187/10000 (12%)\n",
      "\n",
      "Training - Epoch: 124 loss: 2.2962374130249024 accuracy: 11.801666666666666\n",
      "\n",
      "Test set: Average loss: 2.3064, Accuracy: 1197/10000 (12%)\n",
      "\n",
      "Training - Epoch: 125 loss: 2.2962115564982097 accuracy: 11.871666666666666\n",
      "\n",
      "Test set: Average loss: 2.3067, Accuracy: 1179/10000 (12%)\n",
      "\n",
      "Training - Epoch: 126 loss: 2.2961915115356444 accuracy: 11.801666666666666\n",
      "\n",
      "Test set: Average loss: 2.3067, Accuracy: 1186/10000 (12%)\n",
      "\n",
      "Training - Epoch: 127 loss: 2.2961532702128093 accuracy: 11.825\n",
      "\n",
      "Test set: Average loss: 2.3065, Accuracy: 1191/10000 (12%)\n",
      "\n",
      "Training - Epoch: 128 loss: 2.2961120152791343 accuracy: 11.783333333333333\n",
      "\n",
      "Test set: Average loss: 2.3066, Accuracy: 1196/10000 (12%)\n",
      "\n",
      "Training - Epoch: 129 loss: 2.296086429341634 accuracy: 11.886666666666667\n",
      "\n",
      "Test set: Average loss: 2.3067, Accuracy: 1192/10000 (12%)\n",
      "\n",
      "Training - Epoch: 130 loss: 2.29604337310791 accuracy: 11.853333333333333\n",
      "\n",
      "Test set: Average loss: 2.3067, Accuracy: 1185/10000 (12%)\n",
      "\n",
      "Training - Epoch: 131 loss: 2.296011659495036 accuracy: 11.931666666666667\n",
      "\n",
      "Test set: Average loss: 2.3068, Accuracy: 1182/10000 (12%)\n",
      "\n",
      "Training - Epoch: 132 loss: 2.2959793215433755 accuracy: 11.908333333333333\n",
      "\n",
      "Test set: Average loss: 2.3069, Accuracy: 1177/10000 (12%)\n",
      "\n",
      "Training - Epoch: 133 loss: 2.2959441767374673 accuracy: 11.856666666666667\n",
      "\n",
      "Test set: Average loss: 2.3067, Accuracy: 1183/10000 (12%)\n",
      "\n",
      "Training - Epoch: 134 loss: 2.295906021372477 accuracy: 11.856666666666667\n",
      "\n",
      "Test set: Average loss: 2.3070, Accuracy: 1186/10000 (12%)\n",
      "\n",
      "Training - Epoch: 135 loss: 2.295874450937907 accuracy: 11.896666666666667\n",
      "\n",
      "Test set: Average loss: 2.3068, Accuracy: 1196/10000 (12%)\n",
      "\n",
      "Training - Epoch: 136 loss: 2.2958449676513673 accuracy: 11.878333333333334\n",
      "\n",
      "Test set: Average loss: 2.3068, Accuracy: 1191/10000 (12%)\n",
      "\n",
      "Training - Epoch: 137 loss: 2.2958099098205564 accuracy: 11.93\n",
      "\n",
      "Test set: Average loss: 2.3069, Accuracy: 1188/10000 (12%)\n",
      "\n",
      "Training - Epoch: 138 loss: 2.295768017578125 accuracy: 11.946666666666667\n",
      "\n",
      "Test set: Average loss: 2.3067, Accuracy: 1197/10000 (12%)\n",
      "\n",
      "Training - Epoch: 139 loss: 2.2957395655314126 accuracy: 12.028333333333334\n",
      "\n",
      "Test set: Average loss: 2.3068, Accuracy: 1191/10000 (12%)\n",
      "\n",
      "Training - Epoch: 140 loss: 2.2957010228474934 accuracy: 11.86\n",
      "\n",
      "Test set: Average loss: 2.3066, Accuracy: 1194/10000 (12%)\n",
      "\n",
      "Training - Epoch: 141 loss: 2.2956640060424807 accuracy: 11.975\n",
      "\n",
      "Test set: Average loss: 2.3070, Accuracy: 1184/10000 (12%)\n",
      "\n",
      "Training - Epoch: 142 loss: 2.29563427734375 accuracy: 11.966666666666667\n",
      "\n",
      "Test set: Average loss: 2.3069, Accuracy: 1194/10000 (12%)\n",
      "\n",
      "Training - Epoch: 143 loss: 2.2956021980285644 accuracy: 11.941666666666666\n",
      "\n",
      "Test set: Average loss: 2.3070, Accuracy: 1192/10000 (12%)\n",
      "\n",
      "Training - Epoch: 144 loss: 2.2955741099039715 accuracy: 12.02\n",
      "\n",
      "Test set: Average loss: 2.3072, Accuracy: 1187/10000 (12%)\n",
      "\n",
      "Training - Epoch: 145 loss: 2.295538452911377 accuracy: 12.025\n",
      "\n",
      "Test set: Average loss: 2.3070, Accuracy: 1192/10000 (12%)\n",
      "\n",
      "Training - Epoch: 146 loss: 2.2955026237487792 accuracy: 11.95\n",
      "\n",
      "Test set: Average loss: 2.3069, Accuracy: 1196/10000 (12%)\n",
      "\n",
      "Training - Epoch: 147 loss: 2.2954701538085938 accuracy: 11.98\n",
      "\n",
      "Test set: Average loss: 2.3070, Accuracy: 1192/10000 (12%)\n",
      "\n",
      "Training - Epoch: 148 loss: 2.295427678934733 accuracy: 12.03\n",
      "\n",
      "Test set: Average loss: 2.3070, Accuracy: 1186/10000 (12%)\n",
      "\n",
      "Training - Epoch: 149 loss: 2.2953939427693686 accuracy: 12.035\n",
      "\n",
      "Test set: Average loss: 2.3071, Accuracy: 1186/10000 (12%)\n",
      "\n",
      "Training - Epoch: 150 loss: 2.2953509686787923 accuracy: 11.976666666666667\n",
      "\n",
      "Test set: Average loss: 2.3068, Accuracy: 1206/10000 (12%)\n",
      "\n",
      "Training - Epoch: 151 loss: 2.2953274948120117 accuracy: 12.04\n",
      "\n",
      "Test set: Average loss: 2.3069, Accuracy: 1192/10000 (12%)\n",
      "\n",
      "Training - Epoch: 152 loss: 2.29530693359375 accuracy: 12.065\n",
      "\n",
      "Test set: Average loss: 2.3070, Accuracy: 1194/10000 (12%)\n",
      "\n",
      "Training - Epoch: 153 loss: 2.2952542696634928 accuracy: 11.963333333333333\n",
      "\n",
      "Test set: Average loss: 2.3070, Accuracy: 1185/10000 (12%)\n",
      "\n",
      "Training - Epoch: 154 loss: 2.2952297706604003 accuracy: 12.046666666666667\n",
      "\n",
      "Test set: Average loss: 2.3068, Accuracy: 1193/10000 (12%)\n",
      "\n",
      "Training - Epoch: 155 loss: 2.2952032229105632 accuracy: 12.078333333333333\n",
      "\n",
      "Test set: Average loss: 2.3069, Accuracy: 1196/10000 (12%)\n",
      "\n",
      "Training - Epoch: 156 loss: 2.2951652168273924 accuracy: 12.021666666666667\n",
      "\n",
      "Test set: Average loss: 2.3068, Accuracy: 1195/10000 (12%)\n",
      "\n",
      "Training - Epoch: 157 loss: 2.295124770609538 accuracy: 12.076666666666666\n",
      "\n",
      "Test set: Average loss: 2.3070, Accuracy: 1201/10000 (12%)\n",
      "\n",
      "Training - Epoch: 158 loss: 2.2951046630859375 accuracy: 12.086666666666666\n",
      "\n",
      "Test set: Average loss: 2.3071, Accuracy: 1196/10000 (12%)\n",
      "\n",
      "Training - Epoch: 159 loss: 2.295055875905355 accuracy: 12.018333333333333\n",
      "\n",
      "Test set: Average loss: 2.3070, Accuracy: 1205/10000 (12%)\n",
      "\n",
      "Training - Epoch: 160 loss: 2.295021784210205 accuracy: 12.113333333333333\n",
      "\n",
      "Test set: Average loss: 2.3070, Accuracy: 1196/10000 (12%)\n",
      "\n",
      "Training - Epoch: 161 loss: 2.2949857022603353 accuracy: 12.078333333333333\n",
      "\n",
      "Test set: Average loss: 2.3071, Accuracy: 1201/10000 (12%)\n",
      "\n",
      "Training - Epoch: 162 loss: 2.2949596557617187 accuracy: 12.068333333333333\n",
      "\n",
      "Test set: Average loss: 2.3072, Accuracy: 1193/10000 (12%)\n",
      "\n",
      "Training - Epoch: 163 loss: 2.2949192642211913 accuracy: 12.068333333333333\n",
      "\n",
      "Test set: Average loss: 2.3073, Accuracy: 1199/10000 (12%)\n",
      "\n",
      "Training - Epoch: 164 loss: 2.2948966171264646 accuracy: 12.065\n",
      "\n",
      "Test set: Average loss: 2.3072, Accuracy: 1196/10000 (12%)\n",
      "\n",
      "Training - Epoch: 165 loss: 2.294852062225342 accuracy: 12.103333333333333\n",
      "\n",
      "Test set: Average loss: 2.3074, Accuracy: 1190/10000 (12%)\n",
      "\n",
      "Training - Epoch: 166 loss: 2.2948131909688314 accuracy: 12.05\n",
      "\n",
      "Test set: Average loss: 2.3074, Accuracy: 1188/10000 (12%)\n",
      "\n",
      "Training - Epoch: 167 loss: 2.2947798530578614 accuracy: 12.068333333333333\n",
      "\n",
      "Test set: Average loss: 2.3071, Accuracy: 1196/10000 (12%)\n",
      "\n",
      "Training - Epoch: 168 loss: 2.2947523048400877 accuracy: 12.183333333333334\n",
      "\n",
      "Test set: Average loss: 2.3072, Accuracy: 1189/10000 (12%)\n",
      "\n",
      "Training - Epoch: 169 loss: 2.294710076904297 accuracy: 12.095\n",
      "\n",
      "Test set: Average loss: 2.3072, Accuracy: 1199/10000 (12%)\n",
      "\n",
      "Training - Epoch: 170 loss: 2.2946806966145834 accuracy: 12.118333333333334\n",
      "\n",
      "Test set: Average loss: 2.3073, Accuracy: 1188/10000 (12%)\n",
      "\n",
      "Training - Epoch: 171 loss: 2.294647841644287 accuracy: 12.136666666666667\n",
      "\n",
      "Test set: Average loss: 2.3072, Accuracy: 1199/10000 (12%)\n",
      "\n",
      "Training - Epoch: 172 loss: 2.2946075040181477 accuracy: 12.185\n",
      "\n",
      "Test set: Average loss: 2.3075, Accuracy: 1194/10000 (12%)\n",
      "\n",
      "Training - Epoch: 173 loss: 2.2945771914164226 accuracy: 12.101666666666667\n",
      "\n",
      "Test set: Average loss: 2.3072, Accuracy: 1195/10000 (12%)\n",
      "\n",
      "Training - Epoch: 174 loss: 2.2945457176208497 accuracy: 12.178333333333333\n",
      "\n",
      "Test set: Average loss: 2.3075, Accuracy: 1186/10000 (12%)\n",
      "\n",
      "Training - Epoch: 175 loss: 2.2945125147501626 accuracy: 12.216666666666667\n",
      "\n",
      "Test set: Average loss: 2.3076, Accuracy: 1184/10000 (12%)\n",
      "\n",
      "Training - Epoch: 176 loss: 2.294473491160075 accuracy: 12.12\n",
      "\n",
      "Test set: Average loss: 2.3074, Accuracy: 1183/10000 (12%)\n",
      "\n",
      "Training - Epoch: 177 loss: 2.294439006551107 accuracy: 12.185\n",
      "\n",
      "Test set: Average loss: 2.3073, Accuracy: 1189/10000 (12%)\n",
      "\n",
      "Training - Epoch: 178 loss: 2.2944000798543294 accuracy: 12.171666666666667\n",
      "\n",
      "Test set: Average loss: 2.3072, Accuracy: 1197/10000 (12%)\n",
      "\n",
      "Training - Epoch: 179 loss: 2.2943707931518555 accuracy: 12.19\n",
      "\n",
      "Test set: Average loss: 2.3071, Accuracy: 1189/10000 (12%)\n",
      "\n",
      "Training - Epoch: 180 loss: 2.29433470509847 accuracy: 12.146666666666667\n",
      "\n",
      "Test set: Average loss: 2.3074, Accuracy: 1187/10000 (12%)\n",
      "\n",
      "Training - Epoch: 181 loss: 2.2942951461791994 accuracy: 12.148333333333333\n",
      "\n",
      "Test set: Average loss: 2.3071, Accuracy: 1184/10000 (12%)\n",
      "\n",
      "Training - Epoch: 182 loss: 2.2942686075846352 accuracy: 12.198333333333334\n",
      "\n",
      "Test set: Average loss: 2.3074, Accuracy: 1180/10000 (12%)\n",
      "\n",
      "Training - Epoch: 183 loss: 2.2942331652323404 accuracy: 12.268333333333333\n",
      "\n",
      "Test set: Average loss: 2.3074, Accuracy: 1187/10000 (12%)\n",
      "\n",
      "Training - Epoch: 184 loss: 2.294193772125244 accuracy: 12.178333333333333\n",
      "\n",
      "Test set: Average loss: 2.3076, Accuracy: 1184/10000 (12%)\n",
      "\n",
      "Training - Epoch: 185 loss: 2.2941564394632974 accuracy: 12.276666666666667\n",
      "\n",
      "Test set: Average loss: 2.3077, Accuracy: 1190/10000 (12%)\n",
      "\n",
      "Training - Epoch: 186 loss: 2.2941264790852864 accuracy: 12.11\n",
      "\n",
      "Test set: Average loss: 2.3074, Accuracy: 1188/10000 (12%)\n",
      "\n",
      "Training - Epoch: 187 loss: 2.2940979309082032 accuracy: 12.205\n",
      "\n",
      "Test set: Average loss: 2.3074, Accuracy: 1191/10000 (12%)\n",
      "\n",
      "Training - Epoch: 188 loss: 2.294068475087484 accuracy: 12.245\n",
      "\n",
      "Test set: Average loss: 2.3074, Accuracy: 1189/10000 (12%)\n",
      "\n",
      "Training - Epoch: 189 loss: 2.2940188791910807 accuracy: 12.221666666666666\n",
      "\n",
      "Test set: Average loss: 2.3072, Accuracy: 1176/10000 (12%)\n",
      "\n",
      "Training - Epoch: 190 loss: 2.2939813448588056 accuracy: 12.385\n",
      "\n",
      "Test set: Average loss: 2.3075, Accuracy: 1180/10000 (12%)\n",
      "\n",
      "Training - Epoch: 191 loss: 2.293955699412028 accuracy: 12.19\n",
      "\n",
      "Test set: Average loss: 2.3075, Accuracy: 1194/10000 (12%)\n",
      "\n",
      "Training - Epoch: 192 loss: 2.2939230977376304 accuracy: 12.258333333333333\n",
      "\n",
      "Test set: Average loss: 2.3076, Accuracy: 1180/10000 (12%)\n",
      "\n",
      "Training - Epoch: 193 loss: 2.293881635284424 accuracy: 12.295\n",
      "\n",
      "Test set: Average loss: 2.3074, Accuracy: 1171/10000 (12%)\n",
      "\n",
      "Training - Epoch: 194 loss: 2.293851267242432 accuracy: 12.323333333333334\n",
      "\n",
      "Test set: Average loss: 2.3077, Accuracy: 1188/10000 (12%)\n",
      "\n",
      "Training - Epoch: 195 loss: 2.293817891693115 accuracy: 12.376666666666667\n",
      "\n",
      "Test set: Average loss: 2.3076, Accuracy: 1189/10000 (12%)\n",
      "\n",
      "Training - Epoch: 196 loss: 2.293783690897624 accuracy: 12.32\n",
      "\n",
      "Test set: Average loss: 2.3076, Accuracy: 1189/10000 (12%)\n",
      "\n",
      "Training - Epoch: 197 loss: 2.2937464185078937 accuracy: 12.353333333333333\n",
      "\n",
      "Test set: Average loss: 2.3075, Accuracy: 1200/10000 (12%)\n",
      "\n",
      "Training - Epoch: 198 loss: 2.293706884511312 accuracy: 12.303333333333333\n",
      "\n",
      "Test set: Average loss: 2.3074, Accuracy: 1194/10000 (12%)\n",
      "\n",
      "Training - Epoch: 199 loss: 2.2936854451497397 accuracy: 12.356666666666667\n",
      "\n",
      "Test set: Average loss: 2.3077, Accuracy: 1176/10000 (12%)\n",
      "\n",
      "Training - Epoch: 200 loss: 2.2936409245808917 accuracy: 12.368333333333334\n",
      "\n",
      "Test set: Average loss: 2.3077, Accuracy: 1197/10000 (12%)\n",
      "\n",
      "Training - Epoch: 201 loss: 2.2936038416544595 accuracy: 12.323333333333334\n",
      "\n",
      "Test set: Average loss: 2.3077, Accuracy: 1192/10000 (12%)\n",
      "\n",
      "Training - Epoch: 202 loss: 2.293578988393148 accuracy: 12.373333333333333\n",
      "\n",
      "Test set: Average loss: 2.3077, Accuracy: 1179/10000 (12%)\n",
      "\n",
      "Training - Epoch: 203 loss: 2.2935400716145833 accuracy: 12.413333333333334\n",
      "\n",
      "Test set: Average loss: 2.3078, Accuracy: 1187/10000 (12%)\n",
      "\n",
      "Training - Epoch: 204 loss: 2.293503894551595 accuracy: 12.37\n",
      "\n",
      "Test set: Average loss: 2.3080, Accuracy: 1169/10000 (12%)\n",
      "\n",
      "Training - Epoch: 205 loss: 2.2934738034566244 accuracy: 12.321666666666667\n",
      "\n",
      "Test set: Average loss: 2.3077, Accuracy: 1171/10000 (12%)\n",
      "\n",
      "Training - Epoch: 206 loss: 2.293437214914958 accuracy: 12.426666666666666\n",
      "\n",
      "Test set: Average loss: 2.3077, Accuracy: 1178/10000 (12%)\n",
      "\n",
      "Training - Epoch: 207 loss: 2.293402025858561 accuracy: 12.386666666666667\n",
      "\n",
      "Test set: Average loss: 2.3077, Accuracy: 1163/10000 (12%)\n",
      "\n",
      "Training - Epoch: 208 loss: 2.293361922200521 accuracy: 12.445\n",
      "\n",
      "Test set: Average loss: 2.3075, Accuracy: 1154/10000 (12%)\n",
      "\n",
      "Training - Epoch: 209 loss: 2.2933359013875325 accuracy: 12.453333333333333\n",
      "\n",
      "Test set: Average loss: 2.3077, Accuracy: 1161/10000 (12%)\n",
      "\n",
      "Training - Epoch: 210 loss: 2.2932907689412434 accuracy: 12.538333333333334\n",
      "\n",
      "Test set: Average loss: 2.3079, Accuracy: 1191/10000 (12%)\n",
      "\n",
      "Training - Epoch: 211 loss: 2.293267269643148 accuracy: 12.436666666666667\n",
      "\n",
      "Test set: Average loss: 2.3080, Accuracy: 1173/10000 (12%)\n",
      "\n",
      "Training - Epoch: 212 loss: 2.2932254130045573 accuracy: 12.358333333333333\n",
      "\n",
      "Test set: Average loss: 2.3077, Accuracy: 1177/10000 (12%)\n",
      "\n",
      "Training - Epoch: 213 loss: 2.293186220550537 accuracy: 12.433333333333334\n",
      "\n",
      "Test set: Average loss: 2.3077, Accuracy: 1157/10000 (12%)\n",
      "\n",
      "Training - Epoch: 214 loss: 2.2931614819844564 accuracy: 12.446666666666667\n",
      "\n",
      "Test set: Average loss: 2.3081, Accuracy: 1157/10000 (12%)\n",
      "\n",
      "Training - Epoch: 215 loss: 2.2931169812520347 accuracy: 12.465\n",
      "\n",
      "Test set: Average loss: 2.3079, Accuracy: 1166/10000 (12%)\n",
      "\n",
      "Training - Epoch: 216 loss: 2.2930713556925455 accuracy: 12.481666666666667\n",
      "\n",
      "Test set: Average loss: 2.3077, Accuracy: 1168/10000 (12%)\n",
      "\n",
      "Training - Epoch: 217 loss: 2.2930637835184733 accuracy: 12.561666666666667\n",
      "\n",
      "Test set: Average loss: 2.3081, Accuracy: 1166/10000 (12%)\n",
      "\n",
      "Training - Epoch: 218 loss: 2.2930129689534504 accuracy: 12.536666666666667\n",
      "\n",
      "Test set: Average loss: 2.3081, Accuracy: 1149/10000 (11%)\n",
      "\n",
      "Training - Epoch: 219 loss: 2.2929834660847983 accuracy: 12.508333333333333\n",
      "\n",
      "Test set: Average loss: 2.3081, Accuracy: 1131/10000 (11%)\n",
      "\n",
      "Training - Epoch: 220 loss: 2.2929450805664064 accuracy: 12.498333333333333\n",
      "\n",
      "Test set: Average loss: 2.3081, Accuracy: 1177/10000 (12%)\n",
      "\n",
      "Training - Epoch: 221 loss: 2.292893943532308 accuracy: 12.52\n",
      "\n",
      "Test set: Average loss: 2.3083, Accuracy: 1152/10000 (12%)\n",
      "\n",
      "Training - Epoch: 222 loss: 2.292871219889323 accuracy: 12.521666666666667\n",
      "\n",
      "Test set: Average loss: 2.3083, Accuracy: 1159/10000 (12%)\n",
      "\n",
      "Training - Epoch: 223 loss: 2.292826806640625 accuracy: 12.48\n",
      "\n",
      "Test set: Average loss: 2.3079, Accuracy: 1161/10000 (12%)\n",
      "\n",
      "Training - Epoch: 224 loss: 2.29279596862793 accuracy: 12.57\n",
      "\n",
      "Test set: Average loss: 2.3084, Accuracy: 1176/10000 (12%)\n",
      "\n",
      "Training - Epoch: 225 loss: 2.2927625732421877 accuracy: 12.443333333333333\n",
      "\n",
      "Test set: Average loss: 2.3081, Accuracy: 1163/10000 (12%)\n",
      "\n",
      "Training - Epoch: 226 loss: 2.2927184524536135 accuracy: 12.581666666666667\n",
      "\n",
      "Test set: Average loss: 2.3085, Accuracy: 1178/10000 (12%)\n",
      "\n",
      "Training - Epoch: 227 loss: 2.29269439163208 accuracy: 12.506666666666666\n",
      "\n",
      "Test set: Average loss: 2.3081, Accuracy: 1183/10000 (12%)\n",
      "\n",
      "Training - Epoch: 228 loss: 2.2926568717956544 accuracy: 12.553333333333333\n",
      "\n",
      "Test set: Average loss: 2.3085, Accuracy: 1135/10000 (11%)\n",
      "\n",
      "Training - Epoch: 229 loss: 2.2926254852294923 accuracy: 12.535\n",
      "\n",
      "Test set: Average loss: 2.3083, Accuracy: 1160/10000 (12%)\n",
      "\n",
      "Training - Epoch: 230 loss: 2.29258787206014 accuracy: 12.55\n",
      "\n",
      "Test set: Average loss: 2.3080, Accuracy: 1161/10000 (12%)\n",
      "\n",
      "Training - Epoch: 231 loss: 2.2925447944641113 accuracy: 12.548333333333334\n",
      "\n",
      "Test set: Average loss: 2.3080, Accuracy: 1175/10000 (12%)\n",
      "\n",
      "Training - Epoch: 232 loss: 2.292521103668213 accuracy: 12.643333333333333\n",
      "\n",
      "Test set: Average loss: 2.3082, Accuracy: 1158/10000 (12%)\n",
      "\n",
      "Training - Epoch: 233 loss: 2.292489345041911 accuracy: 12.633333333333333\n",
      "\n",
      "Test set: Average loss: 2.3083, Accuracy: 1152/10000 (12%)\n",
      "\n",
      "Training - Epoch: 234 loss: 2.292442416381836 accuracy: 12.595\n",
      "\n",
      "Test set: Average loss: 2.3081, Accuracy: 1159/10000 (12%)\n",
      "\n",
      "Training - Epoch: 235 loss: 2.292420166269938 accuracy: 12.63\n",
      "\n",
      "Test set: Average loss: 2.3082, Accuracy: 1152/10000 (12%)\n",
      "\n",
      "Training - Epoch: 236 loss: 2.2923773076375324 accuracy: 12.616666666666667\n",
      "\n",
      "Test set: Average loss: 2.3083, Accuracy: 1170/10000 (12%)\n",
      "\n",
      "Training - Epoch: 237 loss: 2.292342726135254 accuracy: 12.723333333333333\n",
      "\n",
      "Test set: Average loss: 2.3086, Accuracy: 1144/10000 (11%)\n",
      "\n",
      "Training - Epoch: 238 loss: 2.2923066062927244 accuracy: 12.638333333333334\n",
      "\n",
      "Test set: Average loss: 2.3085, Accuracy: 1153/10000 (12%)\n",
      "\n",
      "Training - Epoch: 239 loss: 2.29226633199056 accuracy: 12.633333333333333\n",
      "\n",
      "Test set: Average loss: 2.3085, Accuracy: 1173/10000 (12%)\n",
      "\n",
      "Training - Epoch: 240 loss: 2.2922389506022136 accuracy: 12.633333333333333\n",
      "\n",
      "Test set: Average loss: 2.3084, Accuracy: 1165/10000 (12%)\n",
      "\n",
      "Training - Epoch: 241 loss: 2.2922030436197915 accuracy: 12.658333333333333\n",
      "\n",
      "Test set: Average loss: 2.3083, Accuracy: 1147/10000 (11%)\n",
      "\n",
      "Training - Epoch: 242 loss: 2.292163535817464 accuracy: 12.601666666666667\n",
      "\n",
      "Test set: Average loss: 2.3084, Accuracy: 1155/10000 (12%)\n",
      "\n",
      "Training - Epoch: 243 loss: 2.292125114695231 accuracy: 12.723333333333333\n",
      "\n",
      "Test set: Average loss: 2.3088, Accuracy: 1157/10000 (12%)\n",
      "\n",
      "Training - Epoch: 244 loss: 2.2920827896118166 accuracy: 12.631666666666666\n",
      "\n",
      "Test set: Average loss: 2.3083, Accuracy: 1133/10000 (11%)\n",
      "\n",
      "Training - Epoch: 245 loss: 2.292035506184896 accuracy: 12.745\n",
      "\n",
      "Test set: Average loss: 2.3088, Accuracy: 1133/10000 (11%)\n",
      "\n",
      "Training - Epoch: 246 loss: 2.2920178126017254 accuracy: 12.631666666666666\n",
      "\n",
      "Test set: Average loss: 2.3085, Accuracy: 1159/10000 (12%)\n",
      "\n",
      "Training - Epoch: 247 loss: 2.2919786682128906 accuracy: 12.691666666666666\n",
      "\n",
      "Test set: Average loss: 2.3085, Accuracy: 1155/10000 (12%)\n",
      "\n",
      "Training - Epoch: 248 loss: 2.291937975311279 accuracy: 12.616666666666667\n",
      "\n",
      "Test set: Average loss: 2.3083, Accuracy: 1156/10000 (12%)\n",
      "\n",
      "Training - Epoch: 249 loss: 2.291899145762126 accuracy: 12.728333333333333\n",
      "\n",
      "Test set: Average loss: 2.3083, Accuracy: 1153/10000 (12%)\n",
      "\n",
      "Training - Epoch: 250 loss: 2.291860829925537 accuracy: 12.771666666666667\n",
      "\n",
      "Test set: Average loss: 2.3086, Accuracy: 1153/10000 (12%)\n",
      "\n",
      "Training - Epoch: 251 loss: 2.2918384938557943 accuracy: 12.736666666666666\n",
      "\n",
      "Test set: Average loss: 2.3085, Accuracy: 1143/10000 (11%)\n",
      "\n",
      "Training - Epoch: 252 loss: 2.2917909205118816 accuracy: 12.635\n",
      "\n",
      "Test set: Average loss: 2.3082, Accuracy: 1148/10000 (11%)\n",
      "\n",
      "Training - Epoch: 253 loss: 2.2917583201090497 accuracy: 12.786666666666667\n",
      "\n",
      "Test set: Average loss: 2.3087, Accuracy: 1150/10000 (12%)\n",
      "\n",
      "Training - Epoch: 254 loss: 2.291714064534505 accuracy: 12.706666666666667\n",
      "\n",
      "Test set: Average loss: 2.3085, Accuracy: 1115/10000 (11%)\n",
      "\n",
      "Training - Epoch: 255 loss: 2.291687381998698 accuracy: 12.808333333333334\n",
      "\n",
      "Test set: Average loss: 2.3087, Accuracy: 1138/10000 (11%)\n",
      "\n",
      "Training - Epoch: 256 loss: 2.291648770650228 accuracy: 12.72\n",
      "\n",
      "Test set: Average loss: 2.3085, Accuracy: 1129/10000 (11%)\n",
      "\n",
      "Training - Epoch: 257 loss: 2.291614915974935 accuracy: 12.761666666666667\n",
      "\n",
      "Test set: Average loss: 2.3086, Accuracy: 1156/10000 (12%)\n",
      "\n",
      "Training - Epoch: 258 loss: 2.2915804707845053 accuracy: 12.765\n",
      "\n",
      "Test set: Average loss: 2.3088, Accuracy: 1133/10000 (11%)\n",
      "\n",
      "Training - Epoch: 259 loss: 2.291538683827718 accuracy: 12.82\n",
      "\n",
      "Test set: Average loss: 2.3090, Accuracy: 1147/10000 (11%)\n",
      "\n",
      "Training - Epoch: 260 loss: 2.291476114145915 accuracy: 12.718333333333334\n",
      "\n",
      "Test set: Average loss: 2.3087, Accuracy: 1147/10000 (11%)\n",
      "\n",
      "Training - Epoch: 261 loss: 2.2914803802490233 accuracy: 12.815\n",
      "\n",
      "Test set: Average loss: 2.3088, Accuracy: 1131/10000 (11%)\n",
      "\n",
      "Training - Epoch: 262 loss: 2.2914394032796226 accuracy: 12.776666666666667\n",
      "\n",
      "Test set: Average loss: 2.3090, Accuracy: 1132/10000 (11%)\n",
      "\n",
      "Training - Epoch: 263 loss: 2.291394179789225 accuracy: 12.838333333333333\n",
      "\n",
      "Test set: Average loss: 2.3090, Accuracy: 1139/10000 (11%)\n",
      "\n",
      "Training - Epoch: 264 loss: 2.291362218983968 accuracy: 12.815\n",
      "\n",
      "Test set: Average loss: 2.3088, Accuracy: 1118/10000 (11%)\n",
      "\n",
      "Training - Epoch: 265 loss: 2.291314350128174 accuracy: 12.781666666666666\n",
      "\n",
      "Test set: Average loss: 2.3087, Accuracy: 1122/10000 (11%)\n",
      "\n",
      "Training - Epoch: 266 loss: 2.291289830525716 accuracy: 12.933333333333334\n",
      "\n",
      "Test set: Average loss: 2.3089, Accuracy: 1136/10000 (11%)\n",
      "\n",
      "Training - Epoch: 267 loss: 2.291250444793701 accuracy: 12.895\n",
      "\n",
      "Test set: Average loss: 2.3090, Accuracy: 1139/10000 (11%)\n",
      "\n",
      "Training - Epoch: 268 loss: 2.2912059814453123 accuracy: 12.951666666666666\n",
      "\n",
      "Test set: Average loss: 2.3091, Accuracy: 1144/10000 (11%)\n",
      "\n",
      "Training - Epoch: 269 loss: 2.2911620887756348 accuracy: 12.81\n",
      "\n",
      "Test set: Average loss: 2.3089, Accuracy: 1129/10000 (11%)\n",
      "\n",
      "Training - Epoch: 270 loss: 2.2911483561197916 accuracy: 12.78\n",
      "\n",
      "Test set: Average loss: 2.3087, Accuracy: 1110/10000 (11%)\n",
      "\n",
      "Training - Epoch: 271 loss: 2.2910915321350096 accuracy: 12.871666666666666\n",
      "\n",
      "Test set: Average loss: 2.3087, Accuracy: 1143/10000 (11%)\n",
      "\n",
      "Training - Epoch: 272 loss: 2.291066360727946 accuracy: 12.845\n",
      "\n",
      "Test set: Average loss: 2.3091, Accuracy: 1122/10000 (11%)\n",
      "\n",
      "Training - Epoch: 273 loss: 2.291019320678711 accuracy: 12.928333333333333\n",
      "\n",
      "Test set: Average loss: 2.3090, Accuracy: 1140/10000 (11%)\n",
      "\n",
      "Training - Epoch: 274 loss: 2.290987232208252 accuracy: 12.843333333333334\n",
      "\n",
      "Test set: Average loss: 2.3088, Accuracy: 1139/10000 (11%)\n",
      "\n",
      "Training - Epoch: 275 loss: 2.2909569012959796 accuracy: 12.885\n",
      "\n",
      "Test set: Average loss: 2.3089, Accuracy: 1118/10000 (11%)\n",
      "\n",
      "Training - Epoch: 276 loss: 2.290916359202067 accuracy: 12.9\n",
      "\n",
      "Test set: Average loss: 2.3090, Accuracy: 1112/10000 (11%)\n",
      "\n",
      "Training - Epoch: 277 loss: 2.290877995300293 accuracy: 12.968333333333334\n",
      "\n",
      "Test set: Average loss: 2.3090, Accuracy: 1141/10000 (11%)\n",
      "\n",
      "Training - Epoch: 278 loss: 2.2908337137858075 accuracy: 12.926666666666666\n",
      "\n",
      "Test set: Average loss: 2.3089, Accuracy: 1136/10000 (11%)\n",
      "\n",
      "Training - Epoch: 279 loss: 2.2908035133361815 accuracy: 12.98\n",
      "\n",
      "Test set: Average loss: 2.3092, Accuracy: 1125/10000 (11%)\n",
      "\n",
      "Training - Epoch: 280 loss: 2.290766094970703 accuracy: 12.943333333333333\n",
      "\n",
      "Test set: Average loss: 2.3092, Accuracy: 1145/10000 (11%)\n",
      "\n",
      "Training - Epoch: 281 loss: 2.2907233207702635 accuracy: 12.96\n",
      "\n",
      "Test set: Average loss: 2.3089, Accuracy: 1137/10000 (11%)\n",
      "\n",
      "Training - Epoch: 282 loss: 2.2906917449951174 accuracy: 12.938333333333333\n",
      "\n",
      "Test set: Average loss: 2.3092, Accuracy: 1126/10000 (11%)\n",
      "\n",
      "Training - Epoch: 283 loss: 2.290653544108073 accuracy: 12.976666666666667\n",
      "\n",
      "Test set: Average loss: 2.3094, Accuracy: 1121/10000 (11%)\n",
      "\n",
      "Training - Epoch: 284 loss: 2.290616744740804 accuracy: 12.973333333333333\n",
      "\n",
      "Test set: Average loss: 2.3089, Accuracy: 1130/10000 (11%)\n",
      "\n",
      "Training - Epoch: 285 loss: 2.290585046641032 accuracy: 12.956666666666667\n",
      "\n",
      "Test set: Average loss: 2.3092, Accuracy: 1146/10000 (11%)\n",
      "\n",
      "Training - Epoch: 286 loss: 2.290544315846761 accuracy: 13.008333333333333\n",
      "\n",
      "Test set: Average loss: 2.3094, Accuracy: 1131/10000 (11%)\n",
      "\n",
      "Training - Epoch: 287 loss: 2.290517739868164 accuracy: 12.953333333333333\n",
      "\n",
      "Test set: Average loss: 2.3091, Accuracy: 1121/10000 (11%)\n",
      "\n",
      "Training - Epoch: 288 loss: 2.290459046681722 accuracy: 13.101666666666667\n",
      "\n",
      "Test set: Average loss: 2.3095, Accuracy: 1127/10000 (11%)\n",
      "\n",
      "Training - Epoch: 289 loss: 2.2904343152364097 accuracy: 12.988333333333333\n",
      "\n",
      "Test set: Average loss: 2.3095, Accuracy: 1117/10000 (11%)\n",
      "\n",
      "Training - Epoch: 290 loss: 2.2903957389831544 accuracy: 13.048333333333334\n",
      "\n",
      "Test set: Average loss: 2.3091, Accuracy: 1134/10000 (11%)\n",
      "\n",
      "Training - Epoch: 291 loss: 2.290363952891032 accuracy: 13.023333333333333\n",
      "\n",
      "Test set: Average loss: 2.3093, Accuracy: 1138/10000 (11%)\n",
      "\n",
      "Training - Epoch: 292 loss: 2.290320516459147 accuracy: 13.083333333333334\n",
      "\n",
      "Test set: Average loss: 2.3094, Accuracy: 1133/10000 (11%)\n",
      "\n",
      "Training - Epoch: 293 loss: 2.2902870750427247 accuracy: 13.056666666666667\n",
      "\n",
      "Test set: Average loss: 2.3094, Accuracy: 1140/10000 (11%)\n",
      "\n",
      "Training - Epoch: 294 loss: 2.2902495109558108 accuracy: 13.048333333333334\n",
      "\n",
      "Test set: Average loss: 2.3091, Accuracy: 1143/10000 (11%)\n",
      "\n",
      "Training - Epoch: 295 loss: 2.2902157376607257 accuracy: 13.025\n",
      "\n",
      "Test set: Average loss: 2.3090, Accuracy: 1147/10000 (11%)\n",
      "\n",
      "Training - Epoch: 296 loss: 2.290175554402669 accuracy: 13.105\n",
      "\n",
      "Test set: Average loss: 2.3094, Accuracy: 1129/10000 (11%)\n",
      "\n",
      "Training - Epoch: 297 loss: 2.2901308433532717 accuracy: 13.101666666666667\n",
      "\n",
      "Test set: Average loss: 2.3095, Accuracy: 1116/10000 (11%)\n",
      "\n",
      "Training - Epoch: 298 loss: 2.290108173116048 accuracy: 13.088333333333333\n",
      "\n",
      "Test set: Average loss: 2.3094, Accuracy: 1116/10000 (11%)\n",
      "\n",
      "Training - Epoch: 299 loss: 2.290067997233073 accuracy: 13.116666666666667\n",
      "\n",
      "Test set: Average loss: 2.3095, Accuracy: 1134/10000 (11%)\n",
      "\n",
      "Training - Epoch: 300 loss: 2.2900321746826173 accuracy: 13.101666666666667\n",
      "\n",
      "Test set: Average loss: 2.3094, Accuracy: 1144/10000 (11%)\n",
      "\n",
      "Training - Epoch: 301 loss: 2.2899864926656086 accuracy: 13.086666666666666\n",
      "\n",
      "Test set: Average loss: 2.3096, Accuracy: 1138/10000 (11%)\n",
      "\n",
      "Training - Epoch: 302 loss: 2.2899548060099284 accuracy: 13.076666666666666\n",
      "\n",
      "Test set: Average loss: 2.3094, Accuracy: 1133/10000 (11%)\n",
      "\n",
      "Training - Epoch: 303 loss: 2.2899062561035155 accuracy: 13.036666666666667\n",
      "\n",
      "Test set: Average loss: 2.3096, Accuracy: 1133/10000 (11%)\n",
      "\n",
      "Training - Epoch: 304 loss: 2.289864352162679 accuracy: 13.088333333333333\n",
      "\n",
      "Test set: Average loss: 2.3097, Accuracy: 1132/10000 (11%)\n",
      "\n",
      "Training - Epoch: 305 loss: 2.2898564715067544 accuracy: 13.09\n",
      "\n",
      "Test set: Average loss: 2.3095, Accuracy: 1149/10000 (11%)\n",
      "\n",
      "Training - Epoch: 306 loss: 2.2898049774169924 accuracy: 13.141666666666667\n",
      "\n",
      "Test set: Average loss: 2.3096, Accuracy: 1150/10000 (12%)\n",
      "\n",
      "Training - Epoch: 307 loss: 2.2897565691630044 accuracy: 13.135\n",
      "\n",
      "Test set: Average loss: 2.3096, Accuracy: 1146/10000 (11%)\n",
      "\n",
      "Training - Epoch: 308 loss: 2.2897316276550295 accuracy: 13.13\n",
      "\n",
      "Test set: Average loss: 2.3096, Accuracy: 1132/10000 (11%)\n",
      "\n",
      "Training - Epoch: 309 loss: 2.2896954093933104 accuracy: 13.145\n",
      "\n",
      "Test set: Average loss: 2.3094, Accuracy: 1152/10000 (12%)\n",
      "\n",
      "Training - Epoch: 310 loss: 2.2896513631184896 accuracy: 13.206666666666667\n",
      "\n",
      "Test set: Average loss: 2.3094, Accuracy: 1150/10000 (12%)\n",
      "\n",
      "Training - Epoch: 311 loss: 2.2896149859110513 accuracy: 13.213333333333333\n",
      "\n",
      "Test set: Average loss: 2.3095, Accuracy: 1150/10000 (12%)\n",
      "\n",
      "Training - Epoch: 312 loss: 2.289574310048421 accuracy: 13.155\n",
      "\n",
      "Test set: Average loss: 2.3096, Accuracy: 1159/10000 (12%)\n",
      "\n",
      "Training - Epoch: 313 loss: 2.2895212814331054 accuracy: 13.145\n",
      "\n",
      "Test set: Average loss: 2.3093, Accuracy: 1118/10000 (11%)\n",
      "\n",
      "Training - Epoch: 314 loss: 2.2895140126546223 accuracy: 13.145\n",
      "\n",
      "Test set: Average loss: 2.3096, Accuracy: 1131/10000 (11%)\n",
      "\n",
      "Training - Epoch: 315 loss: 2.2894514727274577 accuracy: 13.185\n",
      "\n",
      "Test set: Average loss: 2.3098, Accuracy: 1137/10000 (11%)\n",
      "\n",
      "Training - Epoch: 316 loss: 2.289434378560384 accuracy: 13.175\n",
      "\n",
      "Test set: Average loss: 2.3094, Accuracy: 1135/10000 (11%)\n",
      "\n",
      "Training - Epoch: 317 loss: 2.2893867602030435 accuracy: 13.22\n",
      "\n",
      "Test set: Average loss: 2.3096, Accuracy: 1137/10000 (11%)\n",
      "\n",
      "Training - Epoch: 318 loss: 2.289344966634115 accuracy: 13.246666666666666\n",
      "\n",
      "Test set: Average loss: 2.3098, Accuracy: 1146/10000 (11%)\n",
      "\n",
      "Training - Epoch: 319 loss: 2.289312963612874 accuracy: 13.141666666666667\n",
      "\n",
      "Test set: Average loss: 2.3097, Accuracy: 1152/10000 (12%)\n",
      "\n",
      "Training - Epoch: 320 loss: 2.2892744827270506 accuracy: 13.118333333333334\n",
      "\n",
      "Test set: Average loss: 2.3095, Accuracy: 1126/10000 (11%)\n",
      "\n",
      "Training - Epoch: 321 loss: 2.289235290781657 accuracy: 13.298333333333334\n",
      "\n",
      "Test set: Average loss: 2.3094, Accuracy: 1139/10000 (11%)\n",
      "\n",
      "Training - Epoch: 322 loss: 2.289197993214925 accuracy: 13.198333333333334\n",
      "\n",
      "Test set: Average loss: 2.3099, Accuracy: 1138/10000 (11%)\n",
      "\n",
      "Training - Epoch: 323 loss: 2.2891568603515626 accuracy: 13.27\n",
      "\n",
      "Test set: Average loss: 2.3100, Accuracy: 1147/10000 (11%)\n",
      "\n",
      "Training - Epoch: 324 loss: 2.2891325780232745 accuracy: 13.211666666666666\n",
      "\n",
      "Test set: Average loss: 2.3098, Accuracy: 1130/10000 (11%)\n",
      "\n",
      "Training - Epoch: 325 loss: 2.289086213684082 accuracy: 13.188333333333333\n",
      "\n",
      "Test set: Average loss: 2.3098, Accuracy: 1137/10000 (11%)\n",
      "\n",
      "Training - Epoch: 326 loss: 2.2890641321818035 accuracy: 13.245\n",
      "\n",
      "Test set: Average loss: 2.3100, Accuracy: 1146/10000 (11%)\n",
      "\n",
      "Training - Epoch: 327 loss: 2.2890106384277344 accuracy: 13.241666666666667\n",
      "\n",
      "Test set: Average loss: 2.3102, Accuracy: 1139/10000 (11%)\n",
      "\n",
      "Training - Epoch: 328 loss: 2.2889823211669924 accuracy: 13.218333333333334\n",
      "\n",
      "Test set: Average loss: 2.3098, Accuracy: 1128/10000 (11%)\n",
      "\n",
      "Training - Epoch: 329 loss: 2.2889404393514 accuracy: 13.278333333333334\n",
      "\n",
      "Test set: Average loss: 2.3100, Accuracy: 1135/10000 (11%)\n",
      "\n",
      "Training - Epoch: 330 loss: 2.2889039362589516 accuracy: 13.246666666666666\n",
      "\n",
      "Test set: Average loss: 2.3100, Accuracy: 1145/10000 (11%)\n",
      "\n",
      "Training - Epoch: 331 loss: 2.288858154805501 accuracy: 13.25\n",
      "\n",
      "Test set: Average loss: 2.3099, Accuracy: 1128/10000 (11%)\n",
      "\n",
      "Training - Epoch: 332 loss: 2.288829864501953 accuracy: 13.338333333333333\n",
      "\n",
      "Test set: Average loss: 2.3102, Accuracy: 1148/10000 (11%)\n",
      "\n",
      "Training - Epoch: 333 loss: 2.2887829162597657 accuracy: 13.291666666666666\n",
      "\n",
      "Test set: Average loss: 2.3099, Accuracy: 1126/10000 (11%)\n",
      "\n",
      "Training - Epoch: 334 loss: 2.2887435068766275 accuracy: 13.306666666666667\n",
      "\n",
      "Test set: Average loss: 2.3100, Accuracy: 1138/10000 (11%)\n",
      "\n",
      "Training - Epoch: 335 loss: 2.288720021565755 accuracy: 13.263333333333334\n",
      "\n",
      "Test set: Average loss: 2.3099, Accuracy: 1131/10000 (11%)\n",
      "\n",
      "Training - Epoch: 336 loss: 2.2886650115966796 accuracy: 13.316666666666666\n",
      "\n",
      "Test set: Average loss: 2.3098, Accuracy: 1117/10000 (11%)\n",
      "\n",
      "Training - Epoch: 337 loss: 2.288639488474528 accuracy: 13.28\n",
      "\n",
      "Test set: Average loss: 2.3099, Accuracy: 1118/10000 (11%)\n",
      "\n",
      "Training - Epoch: 338 loss: 2.2885991137186688 accuracy: 13.31\n",
      "\n",
      "Test set: Average loss: 2.3099, Accuracy: 1139/10000 (11%)\n",
      "\n",
      "Training - Epoch: 339 loss: 2.288559275563558 accuracy: 13.321666666666667\n",
      "\n",
      "Test set: Average loss: 2.3099, Accuracy: 1120/10000 (11%)\n",
      "\n",
      "Training - Epoch: 340 loss: 2.288516013844808 accuracy: 13.346666666666666\n",
      "\n",
      "Test set: Average loss: 2.3103, Accuracy: 1133/10000 (11%)\n",
      "\n",
      "Training - Epoch: 341 loss: 2.2884916839599607 accuracy: 13.378333333333334\n",
      "\n",
      "Test set: Average loss: 2.3100, Accuracy: 1132/10000 (11%)\n",
      "\n",
      "Training - Epoch: 342 loss: 2.288436270650228 accuracy: 13.383333333333333\n",
      "\n",
      "Test set: Average loss: 2.3102, Accuracy: 1138/10000 (11%)\n",
      "\n",
      "Training - Epoch: 343 loss: 2.2884049072265626 accuracy: 13.32\n",
      "\n",
      "Test set: Average loss: 2.3101, Accuracy: 1127/10000 (11%)\n",
      "\n",
      "Training - Epoch: 344 loss: 2.288360615285238 accuracy: 13.323333333333334\n",
      "\n",
      "Test set: Average loss: 2.3100, Accuracy: 1129/10000 (11%)\n",
      "\n",
      "Training - Epoch: 345 loss: 2.2883255264282227 accuracy: 13.385\n",
      "\n",
      "Test set: Average loss: 2.3105, Accuracy: 1150/10000 (12%)\n",
      "\n",
      "Training - Epoch: 346 loss: 2.2882796160380043 accuracy: 13.363333333333333\n",
      "\n",
      "Test set: Average loss: 2.3103, Accuracy: 1142/10000 (11%)\n",
      "\n",
      "Training - Epoch: 347 loss: 2.2882556007385255 accuracy: 13.356666666666667\n",
      "\n",
      "Test set: Average loss: 2.3102, Accuracy: 1135/10000 (11%)\n",
      "\n",
      "Training - Epoch: 348 loss: 2.2882049057006837 accuracy: 13.378333333333334\n",
      "\n",
      "Test set: Average loss: 2.3102, Accuracy: 1128/10000 (11%)\n",
      "\n",
      "Training - Epoch: 349 loss: 2.2881606875101723 accuracy: 13.425\n",
      "\n",
      "Test set: Average loss: 2.3101, Accuracy: 1129/10000 (11%)\n",
      "\n",
      "Training - Epoch: 350 loss: 2.288135403951009 accuracy: 13.41\n",
      "\n",
      "Test set: Average loss: 2.3104, Accuracy: 1133/10000 (11%)\n",
      "\n",
      "Training - Epoch: 351 loss: 2.288097632598877 accuracy: 13.348333333333333\n",
      "\n",
      "Test set: Average loss: 2.3104, Accuracy: 1126/10000 (11%)\n",
      "\n",
      "Training - Epoch: 352 loss: 2.288047512817383 accuracy: 13.336666666666666\n",
      "\n",
      "Test set: Average loss: 2.3102, Accuracy: 1114/10000 (11%)\n",
      "\n",
      "Training - Epoch: 353 loss: 2.288011543782552 accuracy: 13.425\n",
      "\n",
      "Test set: Average loss: 2.3102, Accuracy: 1129/10000 (11%)\n",
      "\n",
      "Training - Epoch: 354 loss: 2.287971310933431 accuracy: 13.411666666666667\n",
      "\n",
      "Test set: Average loss: 2.3102, Accuracy: 1146/10000 (11%)\n",
      "\n",
      "Training - Epoch: 355 loss: 2.2879453267415366 accuracy: 13.43\n",
      "\n",
      "Test set: Average loss: 2.3103, Accuracy: 1126/10000 (11%)\n",
      "\n",
      "Training - Epoch: 356 loss: 2.287899582417806 accuracy: 13.368333333333334\n",
      "\n",
      "Test set: Average loss: 2.3102, Accuracy: 1120/10000 (11%)\n",
      "\n",
      "Training - Epoch: 357 loss: 2.287863682047526 accuracy: 13.405\n",
      "\n",
      "Test set: Average loss: 2.3104, Accuracy: 1125/10000 (11%)\n",
      "\n",
      "Training - Epoch: 358 loss: 2.2878340393066408 accuracy: 13.42\n",
      "\n",
      "Test set: Average loss: 2.3102, Accuracy: 1125/10000 (11%)\n",
      "\n",
      "Training - Epoch: 359 loss: 2.287769847869873 accuracy: 13.37\n",
      "\n",
      "Test set: Average loss: 2.3100, Accuracy: 1096/10000 (11%)\n",
      "\n",
      "Training - Epoch: 360 loss: 2.287752270762126 accuracy: 13.453333333333333\n",
      "\n",
      "Test set: Average loss: 2.3106, Accuracy: 1121/10000 (11%)\n",
      "\n",
      "Training - Epoch: 361 loss: 2.2877004931131997 accuracy: 13.45\n",
      "\n",
      "Test set: Average loss: 2.3102, Accuracy: 1123/10000 (11%)\n",
      "\n",
      "Training - Epoch: 362 loss: 2.287656863149007 accuracy: 13.463333333333333\n",
      "\n",
      "Test set: Average loss: 2.3101, Accuracy: 1089/10000 (11%)\n",
      "\n",
      "Training - Epoch: 363 loss: 2.2876275293986 accuracy: 13.456666666666667\n",
      "\n",
      "Test set: Average loss: 2.3102, Accuracy: 1110/10000 (11%)\n",
      "\n",
      "Training - Epoch: 364 loss: 2.2875907587687174 accuracy: 13.458333333333334\n",
      "\n",
      "Test set: Average loss: 2.3106, Accuracy: 1125/10000 (11%)\n",
      "\n",
      "Training - Epoch: 365 loss: 2.287539016977946 accuracy: 13.403333333333334\n",
      "\n",
      "Test set: Average loss: 2.3107, Accuracy: 1141/10000 (11%)\n",
      "\n",
      "Training - Epoch: 366 loss: 2.287511921183268 accuracy: 13.448333333333334\n",
      "\n",
      "Test set: Average loss: 2.3106, Accuracy: 1128/10000 (11%)\n",
      "\n",
      "Training - Epoch: 367 loss: 2.287477029164632 accuracy: 13.476666666666667\n",
      "\n",
      "Test set: Average loss: 2.3105, Accuracy: 1126/10000 (11%)\n",
      "\n",
      "Training - Epoch: 368 loss: 2.287430399576823 accuracy: 13.468333333333334\n",
      "\n",
      "Test set: Average loss: 2.3104, Accuracy: 1118/10000 (11%)\n",
      "\n",
      "Training - Epoch: 369 loss: 2.2874010203043618 accuracy: 13.528333333333334\n",
      "\n",
      "Test set: Average loss: 2.3104, Accuracy: 1112/10000 (11%)\n",
      "\n",
      "Training - Epoch: 370 loss: 2.2873536608378093 accuracy: 13.418333333333333\n",
      "\n",
      "Test set: Average loss: 2.3108, Accuracy: 1119/10000 (11%)\n",
      "\n",
      "Training - Epoch: 371 loss: 2.2873121187845866 accuracy: 13.473333333333333\n",
      "\n",
      "Test set: Average loss: 2.3111, Accuracy: 1122/10000 (11%)\n",
      "\n",
      "Training - Epoch: 372 loss: 2.2872851468404134 accuracy: 13.433333333333334\n",
      "\n",
      "Test set: Average loss: 2.3105, Accuracy: 1104/10000 (11%)\n",
      "\n",
      "Training - Epoch: 373 loss: 2.2872402684529622 accuracy: 13.486666666666666\n",
      "\n",
      "Test set: Average loss: 2.3106, Accuracy: 1100/10000 (11%)\n",
      "\n",
      "Training - Epoch: 374 loss: 2.287201404825846 accuracy: 13.528333333333334\n",
      "\n",
      "Test set: Average loss: 2.3104, Accuracy: 1105/10000 (11%)\n",
      "\n",
      "Training - Epoch: 375 loss: 2.2871551973978677 accuracy: 13.458333333333334\n",
      "\n",
      "Test set: Average loss: 2.3107, Accuracy: 1120/10000 (11%)\n",
      "\n",
      "Training - Epoch: 376 loss: 2.2871113975524904 accuracy: 13.491666666666667\n",
      "\n",
      "Test set: Average loss: 2.3108, Accuracy: 1115/10000 (11%)\n",
      "\n",
      "Training - Epoch: 377 loss: 2.287087390645345 accuracy: 13.543333333333333\n",
      "\n",
      "Test set: Average loss: 2.3108, Accuracy: 1104/10000 (11%)\n",
      "\n",
      "Training - Epoch: 378 loss: 2.2870468266805015 accuracy: 13.623333333333333\n",
      "\n",
      "Test set: Average loss: 2.3110, Accuracy: 1119/10000 (11%)\n",
      "\n",
      "Training - Epoch: 379 loss: 2.2870044766743978 accuracy: 13.525\n",
      "\n",
      "Test set: Average loss: 2.3108, Accuracy: 1115/10000 (11%)\n",
      "\n",
      "Training - Epoch: 380 loss: 2.2869609629313152 accuracy: 13.481666666666667\n",
      "\n",
      "Test set: Average loss: 2.3103, Accuracy: 1078/10000 (11%)\n",
      "\n",
      "Training - Epoch: 381 loss: 2.28692179107666 accuracy: 13.518333333333333\n",
      "\n",
      "Test set: Average loss: 2.3105, Accuracy: 1087/10000 (11%)\n",
      "\n",
      "Training - Epoch: 382 loss: 2.2868769284566244 accuracy: 13.55\n",
      "\n",
      "Test set: Average loss: 2.3104, Accuracy: 1099/10000 (11%)\n",
      "\n",
      "Training - Epoch: 383 loss: 2.2868544057210287 accuracy: 13.531666666666666\n",
      "\n",
      "Test set: Average loss: 2.3108, Accuracy: 1110/10000 (11%)\n",
      "\n",
      "Training - Epoch: 384 loss: 2.2867882537841795 accuracy: 13.525\n",
      "\n",
      "Test set: Average loss: 2.3106, Accuracy: 1079/10000 (11%)\n",
      "\n",
      "Training - Epoch: 385 loss: 2.286775934346517 accuracy: 13.665\n",
      "\n",
      "Test set: Average loss: 2.3112, Accuracy: 1117/10000 (11%)\n",
      "\n",
      "Training - Epoch: 386 loss: 2.286723677062988 accuracy: 13.478333333333333\n",
      "\n",
      "Test set: Average loss: 2.3110, Accuracy: 1119/10000 (11%)\n",
      "\n",
      "Training - Epoch: 387 loss: 2.286686386871338 accuracy: 13.563333333333333\n",
      "\n",
      "Test set: Average loss: 2.3108, Accuracy: 1105/10000 (11%)\n",
      "\n",
      "Training - Epoch: 388 loss: 2.286640264638265 accuracy: 13.533333333333333\n",
      "\n",
      "Test set: Average loss: 2.3108, Accuracy: 1079/10000 (11%)\n",
      "\n",
      "Training - Epoch: 389 loss: 2.2866139945983885 accuracy: 13.608333333333333\n",
      "\n",
      "Test set: Average loss: 2.3110, Accuracy: 1117/10000 (11%)\n",
      "\n",
      "Training - Epoch: 390 loss: 2.2865676811218263 accuracy: 13.625\n",
      "\n",
      "Test set: Average loss: 2.3109, Accuracy: 1101/10000 (11%)\n",
      "\n",
      "Training - Epoch: 391 loss: 2.2865258862813316 accuracy: 13.55\n",
      "\n",
      "Test set: Average loss: 2.3108, Accuracy: 1099/10000 (11%)\n",
      "\n",
      "Training - Epoch: 392 loss: 2.286476709493001 accuracy: 13.618333333333334\n",
      "\n",
      "Test set: Average loss: 2.3109, Accuracy: 1125/10000 (11%)\n",
      "\n",
      "Training - Epoch: 393 loss: 2.286459470876058 accuracy: 13.635\n",
      "\n",
      "Test set: Average loss: 2.3108, Accuracy: 1133/10000 (11%)\n",
      "\n",
      "Training - Epoch: 394 loss: 2.2864105265299477 accuracy: 13.501666666666667\n",
      "\n",
      "Test set: Average loss: 2.3107, Accuracy: 1095/10000 (11%)\n",
      "\n",
      "Training - Epoch: 395 loss: 2.2863714416503904 accuracy: 13.695\n",
      "\n",
      "Test set: Average loss: 2.3111, Accuracy: 1106/10000 (11%)\n",
      "\n",
      "Training - Epoch: 396 loss: 2.2863315287272137 accuracy: 13.643333333333333\n",
      "\n",
      "Test set: Average loss: 2.3108, Accuracy: 1112/10000 (11%)\n",
      "\n",
      "Training - Epoch: 397 loss: 2.2863029792785645 accuracy: 13.56\n",
      "\n",
      "Test set: Average loss: 2.3110, Accuracy: 1112/10000 (11%)\n",
      "\n",
      "Training - Epoch: 398 loss: 2.28623829981486 accuracy: 13.623333333333333\n",
      "\n",
      "Test set: Average loss: 2.3107, Accuracy: 1080/10000 (11%)\n",
      "\n",
      "Training - Epoch: 399 loss: 2.2862172419230142 accuracy: 13.65\n",
      "\n",
      "Test set: Average loss: 2.3108, Accuracy: 1083/10000 (11%)\n",
      "\n",
      "Training - Epoch: 400 loss: 2.286183852895101 accuracy: 13.568333333333333\n",
      "\n",
      "Test set: Average loss: 2.3109, Accuracy: 1088/10000 (11%)\n",
      "\n",
      "Training - Epoch: 401 loss: 2.286134631347656 accuracy: 13.626666666666667\n",
      "\n",
      "Test set: Average loss: 2.3107, Accuracy: 1092/10000 (11%)\n",
      "\n",
      "Training - Epoch: 402 loss: 2.286100040435791 accuracy: 13.653333333333334\n",
      "\n",
      "Test set: Average loss: 2.3112, Accuracy: 1120/10000 (11%)\n",
      "\n",
      "Training - Epoch: 403 loss: 2.2860708663940428 accuracy: 13.715\n",
      "\n",
      "Test set: Average loss: 2.3111, Accuracy: 1104/10000 (11%)\n",
      "\n",
      "Training - Epoch: 404 loss: 2.2860173853556316 accuracy: 13.678333333333333\n",
      "\n",
      "Test set: Average loss: 2.3111, Accuracy: 1104/10000 (11%)\n",
      "\n",
      "Training - Epoch: 405 loss: 2.285975633239746 accuracy: 13.698333333333334\n",
      "\n",
      "Test set: Average loss: 2.3115, Accuracy: 1100/10000 (11%)\n",
      "\n",
      "Training - Epoch: 406 loss: 2.2859391929626467 accuracy: 13.65\n",
      "\n",
      "Test set: Average loss: 2.3112, Accuracy: 1096/10000 (11%)\n",
      "\n",
      "Training - Epoch: 407 loss: 2.2859053316752114 accuracy: 13.78\n",
      "\n",
      "Test set: Average loss: 2.3117, Accuracy: 1119/10000 (11%)\n",
      "\n",
      "Training - Epoch: 408 loss: 2.285865208689372 accuracy: 13.703333333333333\n",
      "\n",
      "Test set: Average loss: 2.3110, Accuracy: 1111/10000 (11%)\n",
      "\n",
      "Training - Epoch: 409 loss: 2.285813256327311 accuracy: 13.561666666666667\n",
      "\n",
      "Test set: Average loss: 2.3106, Accuracy: 1079/10000 (11%)\n",
      "\n",
      "Training - Epoch: 410 loss: 2.2857805681864423 accuracy: 13.736666666666666\n",
      "\n",
      "Test set: Average loss: 2.3108, Accuracy: 1082/10000 (11%)\n",
      "\n",
      "Training - Epoch: 411 loss: 2.2857377535502117 accuracy: 13.753333333333334\n",
      "\n",
      "Test set: Average loss: 2.3110, Accuracy: 1089/10000 (11%)\n",
      "\n",
      "Training - Epoch: 412 loss: 2.2856949076334634 accuracy: 13.77\n",
      "\n",
      "Test set: Average loss: 2.3112, Accuracy: 1106/10000 (11%)\n",
      "\n",
      "Training - Epoch: 413 loss: 2.2856643483479817 accuracy: 13.761666666666667\n",
      "\n",
      "Test set: Average loss: 2.3110, Accuracy: 1124/10000 (11%)\n",
      "\n",
      "Training - Epoch: 414 loss: 2.285626533762614 accuracy: 13.653333333333334\n",
      "\n",
      "Test set: Average loss: 2.3114, Accuracy: 1100/10000 (11%)\n",
      "\n",
      "Training - Epoch: 415 loss: 2.2855899915059408 accuracy: 13.746666666666666\n",
      "\n",
      "Test set: Average loss: 2.3112, Accuracy: 1098/10000 (11%)\n",
      "\n",
      "Training - Epoch: 416 loss: 2.2855560887654622 accuracy: 13.751666666666667\n",
      "\n",
      "Test set: Average loss: 2.3113, Accuracy: 1093/10000 (11%)\n",
      "\n",
      "Training - Epoch: 417 loss: 2.2854905634562175 accuracy: 13.758333333333333\n",
      "\n",
      "Test set: Average loss: 2.3109, Accuracy: 1106/10000 (11%)\n",
      "\n",
      "Training - Epoch: 418 loss: 2.28547976659139 accuracy: 13.743333333333334\n",
      "\n",
      "Test set: Average loss: 2.3111, Accuracy: 1089/10000 (11%)\n",
      "\n",
      "Training - Epoch: 419 loss: 2.285422282409668 accuracy: 13.766666666666667\n",
      "\n",
      "Test set: Average loss: 2.3110, Accuracy: 1081/10000 (11%)\n",
      "\n",
      "Training - Epoch: 420 loss: 2.2853949867248535 accuracy: 13.833333333333334\n",
      "\n",
      "Test set: Average loss: 2.3111, Accuracy: 1096/10000 (11%)\n",
      "\n",
      "Training - Epoch: 421 loss: 2.285342999013265 accuracy: 13.753333333333334\n",
      "\n",
      "Test set: Average loss: 2.3108, Accuracy: 1081/10000 (11%)\n",
      "\n",
      "Training - Epoch: 422 loss: 2.2853061953226725 accuracy: 13.808333333333334\n",
      "\n",
      "Test set: Average loss: 2.3113, Accuracy: 1084/10000 (11%)\n",
      "\n",
      "Training - Epoch: 423 loss: 2.285261404418945 accuracy: 13.763333333333334\n",
      "\n",
      "Test set: Average loss: 2.3114, Accuracy: 1085/10000 (11%)\n",
      "\n",
      "Training - Epoch: 424 loss: 2.2852304893493653 accuracy: 13.725\n",
      "\n",
      "Test set: Average loss: 2.3110, Accuracy: 1090/10000 (11%)\n",
      "\n",
      "Training - Epoch: 425 loss: 2.2851834246317546 accuracy: 13.843333333333334\n",
      "\n",
      "Test set: Average loss: 2.3117, Accuracy: 1114/10000 (11%)\n",
      "\n",
      "Training - Epoch: 426 loss: 2.2851487459818522 accuracy: 13.826666666666666\n",
      "\n",
      "Test set: Average loss: 2.3110, Accuracy: 1110/10000 (11%)\n",
      "\n",
      "Training - Epoch: 427 loss: 2.285116394297282 accuracy: 13.806666666666667\n",
      "\n",
      "Test set: Average loss: 2.3110, Accuracy: 1098/10000 (11%)\n",
      "\n",
      "Training - Epoch: 428 loss: 2.2850749725341797 accuracy: 13.798333333333334\n",
      "\n",
      "Test set: Average loss: 2.3111, Accuracy: 1083/10000 (11%)\n",
      "\n",
      "Training - Epoch: 429 loss: 2.285034350077311 accuracy: 13.811666666666667\n",
      "\n",
      "Test set: Average loss: 2.3114, Accuracy: 1086/10000 (11%)\n",
      "\n",
      "Training - Epoch: 430 loss: 2.2849951655069987 accuracy: 13.811666666666667\n",
      "\n",
      "Test set: Average loss: 2.3110, Accuracy: 1088/10000 (11%)\n",
      "\n",
      "Training - Epoch: 431 loss: 2.284948710378011 accuracy: 13.898333333333333\n",
      "\n",
      "Test set: Average loss: 2.3115, Accuracy: 1089/10000 (11%)\n",
      "\n",
      "Training - Epoch: 432 loss: 2.2849082374572753 accuracy: 13.831666666666667\n",
      "\n",
      "Test set: Average loss: 2.3110, Accuracy: 1079/10000 (11%)\n",
      "\n",
      "Training - Epoch: 433 loss: 2.2848788441975914 accuracy: 13.825\n",
      "\n",
      "Test set: Average loss: 2.3114, Accuracy: 1087/10000 (11%)\n",
      "\n",
      "Training - Epoch: 434 loss: 2.2848293251037597 accuracy: 13.84\n",
      "\n",
      "Test set: Average loss: 2.3110, Accuracy: 1087/10000 (11%)\n",
      "\n",
      "Training - Epoch: 435 loss: 2.284788542175293 accuracy: 13.91\n",
      "\n",
      "Test set: Average loss: 2.3116, Accuracy: 1114/10000 (11%)\n",
      "\n",
      "Training - Epoch: 436 loss: 2.284748585764567 accuracy: 13.82\n",
      "\n",
      "Test set: Average loss: 2.3115, Accuracy: 1119/10000 (11%)\n",
      "\n",
      "Training - Epoch: 437 loss: 2.2846962529500328 accuracy: 13.893333333333333\n",
      "\n",
      "Test set: Average loss: 2.3116, Accuracy: 1093/10000 (11%)\n",
      "\n",
      "Training - Epoch: 438 loss: 2.284674824523926 accuracy: 13.898333333333333\n",
      "\n",
      "Test set: Average loss: 2.3113, Accuracy: 1079/10000 (11%)\n",
      "\n",
      "Training - Epoch: 439 loss: 2.2846255327860514 accuracy: 13.901666666666667\n",
      "\n",
      "Test set: Average loss: 2.3113, Accuracy: 1066/10000 (11%)\n",
      "\n",
      "Training - Epoch: 440 loss: 2.284580837504069 accuracy: 13.926666666666666\n",
      "\n",
      "Test set: Average loss: 2.3111, Accuracy: 1088/10000 (11%)\n",
      "\n",
      "Training - Epoch: 441 loss: 2.284557219950358 accuracy: 13.928333333333333\n",
      "\n",
      "Test set: Average loss: 2.3114, Accuracy: 1087/10000 (11%)\n",
      "\n",
      "Training - Epoch: 442 loss: 2.28450234858195 accuracy: 13.898333333333333\n",
      "\n",
      "Test set: Average loss: 2.3113, Accuracy: 1084/10000 (11%)\n",
      "\n",
      "Training - Epoch: 443 loss: 2.284468154144287 accuracy: 13.883333333333333\n",
      "\n",
      "Test set: Average loss: 2.3114, Accuracy: 1092/10000 (11%)\n",
      "\n",
      "Training - Epoch: 444 loss: 2.284432169342041 accuracy: 13.881666666666666\n",
      "\n",
      "Test set: Average loss: 2.3113, Accuracy: 1081/10000 (11%)\n",
      "\n",
      "Training - Epoch: 445 loss: 2.2843930536905925 accuracy: 13.901666666666667\n",
      "\n",
      "Test set: Average loss: 2.3114, Accuracy: 1075/10000 (11%)\n",
      "\n",
      "Training - Epoch: 446 loss: 2.2843628791809083 accuracy: 13.9\n",
      "\n",
      "Test set: Average loss: 2.3114, Accuracy: 1075/10000 (11%)\n",
      "\n",
      "Training - Epoch: 447 loss: 2.2843204305013023 accuracy: 13.951666666666666\n",
      "\n",
      "Test set: Average loss: 2.3112, Accuracy: 1096/10000 (11%)\n",
      "\n",
      "Training - Epoch: 448 loss: 2.2842799596150716 accuracy: 13.901666666666667\n",
      "\n",
      "Test set: Average loss: 2.3114, Accuracy: 1081/10000 (11%)\n",
      "\n",
      "Training - Epoch: 449 loss: 2.284244192759196 accuracy: 13.906666666666666\n",
      "\n",
      "Test set: Average loss: 2.3111, Accuracy: 1079/10000 (11%)\n",
      "\n",
      "Training - Epoch: 450 loss: 2.2841845815022785 accuracy: 13.981666666666667\n",
      "\n",
      "Test set: Average loss: 2.3114, Accuracy: 1076/10000 (11%)\n",
      "\n",
      "Training - Epoch: 451 loss: 2.284151377360026 accuracy: 13.993333333333334\n",
      "\n",
      "Test set: Average loss: 2.3115, Accuracy: 1087/10000 (11%)\n",
      "\n",
      "Training - Epoch: 452 loss: 2.2841216885884603 accuracy: 13.905\n",
      "\n",
      "Test set: Average loss: 2.3115, Accuracy: 1096/10000 (11%)\n",
      "\n",
      "Training - Epoch: 453 loss: 2.2840719261169435 accuracy: 13.925\n",
      "\n",
      "Test set: Average loss: 2.3115, Accuracy: 1080/10000 (11%)\n",
      "\n",
      "Training - Epoch: 454 loss: 2.2840288251241048 accuracy: 14.015\n",
      "\n",
      "Test set: Average loss: 2.3115, Accuracy: 1094/10000 (11%)\n",
      "\n",
      "Training - Epoch: 455 loss: 2.2839889539082847 accuracy: 13.931666666666667\n",
      "\n",
      "Test set: Average loss: 2.3114, Accuracy: 1085/10000 (11%)\n",
      "\n",
      "Training - Epoch: 456 loss: 2.283947984313965 accuracy: 14.043333333333333\n",
      "\n",
      "Test set: Average loss: 2.3118, Accuracy: 1108/10000 (11%)\n",
      "\n",
      "Training - Epoch: 457 loss: 2.283908108774821 accuracy: 13.966666666666667\n",
      "\n",
      "Test set: Average loss: 2.3116, Accuracy: 1093/10000 (11%)\n",
      "\n",
      "Training - Epoch: 458 loss: 2.2838824531555177 accuracy: 14.01\n",
      "\n",
      "Test set: Average loss: 2.3113, Accuracy: 1077/10000 (11%)\n",
      "\n",
      "Training - Epoch: 459 loss: 2.283839608001709 accuracy: 14.03\n",
      "\n",
      "Test set: Average loss: 2.3115, Accuracy: 1079/10000 (11%)\n",
      "\n",
      "Training - Epoch: 460 loss: 2.283797322845459 accuracy: 14.026666666666667\n",
      "\n",
      "Test set: Average loss: 2.3115, Accuracy: 1077/10000 (11%)\n",
      "\n",
      "Training - Epoch: 461 loss: 2.2837627431233725 accuracy: 14.025\n",
      "\n",
      "Test set: Average loss: 2.3113, Accuracy: 1101/10000 (11%)\n",
      "\n",
      "Training - Epoch: 462 loss: 2.283718443552653 accuracy: 13.998333333333333\n",
      "\n",
      "Test set: Average loss: 2.3116, Accuracy: 1074/10000 (11%)\n",
      "\n",
      "Training - Epoch: 463 loss: 2.283670079803467 accuracy: 14.026666666666667\n",
      "\n",
      "Test set: Average loss: 2.3118, Accuracy: 1088/10000 (11%)\n",
      "\n",
      "Training - Epoch: 464 loss: 2.2836394922892254 accuracy: 13.975\n",
      "\n",
      "Test set: Average loss: 2.3116, Accuracy: 1089/10000 (11%)\n",
      "\n",
      "Training - Epoch: 465 loss: 2.2835981440226236 accuracy: 13.991666666666667\n",
      "\n",
      "Test set: Average loss: 2.3114, Accuracy: 1067/10000 (11%)\n",
      "\n",
      "Training - Epoch: 466 loss: 2.2835619336446125 accuracy: 14.093333333333334\n",
      "\n",
      "Test set: Average loss: 2.3114, Accuracy: 1115/10000 (11%)\n",
      "\n",
      "Training - Epoch: 467 loss: 2.283519788614909 accuracy: 14.04\n",
      "\n",
      "Test set: Average loss: 2.3113, Accuracy: 1080/10000 (11%)\n",
      "\n",
      "Training - Epoch: 468 loss: 2.2834643625895183 accuracy: 14.051666666666666\n",
      "\n",
      "Test set: Average loss: 2.3116, Accuracy: 1104/10000 (11%)\n",
      "\n",
      "Training - Epoch: 469 loss: 2.2834371050516764 accuracy: 14.055\n",
      "\n",
      "Test set: Average loss: 2.3117, Accuracy: 1092/10000 (11%)\n",
      "\n",
      "Training - Epoch: 470 loss: 2.2833928016662597 accuracy: 14.03\n",
      "\n",
      "Test set: Average loss: 2.3118, Accuracy: 1096/10000 (11%)\n",
      "\n",
      "Training - Epoch: 471 loss: 2.2833252113342284 accuracy: 14.02\n",
      "\n",
      "Test set: Average loss: 2.3117, Accuracy: 1112/10000 (11%)\n",
      "\n",
      "Training - Epoch: 472 loss: 2.283323806254069 accuracy: 14.088333333333333\n",
      "\n",
      "Test set: Average loss: 2.3121, Accuracy: 1105/10000 (11%)\n",
      "\n",
      "Training - Epoch: 473 loss: 2.283270624287923 accuracy: 14.041666666666666\n",
      "\n",
      "Test set: Average loss: 2.3117, Accuracy: 1106/10000 (11%)\n",
      "\n",
      "Training - Epoch: 474 loss: 2.2832188000996907 accuracy: 14.026666666666667\n",
      "\n",
      "Test set: Average loss: 2.3114, Accuracy: 1109/10000 (11%)\n",
      "\n",
      "Training - Epoch: 475 loss: 2.2831945287068685 accuracy: 14.06\n",
      "\n",
      "Test set: Average loss: 2.3118, Accuracy: 1099/10000 (11%)\n",
      "\n",
      "Training - Epoch: 476 loss: 2.2831629740397137 accuracy: 14.091666666666667\n",
      "\n",
      "Test set: Average loss: 2.3120, Accuracy: 1079/10000 (11%)\n",
      "\n",
      "Training - Epoch: 477 loss: 2.283114426167806 accuracy: 14.026666666666667\n",
      "\n",
      "Test set: Average loss: 2.3120, Accuracy: 1075/10000 (11%)\n",
      "\n",
      "Training - Epoch: 478 loss: 2.2830780939737956 accuracy: 14.095\n",
      "\n",
      "Test set: Average loss: 2.3114, Accuracy: 1090/10000 (11%)\n",
      "\n",
      "Training - Epoch: 479 loss: 2.2830314397176106 accuracy: 14.158333333333333\n",
      "\n",
      "Test set: Average loss: 2.3118, Accuracy: 1084/10000 (11%)\n",
      "\n",
      "Training - Epoch: 480 loss: 2.2829944981892902 accuracy: 14.09\n",
      "\n",
      "Test set: Average loss: 2.3120, Accuracy: 1076/10000 (11%)\n",
      "\n",
      "Training - Epoch: 481 loss: 2.2829542004903156 accuracy: 14.113333333333333\n",
      "\n",
      "Test set: Average loss: 2.3118, Accuracy: 1091/10000 (11%)\n",
      "\n",
      "Training - Epoch: 482 loss: 2.282904384358724 accuracy: 14.135\n",
      "\n",
      "Test set: Average loss: 2.3119, Accuracy: 1095/10000 (11%)\n",
      "\n",
      "Training - Epoch: 483 loss: 2.282875988515218 accuracy: 14.136666666666667\n",
      "\n",
      "Test set: Average loss: 2.3117, Accuracy: 1079/10000 (11%)\n",
      "\n",
      "Training - Epoch: 484 loss: 2.282824741872152 accuracy: 14.078333333333333\n",
      "\n",
      "Test set: Average loss: 2.3113, Accuracy: 1104/10000 (11%)\n",
      "\n",
      "Training - Epoch: 485 loss: 2.282798353322347 accuracy: 14.113333333333333\n",
      "\n",
      "Test set: Average loss: 2.3117, Accuracy: 1106/10000 (11%)\n",
      "\n",
      "Training - Epoch: 486 loss: 2.282741960652669 accuracy: 14.168333333333333\n",
      "\n",
      "Test set: Average loss: 2.3118, Accuracy: 1118/10000 (11%)\n",
      "\n",
      "Training - Epoch: 487 loss: 2.28272532043457 accuracy: 14.101666666666667\n",
      "\n",
      "Test set: Average loss: 2.3117, Accuracy: 1090/10000 (11%)\n",
      "\n",
      "Training - Epoch: 488 loss: 2.28265319035848 accuracy: 14.168333333333333\n",
      "\n",
      "Test set: Average loss: 2.3119, Accuracy: 1079/10000 (11%)\n",
      "\n",
      "Training - Epoch: 489 loss: 2.282629372914632 accuracy: 14.186666666666667\n",
      "\n",
      "Test set: Average loss: 2.3119, Accuracy: 1095/10000 (11%)\n",
      "\n",
      "Training - Epoch: 490 loss: 2.2825881436665854 accuracy: 14.216666666666667\n",
      "\n",
      "Test set: Average loss: 2.3120, Accuracy: 1106/10000 (11%)\n",
      "\n",
      "Training - Epoch: 491 loss: 2.282547938791911 accuracy: 14.148333333333333\n",
      "\n",
      "Test set: Average loss: 2.3121, Accuracy: 1063/10000 (11%)\n",
      "\n",
      "Training - Epoch: 492 loss: 2.2824921447753908 accuracy: 14.191666666666666\n",
      "\n",
      "Test set: Average loss: 2.3125, Accuracy: 1080/10000 (11%)\n",
      "\n",
      "Training - Epoch: 493 loss: 2.2824684074401858 accuracy: 14.181666666666667\n",
      "\n",
      "Test set: Average loss: 2.3123, Accuracy: 1069/10000 (11%)\n",
      "\n",
      "Training - Epoch: 494 loss: 2.2824373916625977 accuracy: 14.188333333333333\n",
      "\n",
      "Test set: Average loss: 2.3117, Accuracy: 1083/10000 (11%)\n",
      "\n",
      "Training - Epoch: 495 loss: 2.282391414133708 accuracy: 14.22\n",
      "\n",
      "Test set: Average loss: 2.3118, Accuracy: 1076/10000 (11%)\n",
      "\n",
      "Training - Epoch: 496 loss: 2.282334006500244 accuracy: 14.278333333333334\n",
      "\n",
      "Test set: Average loss: 2.3121, Accuracy: 1091/10000 (11%)\n",
      "\n",
      "Training - Epoch: 497 loss: 2.282306477864583 accuracy: 14.18\n",
      "\n",
      "Test set: Average loss: 2.3118, Accuracy: 1080/10000 (11%)\n",
      "\n",
      "Training - Epoch: 498 loss: 2.2822680417378742 accuracy: 14.193333333333333\n",
      "\n",
      "Test set: Average loss: 2.3116, Accuracy: 1094/10000 (11%)\n",
      "\n",
      "Training - Epoch: 499 loss: 2.282227879333496 accuracy: 14.238333333333333\n",
      "\n",
      "Test set: Average loss: 2.3120, Accuracy: 1092/10000 (11%)\n",
      "\n",
      "Training - Epoch: 500 loss: 2.2821838539123536 accuracy: 14.245\n",
      "\n",
      "Test set: Average loss: 2.3121, Accuracy: 1106/10000 (11%)\n",
      "\n",
      "Training - Epoch: 501 loss: 2.2821369242350262 accuracy: 14.3\n",
      "\n",
      "Test set: Average loss: 2.3120, Accuracy: 1090/10000 (11%)\n",
      "\n",
      "Training - Epoch: 502 loss: 2.28211653620402 accuracy: 14.261666666666667\n",
      "\n",
      "Test set: Average loss: 2.3120, Accuracy: 1078/10000 (11%)\n",
      "\n",
      "Training - Epoch: 503 loss: 2.282057606506348 accuracy: 14.255\n",
      "\n",
      "Test set: Average loss: 2.3120, Accuracy: 1085/10000 (11%)\n",
      "\n",
      "Training - Epoch: 504 loss: 2.282023971557617 accuracy: 14.28\n",
      "\n",
      "Test set: Average loss: 2.3124, Accuracy: 1086/10000 (11%)\n",
      "\n",
      "Training - Epoch: 505 loss: 2.281982888285319 accuracy: 14.303333333333333\n",
      "\n",
      "Test set: Average loss: 2.3121, Accuracy: 1090/10000 (11%)\n",
      "\n",
      "Training - Epoch: 506 loss: 2.2819368896484375 accuracy: 14.251666666666667\n",
      "\n",
      "Test set: Average loss: 2.3123, Accuracy: 1099/10000 (11%)\n",
      "\n",
      "Training - Epoch: 507 loss: 2.2818945284525554 accuracy: 14.278333333333334\n",
      "\n",
      "Test set: Average loss: 2.3123, Accuracy: 1086/10000 (11%)\n",
      "\n",
      "Training - Epoch: 508 loss: 2.281867077636719 accuracy: 14.276666666666667\n",
      "\n",
      "Test set: Average loss: 2.3120, Accuracy: 1090/10000 (11%)\n",
      "\n",
      "Training - Epoch: 509 loss: 2.2818193878173827 accuracy: 14.365\n",
      "\n",
      "Test set: Average loss: 2.3118, Accuracy: 1081/10000 (11%)\n",
      "\n",
      "Training - Epoch: 510 loss: 2.2817593098958335 accuracy: 14.278333333333334\n",
      "\n",
      "Test set: Average loss: 2.3123, Accuracy: 1093/10000 (11%)\n",
      "\n",
      "Training - Epoch: 511 loss: 2.2817325439453127 accuracy: 14.291666666666666\n",
      "\n",
      "Test set: Average loss: 2.3123, Accuracy: 1095/10000 (11%)\n",
      "\n",
      "Training - Epoch: 512 loss: 2.2816897977193196 accuracy: 14.266666666666667\n",
      "\n",
      "Test set: Average loss: 2.3123, Accuracy: 1097/10000 (11%)\n",
      "\n",
      "Training - Epoch: 513 loss: 2.2816430330912274 accuracy: 14.381666666666666\n",
      "\n",
      "Test set: Average loss: 2.3124, Accuracy: 1116/10000 (11%)\n",
      "\n",
      "Training - Epoch: 514 loss: 2.28161399230957 accuracy: 14.341666666666667\n",
      "\n",
      "Test set: Average loss: 2.3121, Accuracy: 1110/10000 (11%)\n",
      "\n",
      "Training - Epoch: 515 loss: 2.281574698384603 accuracy: 14.28\n",
      "\n",
      "Test set: Average loss: 2.3116, Accuracy: 1079/10000 (11%)\n",
      "\n",
      "Training - Epoch: 516 loss: 2.2815294878641765 accuracy: 14.318333333333333\n",
      "\n",
      "Test set: Average loss: 2.3119, Accuracy: 1096/10000 (11%)\n",
      "\n",
      "Training - Epoch: 517 loss: 2.2814821230570477 accuracy: 14.313333333333333\n",
      "\n",
      "Test set: Average loss: 2.3120, Accuracy: 1107/10000 (11%)\n",
      "\n",
      "Training - Epoch: 518 loss: 2.2814528093973796 accuracy: 14.331666666666667\n",
      "\n",
      "Test set: Average loss: 2.3124, Accuracy: 1109/10000 (11%)\n",
      "\n",
      "Training - Epoch: 519 loss: 2.2814144912719727 accuracy: 14.283333333333333\n",
      "\n",
      "Test set: Average loss: 2.3125, Accuracy: 1082/10000 (11%)\n",
      "\n",
      "Training - Epoch: 520 loss: 2.2813711415608724 accuracy: 14.35\n",
      "\n",
      "Test set: Average loss: 2.3121, Accuracy: 1094/10000 (11%)\n",
      "\n",
      "Training - Epoch: 521 loss: 2.281316923522949 accuracy: 14.381666666666666\n",
      "\n",
      "Test set: Average loss: 2.3119, Accuracy: 1076/10000 (11%)\n",
      "\n",
      "Training - Epoch: 522 loss: 2.2812897432963055 accuracy: 14.336666666666666\n",
      "\n",
      "Test set: Average loss: 2.3122, Accuracy: 1082/10000 (11%)\n",
      "\n",
      "Training - Epoch: 523 loss: 2.2812427541097007 accuracy: 14.376666666666667\n",
      "\n",
      "Test set: Average loss: 2.3120, Accuracy: 1110/10000 (11%)\n",
      "\n",
      "Training - Epoch: 524 loss: 2.281212266286214 accuracy: 14.376666666666667\n",
      "\n",
      "Test set: Average loss: 2.3121, Accuracy: 1087/10000 (11%)\n",
      "\n",
      "Training - Epoch: 525 loss: 2.2811634941101073 accuracy: 14.415\n",
      "\n",
      "Test set: Average loss: 2.3124, Accuracy: 1094/10000 (11%)\n",
      "\n",
      "Training - Epoch: 526 loss: 2.281115351104736 accuracy: 14.36\n",
      "\n",
      "Test set: Average loss: 2.3123, Accuracy: 1116/10000 (11%)\n",
      "\n",
      "Training - Epoch: 527 loss: 2.2810618731180825 accuracy: 14.421666666666667\n",
      "\n",
      "Test set: Average loss: 2.3122, Accuracy: 1092/10000 (11%)\n",
      "\n",
      "Training - Epoch: 528 loss: 2.2810315149943032 accuracy: 14.356666666666667\n",
      "\n",
      "Test set: Average loss: 2.3125, Accuracy: 1110/10000 (11%)\n",
      "\n",
      "Training - Epoch: 529 loss: 2.280993359375 accuracy: 14.425\n",
      "\n",
      "Test set: Average loss: 2.3127, Accuracy: 1088/10000 (11%)\n",
      "\n",
      "Training - Epoch: 530 loss: 2.2809644465128582 accuracy: 14.385\n",
      "\n",
      "Test set: Average loss: 2.3123, Accuracy: 1095/10000 (11%)\n",
      "\n",
      "Training - Epoch: 531 loss: 2.2809270805358888 accuracy: 14.42\n",
      "\n",
      "Test set: Average loss: 2.3124, Accuracy: 1101/10000 (11%)\n",
      "\n",
      "Training - Epoch: 532 loss: 2.2808718724568684 accuracy: 14.428333333333333\n",
      "\n",
      "Test set: Average loss: 2.3123, Accuracy: 1080/10000 (11%)\n",
      "\n",
      "Training - Epoch: 533 loss: 2.2808119016011554 accuracy: 14.408333333333333\n",
      "\n",
      "Test set: Average loss: 2.3128, Accuracy: 1115/10000 (11%)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "epoch=1000\n",
    "model = CnnNet().to(device)\n",
    "optimizer = optim.Adadelta(model.parameters(), lr=0.001)\n",
    "scheduler = StepLR(optimizer, step_size=1, gamma=0.1)\n",
    "loss = nn.CrossEntropyLoss()\n",
    "train_loss_1, accuracy_train_1, test_loss_1, test_accuracy_1= returnModelAccAndLoss(model, device, train_loader_shuffle, test_loader, optimizer, loss, epoch, scheduler)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "7ddc1177",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x432 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt1=plt.figure(figsize=(10, 6))\n",
    "plt.plot(np.array(train_loss_1), 'r', label='training')\n",
    "plt.plot(np.array(test_loss_1), 'b', label='test')\n",
    "\n",
    "# naming the x axis\n",
    "plt.xlabel('Epochs')\n",
    "#plt.xscale('log')\n",
    "# naming the y axis\n",
    "plt.ylabel('Loss') \n",
    "#plt.yscale('log')\n",
    "\n",
    "plt.legend()\n",
    "plt.title(\"Loss function For MNIST\")\n",
    "plt.show()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24b64187",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d352c5e-eac9-4678-aa2e-6793157cbdf0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
